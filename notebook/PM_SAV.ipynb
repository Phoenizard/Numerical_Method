{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%m%d%H%M\")\n",
    "wandb.init(project='Numerical Method', name=f\"PM_SAV_sum_Example_2_{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80000, 21]),\n",
       " torch.Size([80000]),\n",
       " torch.Size([20000, 21]),\n",
       " torch.Size([20000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = torch.load('../data/dataset_3_20D.pt', weights_only=True)\n",
    "\n",
    "# 将数据移动到适当的设备\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "Y_test = Y_test.to(device)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 21]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# 使用 DataLoader 进行批处理\n",
    "l = 256\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=l, shuffle=True)\n",
    "\n",
    "# 打印第一个批次的大小\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 20\n",
    "m = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.485905170440674, 142.09332275390625)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PM_Euler(nn.Module):\n",
    "    def __init__(self, input, hidden_layer, output):\n",
    "        super(PM_Euler, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_dim = hidden_layer\n",
    "        self.W = nn.Parameter(torch.rand(input, hidden_layer, device=device), requires_grad=True)\n",
    "        # HE初始化\n",
    "        nn.init.kaiming_normal_(self.W, mode='fan_in', nonlinearity='relu')\n",
    "        self.a = nn.Parameter(torch.rand(hidden_layer, output, device=device), requires_grad=True)\n",
    "        nn.init.kaiming_normal_(self.a, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.relu(torch.mm(x, self.W))\n",
    "        z2 = torch.mm(z1, self.a) / self.hidden_dim\n",
    "        return z2\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        return (y_pred - y_true.reshape(y_pred.shape)) ** 2\n",
    "\n",
    "model = PM_Euler(D + 1, m, 1).to(device)\n",
    "\n",
    "# 计算模型W和a的Norm\n",
    "def get_norm(model):\n",
    "    return torch.norm(model.W).item(), torch.norm(model.a).item()\n",
    "\n",
    "get_norm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "lr = 1\n",
    "C = 100\n",
    "_lambda = 4\n",
    "r = [0]\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 21]) torch.Size([80000])\n",
      "tensor([ 0.2917, -0.3087, -0.3028,  0.0076, -0.4275, -1.0602, -0.1303, -1.3861,\n",
      "        -0.3681,  0.2687,  0.0687,  0.2104, -0.1937, -0.9206, -0.2700,  0.6400,\n",
      "        -1.2866,  0.6359, -0.3955,  0.2940,  1.0000], device='cuda:0') tensor(5.5030, device='cuda:0')\n",
      "tensor([[ 0.0006],\n",
      "        [ 0.0003],\n",
      "        [-0.0007],\n",
      "        ...,\n",
      "        [ 0.0001],\n",
      "        [-0.0002],\n",
      "        [-0.0005]], device='cuda:0')\n",
      "Init Loss : 0.9661540985107422\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "    print(X_train[0], Y_train[0])\n",
    "    print(model(X_train))\n",
    "    print(f'Init Loss : {model.loss(model(X_train), Y_train).mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "    'learning_rate': lr,\n",
    "    'batch_size': l,\n",
    "    'epochs': epochs,\n",
    "    'hidden_layer': m,\n",
    "    'input': D + 1,\n",
    "    'output': 1,\n",
    "    'optimizer': 'SAV',\n",
    "    'C': C,\n",
    "    '_lambda': _lambda,\n",
    "    'r': r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8614/1852956516.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r = torch.sqrt(torch.tensor(loss + C, device=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.9549, test loss 1.1249\n",
      "epoch 2, loss 0.9469, test loss 1.1169\n",
      "epoch 3, loss 0.9381, test loss 1.1080\n",
      "epoch 4, loss 0.9292, test loss 1.0989\n",
      "epoch 5, loss 0.9199, test loss 1.0895\n",
      "epoch 6, loss 0.9110, test loss 1.0806\n",
      "epoch 7, loss 0.9007, test loss 1.0704\n",
      "epoch 8, loss 0.8907, test loss 1.0602\n",
      "epoch 9, loss 0.8804, test loss 1.0499\n",
      "epoch 10, loss 0.8680, test loss 1.0372\n",
      "epoch 11, loss 0.8536, test loss 1.0226\n",
      "epoch 12, loss 0.8390, test loss 1.0077\n",
      "epoch 13, loss 0.8231, test loss 0.9914\n",
      "epoch 14, loss 0.8074, test loss 0.9754\n",
      "epoch 15, loss 0.7827, test loss 0.9494\n",
      "epoch 16, loss 0.7633, test loss 0.9293\n",
      "epoch 17, loss 0.7481, test loss 0.9134\n",
      "epoch 18, loss 0.7273, test loss 0.8913\n",
      "epoch 19, loss 0.7012, test loss 0.8640\n",
      "epoch 20, loss 0.6854, test loss 0.8471\n",
      "epoch 21, loss 0.6730, test loss 0.8338\n",
      "epoch 22, loss 0.6596, test loss 0.8191\n",
      "epoch 23, loss 0.6431, test loss 0.8012\n",
      "epoch 24, loss 0.6282, test loss 0.7851\n",
      "epoch 25, loss 0.6179, test loss 0.7737\n",
      "epoch 26, loss 0.6081, test loss 0.7627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7efc290cba40>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/Numerical_Method/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                      ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/envs/Numerical_Method/lib/python3.12/threading.py\", line 1196, in ident\n",
      "    @property\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, loss 0.6002, test loss 0.7539\n",
      "epoch 28, loss 0.5943, test loss 0.7475\n",
      "epoch 29, loss 0.5873, test loss 0.7395\n",
      "epoch 30, loss 0.5826, test loss 0.7344\n",
      "epoch 31, loss 0.5786, test loss 0.7297\n",
      "epoch 32, loss 0.5755, test loss 0.7263\n",
      "epoch 33, loss 0.5726, test loss 0.7227\n",
      "epoch 34, loss 0.5696, test loss 0.7191\n",
      "epoch 35, loss 0.5676, test loss 0.7169\n",
      "epoch 36, loss 0.5653, test loss 0.7144\n",
      "epoch 37, loss 0.5641, test loss 0.7131\n",
      "epoch 38, loss 0.5624, test loss 0.7109\n",
      "epoch 39, loss 0.5615, test loss 0.7098\n",
      "epoch 40, loss 0.5605, test loss 0.7090\n",
      "epoch 41, loss 0.5597, test loss 0.7079\n",
      "epoch 42, loss 0.5591, test loss 0.7075\n",
      "epoch 43, loss 0.5585, test loss 0.7069\n",
      "epoch 44, loss 0.5579, test loss 0.7063\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    cnt = 0\n",
    "    for X, Y in train_loader:\n",
    "        loss = model.loss(model(X), Y).sum()\n",
    "        if cnt == 0:\n",
    "            r = torch.sqrt(torch.tensor(loss + C, device=device))\n",
    "            cnt = 1\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            N_a = model.a.grad.clone()\n",
    "            N_w = model.W.grad.clone()\n",
    "            theta_a_2 = lr * N_a / (torch.sqrt(loss + C) * (1 + lr * _lambda))\n",
    "            theta_w_2 = lr * N_w / (torch.sqrt(loss + C) * (1 + lr * _lambda))\n",
    "            r = r / (1 + lr * (torch.sum(N_a * (N_a / (1 + lr * _lambda))) + torch.sum(N_w * (N_w) / (1 + lr * _lambda))) / (2 * (loss + C)))\n",
    "            model.a -= r.item() * theta_a_2\n",
    "            model.W -= r.item() * theta_w_2\n",
    "            model.a.grad.zero_()\n",
    "            model.W.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        train_loss = model.loss(model(X_train), Y_train).mean()\n",
    "        test_loss = model.loss(model(X_test), Y_test).mean()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        norm = get_norm(model)\n",
    "        # wandb.log({'epoch': epoch + 1,\n",
    "        #            'train_loss': train_loss, \n",
    "        #            'test_loss': test_loss,\n",
    "        #            'norm_W': norm[0],\n",
    "        #            'norm_a': norm[1],\n",
    "        #            'accuracy': 1 - test_loss,\n",
    "        #            'r': r.item()})\n",
    "        print(f'epoch {epoch + 1}, loss {train_loss:.4f}, test loss {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Numerical_Method",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
