{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 41]),\n",
       " torch.Size([8000]),\n",
       " torch.Size([2000, 41]),\n",
       " torch.Size([2000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = torch.load('./data/dataset_1_40D.pt')\n",
    "\n",
    "# 将数据移动到适当的设备\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "Y_test = Y_test.to(device)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 41]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 使用 DataLoader 进行批处理\n",
    "l = 64\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=l, shuffle=True)\n",
    "\n",
    "# 打印第一个批次的大小\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 40\n",
    "m = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 1]), torch.Size([8000, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PM_Euler(nn.Module):\n",
    "    def __init__(self, input, hidden_layer, output):\n",
    "        super(PM_Euler, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input, hidden_layer)\n",
    "        self.fc2 = nn.Linear(hidden_layer, output)\n",
    "        self.hidden_dim = hidden_layer\n",
    "        # He initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.fc1(x)\n",
    "        z2 = self.relu(z1)\n",
    "        f1 = self.fc2(z2) / self.hidden_dim\n",
    "        return f1\n",
    "    \n",
    "model = PM_Euler(D + 1, m, 1).to(device)\n",
    "\n",
    "model(X_train).shape ,Y_train.reshape((-1, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.9969203472137451, Test Loss: 1.0109018087387085\n",
      "Epoch 1, Train Loss: 0.9967473745346069, Test Loss: 1.0107187032699585\n",
      "Epoch 2, Train Loss: 0.9965744018554688, Test Loss: 1.0105355978012085\n",
      "Epoch 3, Train Loss: 0.9964014291763306, Test Loss: 1.0103524923324585\n",
      "Epoch 4, Train Loss: 0.9962283372879028, Test Loss: 1.0101690292358398\n",
      "Epoch 5, Train Loss: 0.9960551261901855, Test Loss: 1.0099854469299316\n",
      "Epoch 6, Train Loss: 0.9958816766738892, Test Loss: 1.0098015069961548\n",
      "Epoch 7, Train Loss: 0.9957079291343689, Test Loss: 1.0096172094345093\n",
      "Epoch 8, Train Loss: 0.9955338835716248, Test Loss: 1.0094321966171265\n",
      "Epoch 9, Train Loss: 0.9953592419624329, Test Loss: 1.009246826171875\n",
      "Epoch 10, Train Loss: 0.9951841235160828, Test Loss: 1.0090607404708862\n",
      "Epoch 11, Train Loss: 0.9950084090232849, Test Loss: 1.0088740587234497\n",
      "Epoch 12, Train Loss: 0.9948320984840393, Test Loss: 1.0086865425109863\n",
      "Epoch 13, Train Loss: 0.9946549534797668, Test Loss: 1.008498191833496\n",
      "Epoch 14, Train Loss: 0.9944770336151123, Test Loss: 1.0083087682724\n",
      "Epoch 15, Train Loss: 0.9942982792854309, Test Loss: 1.0081185102462769\n",
      "Epoch 16, Train Loss: 0.9941184520721436, Test Loss: 1.0079271793365479\n",
      "Epoch 17, Train Loss: 0.9939376711845398, Test Loss: 1.0077345371246338\n",
      "Epoch 18, Train Loss: 0.9937556982040405, Test Loss: 1.0075407028198242\n",
      "Epoch 19, Train Loss: 0.9935725927352905, Test Loss: 1.0073456764221191\n",
      "Epoch 20, Train Loss: 0.993388295173645, Test Loss: 1.00714910030365\n",
      "Epoch 21, Train Loss: 0.9932026267051697, Test Loss: 1.006951093673706\n",
      "Epoch 22, Train Loss: 0.993015468120575, Test Loss: 1.006751537322998\n",
      "Epoch 23, Train Loss: 0.9928269386291504, Test Loss: 1.0065501928329468\n",
      "Epoch 24, Train Loss: 0.9926368594169617, Test Loss: 1.0063472986221313\n",
      "Epoch 25, Train Loss: 0.9924451112747192, Test Loss: 1.006142497062683\n",
      "Epoch 26, Train Loss: 0.9922516942024231, Test Loss: 1.0059359073638916\n",
      "Epoch 27, Train Loss: 0.9920564293861389, Test Loss: 1.0057271718978882\n",
      "Epoch 28, Train Loss: 0.9918593168258667, Test Loss: 1.005516529083252\n",
      "Epoch 29, Train Loss: 0.9916602969169617, Test Loss: 1.0053037405014038\n",
      "Epoch 30, Train Loss: 0.9914592504501343, Test Loss: 1.0050886869430542\n",
      "Epoch 31, Train Loss: 0.9912561774253845, Test Loss: 1.0048714876174927\n",
      "Epoch 32, Train Loss: 0.9910508394241333, Test Loss: 1.004651665687561\n",
      "Epoch 33, Train Loss: 0.9908431768417358, Test Loss: 1.0044294595718384\n",
      "Epoch 34, Train Loss: 0.9906332492828369, Test Loss: 1.0042047500610352\n",
      "Epoch 35, Train Loss: 0.9904208183288574, Test Loss: 1.0039771795272827\n",
      "Epoch 36, Train Loss: 0.9902060031890869, Test Loss: 1.0037472248077393\n",
      "Epoch 37, Train Loss: 0.9899885058403015, Test Loss: 1.0035141706466675\n",
      "Epoch 38, Train Loss: 0.989768385887146, Test Loss: 1.003278136253357\n",
      "Epoch 39, Train Loss: 0.989545464515686, Test Loss: 1.0030392408370972\n",
      "Epoch 40, Train Loss: 0.9893196225166321, Test Loss: 1.00279700756073\n",
      "Epoch 41, Train Loss: 0.9890908598899841, Test Loss: 1.002551794052124\n",
      "Epoch 42, Train Loss: 0.9888591170310974, Test Loss: 1.002303123474121\n",
      "Epoch 43, Train Loss: 0.9886243343353271, Test Loss: 1.0020509958267212\n",
      "Epoch 44, Train Loss: 0.9883861541748047, Test Loss: 1.0017955303192139\n",
      "Epoch 45, Train Loss: 0.9881446957588196, Test Loss: 1.00153648853302\n",
      "Epoch 46, Train Loss: 0.9878998398780823, Test Loss: 1.0012736320495605\n",
      "Epoch 47, Train Loss: 0.987651526927948, Test Loss: 1.001006841659546\n",
      "Epoch 48, Train Loss: 0.9873996376991272, Test Loss: 1.0007364749908447\n",
      "Epoch 49, Train Loss: 0.9871440529823303, Test Loss: 1.0004618167877197\n",
      "Epoch 50, Train Loss: 0.986884593963623, Test Loss: 1.00018310546875\n",
      "Epoch 51, Train Loss: 0.9866212606430054, Test Loss: 0.9999003410339355\n",
      "Epoch 52, Train Loss: 0.9863539338111877, Test Loss: 0.9996129274368286\n",
      "Epoch 53, Train Loss: 0.9860824942588806, Test Loss: 0.9993213415145874\n",
      "Epoch 54, Train Loss: 0.9858068227767944, Test Loss: 0.9990251660346985\n",
      "Epoch 55, Train Loss: 0.9855268001556396, Test Loss: 0.9987241625785828\n",
      "Epoch 56, Train Loss: 0.9852423667907715, Test Loss: 0.998418390750885\n",
      "Epoch 57, Train Loss: 0.9849534034729004, Test Loss: 0.9981076717376709\n",
      "Epoch 58, Train Loss: 0.9846597909927368, Test Loss: 0.9977917671203613\n",
      "Epoch 59, Train Loss: 0.9843613505363464, Test Loss: 0.9974709749221802\n",
      "Epoch 60, Train Loss: 0.9840580821037292, Test Loss: 0.9971445798873901\n",
      "Epoch 61, Train Loss: 0.9837498068809509, Test Loss: 0.9968129396438599\n",
      "Epoch 62, Train Loss: 0.9834363460540771, Test Loss: 0.9964755773544312\n",
      "Epoch 63, Train Loss: 0.9831175804138184, Test Loss: 0.9961325526237488\n",
      "Epoch 64, Train Loss: 0.9827936291694641, Test Loss: 0.9957836270332336\n",
      "Epoch 65, Train Loss: 0.982464075088501, Test Loss: 0.9954288601875305\n",
      "Epoch 66, Train Loss: 0.982128918170929, Test Loss: 0.9950680136680603\n",
      "Epoch 67, Train Loss: 0.9817878603935242, Test Loss: 0.9947010278701782\n",
      "Epoch 68, Train Loss: 0.9814410209655762, Test Loss: 0.9943277835845947\n",
      "Epoch 69, Train Loss: 0.9810881614685059, Test Loss: 0.9939477443695068\n",
      "Epoch 70, Train Loss: 0.9807291030883789, Test Loss: 0.9935611486434937\n",
      "Epoch 71, Train Loss: 0.9803638458251953, Test Loss: 0.9931676983833313\n",
      "Epoch 72, Train Loss: 0.9799919724464417, Test Loss: 0.9927672147750854\n",
      "Epoch 73, Train Loss: 0.9796135425567627, Test Loss: 0.9923595190048218\n",
      "Epoch 74, Train Loss: 0.9792284369468689, Test Loss: 0.9919443726539612\n",
      "Epoch 75, Train Loss: 0.9788363575935364, Test Loss: 0.9915217161178589\n",
      "Epoch 76, Train Loss: 0.9784371852874756, Test Loss: 0.9910915493965149\n",
      "Epoch 77, Train Loss: 0.9780309200286865, Test Loss: 0.9906535148620605\n",
      "Epoch 78, Train Loss: 0.9776173830032349, Test Loss: 0.9902073740959167\n",
      "Epoch 79, Train Loss: 0.9771963953971863, Test Loss: 0.9897534251213074\n",
      "Epoch 80, Train Loss: 0.9767676591873169, Test Loss: 0.9892910718917847\n",
      "Epoch 81, Train Loss: 0.9763312339782715, Test Loss: 0.9888202548027039\n",
      "Epoch 82, Train Loss: 0.9758867621421814, Test Loss: 0.9883407950401306\n",
      "Epoch 83, Train Loss: 0.9754341840744019, Test Loss: 0.9878525733947754\n",
      "Epoch 84, Train Loss: 0.9749733209609985, Test Loss: 0.9873554110527039\n",
      "Epoch 85, Train Loss: 0.9745039939880371, Test Loss: 0.9868490695953369\n",
      "Epoch 86, Train Loss: 0.9740260243415833, Test Loss: 0.986333429813385\n",
      "Epoch 87, Train Loss: 0.973539412021637, Test Loss: 0.9858078956604004\n",
      "Epoch 88, Train Loss: 0.9730437397956848, Test Loss: 0.9852728843688965\n",
      "Epoch 89, Train Loss: 0.9725388884544373, Test Loss: 0.9847279787063599\n",
      "Epoch 90, Train Loss: 0.97202467918396, Test Loss: 0.9841727614402771\n",
      "Epoch 91, Train Loss: 0.9715009331703186, Test Loss: 0.9836072325706482\n",
      "Epoch 92, Train Loss: 0.9709675908088684, Test Loss: 0.9830313920974731\n",
      "Epoch 93, Train Loss: 0.9704243540763855, Test Loss: 0.9824450016021729\n",
      "Epoch 94, Train Loss: 0.9698710441589355, Test Loss: 0.9818474650382996\n",
      "Epoch 95, Train Loss: 0.969307541847229, Test Loss: 0.981238842010498\n",
      "Epoch 96, Train Loss: 0.9687336683273315, Test Loss: 0.9806190729141235\n",
      "Epoch 97, Train Loss: 0.9681492447853088, Test Loss: 0.9799878597259521\n",
      "Epoch 98, Train Loss: 0.9675541520118713, Test Loss: 0.9793447256088257\n",
      "Epoch 99, Train Loss: 0.9669480323791504, Test Loss: 0.9786897897720337\n",
      "Epoch 100, Train Loss: 0.9663307070732117, Test Loss: 0.9780227541923523\n",
      "Epoch 101, Train Loss: 0.9657019376754761, Test Loss: 0.9773432016372681\n",
      "Epoch 102, Train Loss: 0.9650615453720093, Test Loss: 0.9766511917114258\n",
      "Epoch 103, Train Loss: 0.9644094705581665, Test Loss: 0.975946307182312\n",
      "Epoch 104, Train Loss: 0.963745653629303, Test Loss: 0.9752287268638611\n",
      "Epoch 105, Train Loss: 0.9630696773529053, Test Loss: 0.9744978547096252\n",
      "Epoch 106, Train Loss: 0.9623814821243286, Test Loss: 0.9737535119056702\n",
      "Epoch 107, Train Loss: 0.9616806507110596, Test Loss: 0.9729955196380615\n",
      "Epoch 108, Train Loss: 0.960966944694519, Test Loss: 0.9722235798835754\n",
      "Epoch 109, Train Loss: 0.9602403044700623, Test Loss: 0.9714370369911194\n",
      "Epoch 110, Train Loss: 0.9595004320144653, Test Loss: 0.9706364870071411\n",
      "Epoch 111, Train Loss: 0.9587470889091492, Test Loss: 0.9698209762573242\n",
      "Epoch 112, Train Loss: 0.9579802751541138, Test Loss: 0.9689905047416687\n",
      "Epoch 113, Train Loss: 0.9571998715400696, Test Loss: 0.9681451320648193\n",
      "Epoch 114, Train Loss: 0.9564054608345032, Test Loss: 0.9672844409942627\n",
      "Epoch 115, Train Loss: 0.9555968642234802, Test Loss: 0.9664087295532227\n",
      "Epoch 116, Train Loss: 0.9547742009162903, Test Loss: 0.965517520904541\n",
      "Epoch 117, Train Loss: 0.9539370536804199, Test Loss: 0.9646103382110596\n",
      "Epoch 118, Train Loss: 0.9530851244926453, Test Loss: 0.9636871218681335\n",
      "Epoch 119, Train Loss: 0.9522183537483215, Test Loss: 0.962748110294342\n",
      "Epoch 120, Train Loss: 0.9513363838195801, Test Loss: 0.9617923498153687\n",
      "Epoch 121, Train Loss: 0.9504393935203552, Test Loss: 0.9608204960823059\n",
      "Epoch 122, Train Loss: 0.9495270252227783, Test Loss: 0.9598315954208374\n",
      "Epoch 123, Train Loss: 0.9485990405082703, Test Loss: 0.9588261246681213\n",
      "Epoch 124, Train Loss: 0.9476553797721863, Test Loss: 0.9578028917312622\n",
      "Epoch 125, Train Loss: 0.9466961026191711, Test Loss: 0.9567624926567078\n",
      "Epoch 126, Train Loss: 0.9457207918167114, Test Loss: 0.9557053446769714\n",
      "Epoch 127, Train Loss: 0.9447290897369385, Test Loss: 0.9546301960945129\n",
      "Epoch 128, Train Loss: 0.9437209963798523, Test Loss: 0.9535374045372009\n",
      "Epoch 129, Train Loss: 0.9426968097686768, Test Loss: 0.9524267911911011\n",
      "Epoch 130, Train Loss: 0.9416561722755432, Test Loss: 0.9512982368469238\n",
      "Epoch 131, Train Loss: 0.9405989050865173, Test Loss: 0.9501521587371826\n",
      "Epoch 132, Train Loss: 0.9395251870155334, Test Loss: 0.9489878416061401\n",
      "Epoch 133, Train Loss: 0.9384348392486572, Test Loss: 0.9478054046630859\n",
      "Epoch 134, Train Loss: 0.9373276233673096, Test Loss: 0.9466046690940857\n",
      "Epoch 135, Train Loss: 0.9362038969993591, Test Loss: 0.9453858733177185\n",
      "Epoch 136, Train Loss: 0.935063362121582, Test Loss: 0.9441483616828918\n",
      "Epoch 137, Train Loss: 0.9339057207107544, Test Loss: 0.9428922533988953\n",
      "Epoch 138, Train Loss: 0.9327313303947449, Test Loss: 0.9416180849075317\n",
      "Epoch 139, Train Loss: 0.9315397143363953, Test Loss: 0.9403254985809326\n",
      "Epoch 140, Train Loss: 0.9303314089775085, Test Loss: 0.939014732837677\n",
      "Epoch 141, Train Loss: 0.9291064143180847, Test Loss: 0.9376859664916992\n",
      "Epoch 142, Train Loss: 0.9278649091720581, Test Loss: 0.9363390207290649\n",
      "Epoch 143, Train Loss: 0.9266063570976257, Test Loss: 0.9349735379219055\n",
      "Epoch 144, Train Loss: 0.9253314733505249, Test Loss: 0.9335901141166687\n",
      "Epoch 145, Train Loss: 0.9240403175354004, Test Loss: 0.9321892857551575\n",
      "Epoch 146, Train Loss: 0.9227328896522522, Test Loss: 0.9307699203491211\n",
      "Epoch 147, Train Loss: 0.9214097857475281, Test Loss: 0.9293327331542969\n",
      "Epoch 148, Train Loss: 0.9200708270072937, Test Loss: 0.9278777241706848\n",
      "Epoch 149, Train Loss: 0.9187163710594177, Test Loss: 0.9264057874679565\n",
      "Epoch 150, Train Loss: 0.9173464775085449, Test Loss: 0.9249171614646912\n",
      "Epoch 151, Train Loss: 0.915960967540741, Test Loss: 0.9234114289283752\n",
      "Epoch 152, Train Loss: 0.9145607352256775, Test Loss: 0.9218891859054565\n",
      "Epoch 153, Train Loss: 0.9131455421447754, Test Loss: 0.920350193977356\n",
      "Epoch 154, Train Loss: 0.9117156863212585, Test Loss: 0.918794572353363\n",
      "Epoch 155, Train Loss: 0.9102721214294434, Test Loss: 0.9172238111495972\n",
      "Epoch 156, Train Loss: 0.9088143706321716, Test Loss: 0.9156369566917419\n",
      "Epoch 157, Train Loss: 0.9073433876037598, Test Loss: 0.9140351414680481\n",
      "Epoch 158, Train Loss: 0.9058594107627869, Test Loss: 0.9124181270599365\n",
      "Epoch 159, Train Loss: 0.904362678527832, Test Loss: 0.9107876420021057\n",
      "Epoch 160, Train Loss: 0.9028535485267639, Test Loss: 0.9091423749923706\n",
      "Epoch 161, Train Loss: 0.9013321995735168, Test Loss: 0.9074837565422058\n",
      "Epoch 162, Train Loss: 0.8997992277145386, Test Loss: 0.9058110117912292\n",
      "Epoch 163, Train Loss: 0.8982556462287903, Test Loss: 0.9041265249252319\n",
      "Epoch 164, Train Loss: 0.8967015743255615, Test Loss: 0.9024303555488586\n",
      "Epoch 165, Train Loss: 0.8951371312141418, Test Loss: 0.9007228016853333\n",
      "Epoch 166, Train Loss: 0.893562376499176, Test Loss: 0.8990041613578796\n",
      "Epoch 167, Train Loss: 0.8919789791107178, Test Loss: 0.8972753286361694\n",
      "Epoch 168, Train Loss: 0.8903865814208984, Test Loss: 0.8955360651016235\n",
      "Epoch 169, Train Loss: 0.8887856602668762, Test Loss: 0.8937867879867554\n",
      "Epoch 170, Train Loss: 0.8871766924858093, Test Loss: 0.8920287489891052\n",
      "Epoch 171, Train Loss: 0.8855613470077515, Test Loss: 0.8902640342712402\n",
      "Epoch 172, Train Loss: 0.8839393854141235, Test Loss: 0.8884916305541992\n",
      "Epoch 173, Train Loss: 0.8823119401931763, Test Loss: 0.8867128491401672\n",
      "Epoch 174, Train Loss: 0.8806794881820679, Test Loss: 0.8849289417266846\n",
      "Epoch 175, Train Loss: 0.8790424466133118, Test Loss: 0.8831396698951721\n",
      "Epoch 176, Train Loss: 0.8774014115333557, Test Loss: 0.881345808506012\n",
      "Epoch 177, Train Loss: 0.8757563233375549, Test Loss: 0.8795468211174011\n",
      "Epoch 178, Train Loss: 0.8741087913513184, Test Loss: 0.8777455687522888\n",
      "Epoch 179, Train Loss: 0.8724585771560669, Test Loss: 0.87594074010849\n",
      "Epoch 180, Train Loss: 0.8708072900772095, Test Loss: 0.8741332292556763\n",
      "Epoch 181, Train Loss: 0.8691560626029968, Test Loss: 0.8723251819610596\n",
      "Epoch 182, Train Loss: 0.8675050735473633, Test Loss: 0.8705170154571533\n",
      "Epoch 183, Train Loss: 0.865854024887085, Test Loss: 0.8687074184417725\n",
      "Epoch 184, Train Loss: 0.8642047643661499, Test Loss: 0.8669002056121826\n",
      "Epoch 185, Train Loss: 0.8625566959381104, Test Loss: 0.8650944232940674\n",
      "Epoch 186, Train Loss: 0.8609114289283752, Test Loss: 0.863291323184967\n",
      "Epoch 187, Train Loss: 0.8592686057090759, Test Loss: 0.8614894151687622\n",
      "Epoch 188, Train Loss: 0.8576292991638184, Test Loss: 0.8596908450126648\n",
      "Epoch 189, Train Loss: 0.8559945225715637, Test Loss: 0.8578971028327942\n",
      "Epoch 190, Train Loss: 0.8543642163276672, Test Loss: 0.8561080694198608\n",
      "Epoch 191, Train Loss: 0.8527389764785767, Test Loss: 0.8543238639831543\n",
      "Epoch 192, Train Loss: 0.8511193990707397, Test Loss: 0.8525440692901611\n",
      "Epoch 193, Train Loss: 0.8495067358016968, Test Loss: 0.8507721424102783\n",
      "Epoch 194, Train Loss: 0.8479006886482239, Test Loss: 0.849007248878479\n",
      "Epoch 195, Train Loss: 0.8463021516799927, Test Loss: 0.8472487926483154\n",
      "Epoch 196, Train Loss: 0.8447116017341614, Test Loss: 0.8454992175102234\n",
      "Epoch 197, Train Loss: 0.8431285619735718, Test Loss: 0.8437561392784119\n",
      "Epoch 198, Train Loss: 0.8415539860725403, Test Loss: 0.8420215249061584\n",
      "Epoch 199, Train Loss: 0.8399900197982788, Test Loss: 0.8402984738349915\n",
      "Epoch 200, Train Loss: 0.8384349942207336, Test Loss: 0.83858323097229\n",
      "Epoch 201, Train Loss: 0.8368896842002869, Test Loss: 0.836877703666687\n",
      "Epoch 202, Train Loss: 0.8353549242019653, Test Loss: 0.8351844549179077\n",
      "Epoch 203, Train Loss: 0.8338304758071899, Test Loss: 0.8335015177726746\n",
      "Epoch 204, Train Loss: 0.8323166966438293, Test Loss: 0.831830620765686\n",
      "Epoch 205, Train Loss: 0.8308148384094238, Test Loss: 0.8301721811294556\n",
      "Epoch 206, Train Loss: 0.8293238878250122, Test Loss: 0.8285256624221802\n",
      "Epoch 207, Train Loss: 0.8278446793556213, Test Loss: 0.8268928527832031\n",
      "Epoch 208, Train Loss: 0.8263769745826721, Test Loss: 0.8252724409103394\n",
      "Epoch 209, Train Loss: 0.8249205350875854, Test Loss: 0.8236628770828247\n",
      "Epoch 210, Train Loss: 0.8234754204750061, Test Loss: 0.8220659494400024\n",
      "Epoch 211, Train Loss: 0.8220432996749878, Test Loss: 0.8204831480979919\n",
      "Epoch 212, Train Loss: 0.8206233978271484, Test Loss: 0.8189145922660828\n",
      "Epoch 213, Train Loss: 0.8192153573036194, Test Loss: 0.8173585534095764\n",
      "Epoch 214, Train Loss: 0.8178191184997559, Test Loss: 0.8158143758773804\n",
      "Epoch 215, Train Loss: 0.8164353966712952, Test Loss: 0.8142855763435364\n",
      "Epoch 216, Train Loss: 0.8150639533996582, Test Loss: 0.8127679824829102\n",
      "Epoch 217, Train Loss: 0.8137055039405823, Test Loss: 0.8112670183181763\n",
      "Epoch 218, Train Loss: 0.812359094619751, Test Loss: 0.8097779750823975\n",
      "Epoch 219, Train Loss: 0.8110260963439941, Test Loss: 0.8083035349845886\n",
      "Epoch 220, Train Loss: 0.8097058534622192, Test Loss: 0.8068414926528931\n",
      "Epoch 221, Train Loss: 0.808397114276886, Test Loss: 0.8053913712501526\n",
      "Epoch 222, Train Loss: 0.8071010112762451, Test Loss: 0.8039553165435791\n",
      "Epoch 223, Train Loss: 0.805817723274231, Test Loss: 0.8025343418121338\n",
      "Epoch 224, Train Loss: 0.804547131061554, Test Loss: 0.8011269569396973\n",
      "Epoch 225, Train Loss: 0.8032893538475037, Test Loss: 0.799733579158783\n",
      "Epoch 226, Train Loss: 0.8020434975624084, Test Loss: 0.7983546257019043\n",
      "Epoch 227, Train Loss: 0.8008096218109131, Test Loss: 0.796988308429718\n",
      "Epoch 228, Train Loss: 0.7995879054069519, Test Loss: 0.7956352233886719\n",
      "Epoch 229, Train Loss: 0.7983779907226562, Test Loss: 0.7942951917648315\n",
      "Epoch 230, Train Loss: 0.7971795797348022, Test Loss: 0.7929697632789612\n",
      "Epoch 231, Train Loss: 0.7959926128387451, Test Loss: 0.7916574478149414\n",
      "Epoch 232, Train Loss: 0.7948183417320251, Test Loss: 0.7903600335121155\n",
      "Epoch 233, Train Loss: 0.7936543822288513, Test Loss: 0.7890755534172058\n",
      "Epoch 234, Train Loss: 0.792500913143158, Test Loss: 0.7878013253211975\n",
      "Epoch 235, Train Loss: 0.7913592457771301, Test Loss: 0.7865409851074219\n",
      "Epoch 236, Train Loss: 0.7902281284332275, Test Loss: 0.7852922677993774\n",
      "Epoch 237, Train Loss: 0.7891069650650024, Test Loss: 0.7840545773506165\n",
      "Epoch 238, Train Loss: 0.7879964709281921, Test Loss: 0.7828283309936523\n",
      "Epoch 239, Train Loss: 0.7868961095809937, Test Loss: 0.7816143035888672\n",
      "Epoch 240, Train Loss: 0.7858052849769592, Test Loss: 0.7804123759269714\n",
      "Epoch 241, Train Loss: 0.7847248911857605, Test Loss: 0.779219925403595\n",
      "Epoch 242, Train Loss: 0.7836533784866333, Test Loss: 0.7780379056930542\n",
      "Epoch 243, Train Loss: 0.7825921177864075, Test Loss: 0.7768692970275879\n",
      "Epoch 244, Train Loss: 0.78154057264328, Test Loss: 0.7757126092910767\n",
      "Epoch 245, Train Loss: 0.7804979681968689, Test Loss: 0.7745625376701355\n",
      "Epoch 246, Train Loss: 0.7794641256332397, Test Loss: 0.7734233736991882\n",
      "Epoch 247, Train Loss: 0.7784389853477478, Test Loss: 0.7722931504249573\n",
      "Epoch 248, Train Loss: 0.7774220108985901, Test Loss: 0.7711721658706665\n",
      "Epoch 249, Train Loss: 0.7764126062393188, Test Loss: 0.7700617909431458\n",
      "Epoch 250, Train Loss: 0.7754126191139221, Test Loss: 0.7689604163169861\n",
      "Epoch 251, Train Loss: 0.774420440196991, Test Loss: 0.7678692936897278\n",
      "Epoch 252, Train Loss: 0.7734361886978149, Test Loss: 0.7667872309684753\n",
      "Epoch 253, Train Loss: 0.772459089756012, Test Loss: 0.7657133936882019\n",
      "Epoch 254, Train Loss: 0.7714893817901611, Test Loss: 0.7646499872207642\n",
      "Epoch 255, Train Loss: 0.7705262899398804, Test Loss: 0.7635928392410278\n",
      "Epoch 256, Train Loss: 0.7695711851119995, Test Loss: 0.7625467777252197\n",
      "Epoch 257, Train Loss: 0.7686229348182678, Test Loss: 0.7615054845809937\n",
      "Epoch 258, Train Loss: 0.7676813006401062, Test Loss: 0.7604711651802063\n",
      "Epoch 259, Train Loss: 0.7667456269264221, Test Loss: 0.7594443559646606\n",
      "Epoch 260, Train Loss: 0.765816330909729, Test Loss: 0.7584285736083984\n",
      "Epoch 261, Train Loss: 0.7648929953575134, Test Loss: 0.7574197053909302\n",
      "Epoch 262, Train Loss: 0.7639752626419067, Test Loss: 0.756417453289032\n",
      "Epoch 263, Train Loss: 0.7630632519721985, Test Loss: 0.755420982837677\n",
      "Epoch 264, Train Loss: 0.7621573805809021, Test Loss: 0.7544341087341309\n",
      "Epoch 265, Train Loss: 0.7612574696540833, Test Loss: 0.7534531354904175\n",
      "Epoch 266, Train Loss: 0.760362446308136, Test Loss: 0.7524770498275757\n",
      "Epoch 267, Train Loss: 0.759473443031311, Test Loss: 0.7515104413032532\n",
      "Epoch 268, Train Loss: 0.7585898637771606, Test Loss: 0.7505489587783813\n",
      "Epoch 269, Train Loss: 0.757710874080658, Test Loss: 0.7495925426483154\n",
      "Epoch 270, Train Loss: 0.7568368315696716, Test Loss: 0.74864262342453\n",
      "Epoch 271, Train Loss: 0.7559679746627808, Test Loss: 0.7477002143859863\n",
      "Epoch 272, Train Loss: 0.7551039457321167, Test Loss: 0.7467637062072754\n",
      "Epoch 273, Train Loss: 0.7542451620101929, Test Loss: 0.7458329200744629\n",
      "Epoch 274, Train Loss: 0.7533909678459167, Test Loss: 0.7449089884757996\n",
      "Epoch 275, Train Loss: 0.7525416612625122, Test Loss: 0.7439894080162048\n",
      "Epoch 276, Train Loss: 0.7516967058181763, Test Loss: 0.7430739402770996\n",
      "Epoch 277, Train Loss: 0.7508566975593567, Test Loss: 0.7421649098396301\n",
      "Epoch 278, Train Loss: 0.7500205636024475, Test Loss: 0.7412600517272949\n",
      "Epoch 279, Train Loss: 0.7491894960403442, Test Loss: 0.7403638362884521\n",
      "Epoch 280, Train Loss: 0.7483634352684021, Test Loss: 0.7394731640815735\n",
      "Epoch 281, Train Loss: 0.7475418448448181, Test Loss: 0.738583505153656\n",
      "Epoch 282, Train Loss: 0.7467246055603027, Test Loss: 0.7377022504806519\n",
      "Epoch 283, Train Loss: 0.7459115982055664, Test Loss: 0.7368286848068237\n",
      "Epoch 284, Train Loss: 0.7451028227806091, Test Loss: 0.7359603643417358\n",
      "Epoch 285, Train Loss: 0.7442975044250488, Test Loss: 0.7350923418998718\n",
      "Epoch 286, Train Loss: 0.7434955835342407, Test Loss: 0.7342274785041809\n",
      "Epoch 287, Train Loss: 0.7426978945732117, Test Loss: 0.733368992805481\n",
      "Epoch 288, Train Loss: 0.7419054508209229, Test Loss: 0.7325135469436646\n",
      "Epoch 289, Train Loss: 0.7411165833473206, Test Loss: 0.7316629886627197\n",
      "Epoch 290, Train Loss: 0.7403314709663391, Test Loss: 0.7308158874511719\n",
      "Epoch 291, Train Loss: 0.7395503520965576, Test Loss: 0.7299797534942627\n",
      "Epoch 292, Train Loss: 0.738773763179779, Test Loss: 0.7291476130485535\n",
      "Epoch 293, Train Loss: 0.7380005121231079, Test Loss: 0.7283180356025696\n",
      "Epoch 294, Train Loss: 0.7372300028800964, Test Loss: 0.7274870872497559\n",
      "Epoch 295, Train Loss: 0.7364635467529297, Test Loss: 0.7266662120819092\n",
      "Epoch 296, Train Loss: 0.7356998920440674, Test Loss: 0.7258475422859192\n",
      "Epoch 297, Train Loss: 0.7349400520324707, Test Loss: 0.7250317931175232\n",
      "Epoch 298, Train Loss: 0.734183669090271, Test Loss: 0.7242228388786316\n",
      "Epoch 299, Train Loss: 0.7334303259849548, Test Loss: 0.7234180569648743\n",
      "Epoch 300, Train Loss: 0.7326809167861938, Test Loss: 0.7226163744926453\n",
      "Epoch 301, Train Loss: 0.7319353818893433, Test Loss: 0.7218193411827087\n",
      "Epoch 302, Train Loss: 0.7311930060386658, Test Loss: 0.7210264205932617\n",
      "Epoch 303, Train Loss: 0.7304544448852539, Test Loss: 0.7202356457710266\n",
      "Epoch 304, Train Loss: 0.7297202348709106, Test Loss: 0.7194495797157288\n",
      "Epoch 305, Train Loss: 0.7289895415306091, Test Loss: 0.7186712026596069\n",
      "Epoch 306, Train Loss: 0.7282616496086121, Test Loss: 0.7178983092308044\n",
      "Epoch 307, Train Loss: 0.7275381088256836, Test Loss: 0.7171279191970825\n",
      "Epoch 308, Train Loss: 0.726818859577179, Test Loss: 0.7163610458374023\n",
      "Epoch 309, Train Loss: 0.7261028289794922, Test Loss: 0.715599536895752\n",
      "Epoch 310, Train Loss: 0.7253910303115845, Test Loss: 0.7148419618606567\n",
      "Epoch 311, Train Loss: 0.7246828675270081, Test Loss: 0.7140859365463257\n",
      "Epoch 312, Train Loss: 0.7239777445793152, Test Loss: 0.7133339047431946\n",
      "Epoch 313, Train Loss: 0.7232756018638611, Test Loss: 0.7125857472419739\n",
      "Epoch 314, Train Loss: 0.7225761413574219, Test Loss: 0.7118420600891113\n",
      "Epoch 315, Train Loss: 0.7218800187110901, Test Loss: 0.7111063599586487\n",
      "Epoch 316, Train Loss: 0.7211880683898926, Test Loss: 0.7103736996650696\n",
      "Epoch 317, Train Loss: 0.720499575138092, Test Loss: 0.7096465229988098\n",
      "Epoch 318, Train Loss: 0.719814121723175, Test Loss: 0.7089253067970276\n",
      "Epoch 319, Train Loss: 0.719132125377655, Test Loss: 0.7082026600837708\n",
      "Epoch 320, Train Loss: 0.7184540629386902, Test Loss: 0.7074849009513855\n",
      "Epoch 321, Train Loss: 0.7177797555923462, Test Loss: 0.7067716717720032\n",
      "Epoch 322, Train Loss: 0.7171087861061096, Test Loss: 0.706062912940979\n",
      "Epoch 323, Train Loss: 0.7164412140846252, Test Loss: 0.7053590416908264\n",
      "Epoch 324, Train Loss: 0.7157777547836304, Test Loss: 0.7046594023704529\n",
      "Epoch 325, Train Loss: 0.715118408203125, Test Loss: 0.7039620876312256\n",
      "Epoch 326, Train Loss: 0.7144624590873718, Test Loss: 0.7032711505889893\n",
      "Epoch 327, Train Loss: 0.7138102650642395, Test Loss: 0.7025858759880066\n",
      "Epoch 328, Train Loss: 0.713161826133728, Test Loss: 0.7019013166427612\n",
      "Epoch 329, Train Loss: 0.7125171422958374, Test Loss: 0.7012225389480591\n",
      "Epoch 330, Train Loss: 0.711876630783081, Test Loss: 0.7005441188812256\n",
      "Epoch 331, Train Loss: 0.7112401723861694, Test Loss: 0.6998728513717651\n",
      "Epoch 332, Train Loss: 0.7106079459190369, Test Loss: 0.6992082595825195\n",
      "Epoch 333, Train Loss: 0.7099797129631042, Test Loss: 0.6985458135604858\n",
      "Epoch 334, Train Loss: 0.7093552350997925, Test Loss: 0.6978875994682312\n",
      "Epoch 335, Train Loss: 0.7087342739105225, Test Loss: 0.697231113910675\n",
      "Epoch 336, Train Loss: 0.7081164717674255, Test Loss: 0.696580708026886\n",
      "Epoch 337, Train Loss: 0.7075024843215942, Test Loss: 0.695935070514679\n",
      "Epoch 338, Train Loss: 0.7068922519683838, Test Loss: 0.6952939629554749\n",
      "Epoch 339, Train Loss: 0.7062856554985046, Test Loss: 0.6946588754653931\n",
      "Epoch 340, Train Loss: 0.7056829929351807, Test Loss: 0.6940246224403381\n",
      "Epoch 341, Train Loss: 0.7050837874412537, Test Loss: 0.6933953166007996\n",
      "Epoch 342, Train Loss: 0.7044886350631714, Test Loss: 0.6927685737609863\n",
      "Epoch 343, Train Loss: 0.7038975954055786, Test Loss: 0.6921480894088745\n",
      "Epoch 344, Train Loss: 0.7033104300498962, Test Loss: 0.691525936126709\n",
      "Epoch 345, Train Loss: 0.7027270197868347, Test Loss: 0.6909105181694031\n",
      "Epoch 346, Train Loss: 0.7021467685699463, Test Loss: 0.6903008222579956\n",
      "Epoch 347, Train Loss: 0.7015698552131653, Test Loss: 0.6896962523460388\n",
      "Epoch 348, Train Loss: 0.7009967565536499, Test Loss: 0.6890918612480164\n",
      "Epoch 349, Train Loss: 0.7004273533821106, Test Loss: 0.688489556312561\n",
      "Epoch 350, Train Loss: 0.6998615264892578, Test Loss: 0.6878954172134399\n",
      "Epoch 351, Train Loss: 0.699299693107605, Test Loss: 0.6873051524162292\n",
      "Epoch 352, Train Loss: 0.6987413763999939, Test Loss: 0.6867183446884155\n",
      "Epoch 353, Train Loss: 0.6981871724128723, Test Loss: 0.6861364245414734\n",
      "Epoch 354, Train Loss: 0.697636604309082, Test Loss: 0.6855575442314148\n",
      "Epoch 355, Train Loss: 0.6970892548561096, Test Loss: 0.6849794983863831\n",
      "Epoch 356, Train Loss: 0.6965456604957581, Test Loss: 0.6844049692153931\n",
      "Epoch 357, Train Loss: 0.6960057020187378, Test Loss: 0.6838344931602478\n",
      "Epoch 358, Train Loss: 0.6954699158668518, Test Loss: 0.6832717657089233\n",
      "Epoch 359, Train Loss: 0.6949377059936523, Test Loss: 0.6827149987220764\n",
      "Epoch 360, Train Loss: 0.6944087147712708, Test Loss: 0.682159423828125\n",
      "Epoch 361, Train Loss: 0.693882942199707, Test Loss: 0.6816073656082153\n",
      "Epoch 362, Train Loss: 0.6933613419532776, Test Loss: 0.6810563802719116\n",
      "Epoch 363, Train Loss: 0.6928434371948242, Test Loss: 0.6805117726325989\n",
      "Epoch 364, Train Loss: 0.6923294067382812, Test Loss: 0.6799675822257996\n",
      "Epoch 365, Train Loss: 0.69181889295578, Test Loss: 0.6794283986091614\n",
      "Epoch 366, Train Loss: 0.6913118958473206, Test Loss: 0.678894579410553\n",
      "Epoch 367, Train Loss: 0.6908083558082581, Test Loss: 0.6783658862113953\n",
      "Epoch 368, Train Loss: 0.6903082132339478, Test Loss: 0.6778405904769897\n",
      "Epoch 369, Train Loss: 0.6898115277290344, Test Loss: 0.6773189902305603\n",
      "Epoch 370, Train Loss: 0.6893184185028076, Test Loss: 0.67679762840271\n",
      "Epoch 371, Train Loss: 0.6888284087181091, Test Loss: 0.6762861013412476\n",
      "Epoch 372, Train Loss: 0.6883415579795837, Test Loss: 0.6757728457450867\n",
      "Epoch 373, Train Loss: 0.6878585815429688, Test Loss: 0.6752658486366272\n",
      "Epoch 374, Train Loss: 0.6873788833618164, Test Loss: 0.6747621297836304\n",
      "Epoch 375, Train Loss: 0.6869025230407715, Test Loss: 0.6742623448371887\n",
      "Epoch 376, Train Loss: 0.6864295601844788, Test Loss: 0.6737669706344604\n",
      "Epoch 377, Train Loss: 0.6859595775604248, Test Loss: 0.6732697486877441\n",
      "Epoch 378, Train Loss: 0.6854926347732544, Test Loss: 0.67277991771698\n",
      "Epoch 379, Train Loss: 0.6850290894508362, Test Loss: 0.6722967624664307\n",
      "Epoch 380, Train Loss: 0.6845684051513672, Test Loss: 0.6718112826347351\n",
      "Epoch 381, Train Loss: 0.6841106414794922, Test Loss: 0.6713328957557678\n",
      "Epoch 382, Train Loss: 0.6836561560630798, Test Loss: 0.6708530187606812\n",
      "Epoch 383, Train Loss: 0.683204174041748, Test Loss: 0.670376718044281\n",
      "Epoch 384, Train Loss: 0.6827552914619446, Test Loss: 0.6699056625366211\n",
      "Epoch 385, Train Loss: 0.682309627532959, Test Loss: 0.6694374084472656\n",
      "Epoch 386, Train Loss: 0.6818665266036987, Test Loss: 0.6689731478691101\n",
      "Epoch 387, Train Loss: 0.6814266443252563, Test Loss: 0.6685087084770203\n",
      "Epoch 388, Train Loss: 0.6809899210929871, Test Loss: 0.668048083782196\n",
      "Epoch 389, Train Loss: 0.6805561780929565, Test Loss: 0.6675921678543091\n",
      "Epoch 390, Train Loss: 0.6801254153251648, Test Loss: 0.667141854763031\n",
      "Epoch 391, Train Loss: 0.6796980500221252, Test Loss: 0.6666876077651978\n",
      "Epoch 392, Train Loss: 0.6792734861373901, Test Loss: 0.6662431359291077\n",
      "Epoch 393, Train Loss: 0.6788517236709595, Test Loss: 0.6658014059066772\n",
      "Epoch 394, Train Loss: 0.6784330606460571, Test Loss: 0.6653625965118408\n",
      "Epoch 395, Train Loss: 0.6780174970626831, Test Loss: 0.664925754070282\n",
      "Epoch 396, Train Loss: 0.6776047945022583, Test Loss: 0.6644917130470276\n",
      "Epoch 397, Train Loss: 0.6771949529647827, Test Loss: 0.6640620231628418\n",
      "Epoch 398, Train Loss: 0.6767881512641907, Test Loss: 0.6636266708374023\n",
      "Epoch 399, Train Loss: 0.676384449005127, Test Loss: 0.6632043123245239\n",
      "Epoch 400, Train Loss: 0.6759834885597229, Test Loss: 0.6627820730209351\n",
      "Epoch 401, Train Loss: 0.6755858063697815, Test Loss: 0.6623649597167969\n",
      "Epoch 402, Train Loss: 0.6751909255981445, Test Loss: 0.6619511246681213\n",
      "Epoch 403, Train Loss: 0.6747985482215881, Test Loss: 0.6615390777587891\n",
      "Epoch 404, Train Loss: 0.6744086742401123, Test Loss: 0.6611290574073792\n",
      "Epoch 405, Train Loss: 0.6740212440490723, Test Loss: 0.6607261300086975\n",
      "Epoch 406, Train Loss: 0.6736364364624023, Test Loss: 0.6603257060050964\n",
      "Epoch 407, Train Loss: 0.673254668712616, Test Loss: 0.6599305868148804\n",
      "Epoch 408, Train Loss: 0.672875165939331, Test Loss: 0.6595324873924255\n",
      "Epoch 409, Train Loss: 0.6724984645843506, Test Loss: 0.6591379642486572\n",
      "Epoch 410, Train Loss: 0.672124445438385, Test Loss: 0.6587451100349426\n",
      "Epoch 411, Train Loss: 0.6717526912689209, Test Loss: 0.6583588719367981\n",
      "Epoch 412, Train Loss: 0.6713836789131165, Test Loss: 0.6579678654670715\n",
      "Epoch 413, Train Loss: 0.6710174083709717, Test Loss: 0.6575868129730225\n",
      "Epoch 414, Train Loss: 0.670653760433197, Test Loss: 0.6572068333625793\n",
      "Epoch 415, Train Loss: 0.6702926158905029, Test Loss: 0.6568271517753601\n",
      "Epoch 416, Train Loss: 0.6699337363243103, Test Loss: 0.6564503908157349\n",
      "Epoch 417, Train Loss: 0.6695771813392639, Test Loss: 0.656072199344635\n",
      "Epoch 418, Train Loss: 0.6692227125167847, Test Loss: 0.655701756477356\n",
      "Epoch 419, Train Loss: 0.6688703894615173, Test Loss: 0.6553373336791992\n",
      "Epoch 420, Train Loss: 0.6685203909873962, Test Loss: 0.6549713015556335\n",
      "Epoch 421, Train Loss: 0.6681724786758423, Test Loss: 0.6546063423156738\n",
      "Epoch 422, Train Loss: 0.6678267121315002, Test Loss: 0.6542437076568604\n",
      "Epoch 423, Train Loss: 0.6674827933311462, Test Loss: 0.6538860201835632\n",
      "Epoch 424, Train Loss: 0.667141318321228, Test Loss: 0.6535290479660034\n",
      "Epoch 425, Train Loss: 0.666802167892456, Test Loss: 0.6531804800033569\n",
      "Epoch 426, Train Loss: 0.6664651036262512, Test Loss: 0.6528314352035522\n",
      "Epoch 427, Train Loss: 0.6661302447319031, Test Loss: 0.6524847745895386\n",
      "Epoch 428, Train Loss: 0.6657972931861877, Test Loss: 0.6521430611610413\n",
      "Epoch 429, Train Loss: 0.665466845035553, Test Loss: 0.6517938375473022\n",
      "Epoch 430, Train Loss: 0.6651385426521301, Test Loss: 0.6514481902122498\n",
      "Epoch 431, Train Loss: 0.664812445640564, Test Loss: 0.6511145234107971\n",
      "Epoch 432, Train Loss: 0.664488673210144, Test Loss: 0.6507751941680908\n",
      "Epoch 433, Train Loss: 0.6641666293144226, Test Loss: 0.6504384875297546\n",
      "Epoch 434, Train Loss: 0.6638465523719788, Test Loss: 0.650104284286499\n",
      "Epoch 435, Train Loss: 0.6635285019874573, Test Loss: 0.6497740149497986\n",
      "Epoch 436, Train Loss: 0.6632124185562134, Test Loss: 0.649444043636322\n",
      "Epoch 437, Train Loss: 0.6628987193107605, Test Loss: 0.6491209864616394\n",
      "Epoch 438, Train Loss: 0.6625868678092957, Test Loss: 0.6487982273101807\n",
      "Epoch 439, Train Loss: 0.6622772812843323, Test Loss: 0.6484802961349487\n",
      "Epoch 440, Train Loss: 0.6619693636894226, Test Loss: 0.648166298866272\n",
      "Epoch 441, Train Loss: 0.6616633534431458, Test Loss: 0.6478531956672668\n",
      "Epoch 442, Train Loss: 0.6613589525222778, Test Loss: 0.6475406885147095\n",
      "Epoch 443, Train Loss: 0.6610566973686218, Test Loss: 0.6472241282463074\n",
      "Epoch 444, Train Loss: 0.6607563495635986, Test Loss: 0.6469148397445679\n",
      "Epoch 445, Train Loss: 0.6604578495025635, Test Loss: 0.6466119885444641\n",
      "Epoch 446, Train Loss: 0.6601611375808716, Test Loss: 0.6463082432746887\n",
      "Epoch 447, Train Loss: 0.6598660349845886, Test Loss: 0.6460115313529968\n",
      "Epoch 448, Train Loss: 0.6595727801322937, Test Loss: 0.645709753036499\n",
      "Epoch 449, Train Loss: 0.6592812538146973, Test Loss: 0.6454122066497803\n",
      "Epoch 450, Train Loss: 0.6589916348457336, Test Loss: 0.6451124548912048\n",
      "Epoch 451, Train Loss: 0.6587035059928894, Test Loss: 0.644818127155304\n",
      "Epoch 452, Train Loss: 0.6584168672561646, Test Loss: 0.6445247530937195\n",
      "Epoch 453, Train Loss: 0.658132016658783, Test Loss: 0.6442341804504395\n",
      "Epoch 454, Train Loss: 0.6578486561775208, Test Loss: 0.6439477205276489\n",
      "Epoch 455, Train Loss: 0.6575669050216675, Test Loss: 0.6436554193496704\n",
      "Epoch 456, Train Loss: 0.6572868824005127, Test Loss: 0.6433718800544739\n",
      "Epoch 457, Train Loss: 0.6570085883140564, Test Loss: 0.6430888772010803\n",
      "Epoch 458, Train Loss: 0.6567317247390747, Test Loss: 0.6428073644638062\n",
      "Epoch 459, Train Loss: 0.6564563512802124, Test Loss: 0.6425290107727051\n",
      "Epoch 460, Train Loss: 0.6561824083328247, Test Loss: 0.6422524452209473\n",
      "Epoch 461, Train Loss: 0.6559099555015564, Test Loss: 0.6419760584831238\n",
      "Epoch 462, Train Loss: 0.6556389927864075, Test Loss: 0.641696572303772\n",
      "Epoch 463, Train Loss: 0.6553696393966675, Test Loss: 0.641424298286438\n",
      "Epoch 464, Train Loss: 0.6551017165184021, Test Loss: 0.6411484479904175\n",
      "Epoch 465, Train Loss: 0.6548349261283875, Test Loss: 0.6408805847167969\n",
      "Epoch 466, Train Loss: 0.6545695662498474, Test Loss: 0.6406176090240479\n",
      "Epoch 467, Train Loss: 0.6543056964874268, Test Loss: 0.6403501033782959\n",
      "Epoch 468, Train Loss: 0.6540434956550598, Test Loss: 0.6400821805000305\n",
      "Epoch 469, Train Loss: 0.6537826061248779, Test Loss: 0.6398183703422546\n",
      "Epoch 470, Train Loss: 0.653523325920105, Test Loss: 0.6395599842071533\n",
      "Epoch 471, Train Loss: 0.6532654762268066, Test Loss: 0.6393019556999207\n",
      "Epoch 472, Train Loss: 0.6530090570449829, Test Loss: 0.6390407681465149\n",
      "Epoch 473, Train Loss: 0.6527540683746338, Test Loss: 0.6387841105461121\n",
      "Epoch 474, Train Loss: 0.6525005102157593, Test Loss: 0.6385325789451599\n",
      "Epoch 475, Train Loss: 0.6522483825683594, Test Loss: 0.6382765173912048\n",
      "Epoch 476, Train Loss: 0.6519976854324341, Test Loss: 0.6380239129066467\n",
      "Epoch 477, Train Loss: 0.6517482995986938, Test Loss: 0.6377750635147095\n",
      "Epoch 478, Train Loss: 0.6515001654624939, Test Loss: 0.6375178098678589\n",
      "Epoch 479, Train Loss: 0.651253342628479, Test Loss: 0.6372719407081604\n",
      "Epoch 480, Train Loss: 0.6510077118873596, Test Loss: 0.6370291113853455\n",
      "Epoch 481, Train Loss: 0.6507633328437805, Test Loss: 0.6367867588996887\n",
      "Epoch 482, Train Loss: 0.6505200862884521, Test Loss: 0.6365360617637634\n",
      "Epoch 483, Train Loss: 0.6502782106399536, Test Loss: 0.6362954378128052\n",
      "Epoch 484, Train Loss: 0.6500376462936401, Test Loss: 0.6360570192337036\n",
      "Epoch 485, Train Loss: 0.6497980952262878, Test Loss: 0.6358141303062439\n",
      "Epoch 486, Train Loss: 0.649559736251831, Test Loss: 0.6355704069137573\n",
      "Epoch 487, Train Loss: 0.6493223905563354, Test Loss: 0.635330319404602\n",
      "Epoch 488, Train Loss: 0.6490862369537354, Test Loss: 0.6350964903831482\n",
      "Epoch 489, Train Loss: 0.6488509774208069, Test Loss: 0.6348615288734436\n",
      "Epoch 490, Train Loss: 0.6486169099807739, Test Loss: 0.6346259117126465\n",
      "Epoch 491, Train Loss: 0.6483836770057678, Test Loss: 0.6343980431556702\n",
      "Epoch 492, Train Loss: 0.6481516361236572, Test Loss: 0.6341692805290222\n",
      "Epoch 493, Train Loss: 0.6479203104972839, Test Loss: 0.6339380145072937\n",
      "Epoch 494, Train Loss: 0.6476901173591614, Test Loss: 0.6337060928344727\n",
      "Epoch 495, Train Loss: 0.6474606990814209, Test Loss: 0.6334822177886963\n",
      "Epoch 496, Train Loss: 0.6472322940826416, Test Loss: 0.6332556009292603\n",
      "Epoch 497, Train Loss: 0.6470046639442444, Test Loss: 0.63303142786026\n",
      "Epoch 498, Train Loss: 0.6467780470848083, Test Loss: 0.6328057050704956\n",
      "Epoch 499, Train Loss: 0.6465523838996887, Test Loss: 0.6325806975364685\n",
      "Epoch 500, Train Loss: 0.646327793598175, Test Loss: 0.6323526501655579\n",
      "Epoch 501, Train Loss: 0.6461037993431091, Test Loss: 0.6321301460266113\n",
      "Epoch 502, Train Loss: 0.6458808779716492, Test Loss: 0.6319084763526917\n",
      "Epoch 503, Train Loss: 0.6456589102745056, Test Loss: 0.6316900253295898\n",
      "Epoch 504, Train Loss: 0.6454378962516785, Test Loss: 0.6314712166786194\n",
      "Epoch 505, Train Loss: 0.6452175378799438, Test Loss: 0.6312496662139893\n",
      "Epoch 506, Train Loss: 0.6449979543685913, Test Loss: 0.6310416460037231\n",
      "Epoch 507, Train Loss: 0.6447792053222656, Test Loss: 0.6308241486549377\n",
      "Epoch 508, Train Loss: 0.6445612907409668, Test Loss: 0.6306098699569702\n",
      "Epoch 509, Train Loss: 0.6443442106246948, Test Loss: 0.6303910613059998\n",
      "Epoch 510, Train Loss: 0.6441278457641602, Test Loss: 0.6301815509796143\n",
      "Epoch 511, Train Loss: 0.6439122557640076, Test Loss: 0.6299700140953064\n",
      "Epoch 512, Train Loss: 0.6436974406242371, Test Loss: 0.6297567486763\n",
      "Epoch 513, Train Loss: 0.6434831619262695, Test Loss: 0.6295456886291504\n",
      "Epoch 514, Train Loss: 0.6432695388793945, Test Loss: 0.6293390989303589\n",
      "Epoch 515, Train Loss: 0.643056333065033, Test Loss: 0.6291218400001526\n",
      "Epoch 516, Train Loss: 0.6428439021110535, Test Loss: 0.6289119124412537\n",
      "Epoch 517, Train Loss: 0.6426321268081665, Test Loss: 0.6287026405334473\n",
      "Epoch 518, Train Loss: 0.6424209475517273, Test Loss: 0.6284914612770081\n",
      "Epoch 519, Train Loss: 0.6422104239463806, Test Loss: 0.6282843351364136\n",
      "Epoch 520, Train Loss: 0.6420006155967712, Test Loss: 0.6280783414840698\n",
      "Epoch 521, Train Loss: 0.6417914032936096, Test Loss: 0.6278789639472961\n",
      "Epoch 522, Train Loss: 0.6415826678276062, Test Loss: 0.6276726126670837\n",
      "Epoch 523, Train Loss: 0.6413744688034058, Test Loss: 0.6274713277816772\n",
      "Epoch 524, Train Loss: 0.6411668062210083, Test Loss: 0.6272677779197693\n",
      "Epoch 525, Train Loss: 0.6409597396850586, Test Loss: 0.6270627975463867\n",
      "Epoch 526, Train Loss: 0.6407533288002014, Test Loss: 0.626862108707428\n",
      "Epoch 527, Train Loss: 0.6405476927757263, Test Loss: 0.6266665458679199\n",
      "Epoch 528, Train Loss: 0.6403425335884094, Test Loss: 0.6264708638191223\n",
      "Epoch 529, Train Loss: 0.6401379704475403, Test Loss: 0.6262712478637695\n",
      "Epoch 530, Train Loss: 0.6399340033531189, Test Loss: 0.6260713338851929\n",
      "Epoch 531, Train Loss: 0.6397306323051453, Test Loss: 0.6258796453475952\n",
      "Epoch 532, Train Loss: 0.6395276784896851, Test Loss: 0.6256813406944275\n",
      "Epoch 533, Train Loss: 0.6393252015113831, Test Loss: 0.6254927515983582\n",
      "Epoch 534, Train Loss: 0.6391231417655945, Test Loss: 0.6253013014793396\n",
      "Epoch 535, Train Loss: 0.6389217972755432, Test Loss: 0.625103771686554\n",
      "Epoch 536, Train Loss: 0.6387209892272949, Test Loss: 0.6249146461486816\n",
      "Epoch 537, Train Loss: 0.6385205388069153, Test Loss: 0.6247289180755615\n",
      "Epoch 538, Train Loss: 0.6383204460144043, Test Loss: 0.6245366930961609\n",
      "Epoch 539, Train Loss: 0.6381208300590515, Test Loss: 0.6243454813957214\n",
      "Epoch 540, Train Loss: 0.6379218101501465, Test Loss: 0.6241546869277954\n",
      "Epoch 541, Train Loss: 0.6377231478691101, Test Loss: 0.6239702701568604\n",
      "Epoch 542, Train Loss: 0.6375250816345215, Test Loss: 0.6237828731536865\n",
      "Epoch 543, Train Loss: 0.6373274326324463, Test Loss: 0.6235877871513367\n",
      "Epoch 544, Train Loss: 0.6371301412582397, Test Loss: 0.6234051585197449\n",
      "Epoch 545, Train Loss: 0.6369332671165466, Test Loss: 0.6232178807258606\n",
      "Epoch 546, Train Loss: 0.6367366313934326, Test Loss: 0.6230320930480957\n",
      "Epoch 547, Train Loss: 0.6365402936935425, Test Loss: 0.622841477394104\n",
      "Epoch 548, Train Loss: 0.6363443732261658, Test Loss: 0.6226508617401123\n",
      "Epoch 549, Train Loss: 0.6361488103866577, Test Loss: 0.622465193271637\n",
      "Epoch 550, Train Loss: 0.6359536051750183, Test Loss: 0.6222802400588989\n",
      "Epoch 551, Train Loss: 0.6357588171958923, Test Loss: 0.6220949292182922\n",
      "Epoch 552, Train Loss: 0.635564386844635, Test Loss: 0.6219110488891602\n",
      "Epoch 553, Train Loss: 0.6353703737258911, Test Loss: 0.6217320561408997\n",
      "Epoch 554, Train Loss: 0.6351766586303711, Test Loss: 0.6215423941612244\n",
      "Epoch 555, Train Loss: 0.6349833607673645, Test Loss: 0.6213558912277222\n",
      "Epoch 556, Train Loss: 0.634790301322937, Test Loss: 0.6211786270141602\n",
      "Epoch 557, Train Loss: 0.6345975399017334, Test Loss: 0.6210013628005981\n",
      "Epoch 558, Train Loss: 0.6344053149223328, Test Loss: 0.6208173632621765\n",
      "Epoch 559, Train Loss: 0.6342132687568665, Test Loss: 0.6206384301185608\n",
      "Epoch 560, Train Loss: 0.6340216398239136, Test Loss: 0.6204585433006287\n",
      "Epoch 561, Train Loss: 0.6338303089141846, Test Loss: 0.6202771067619324\n",
      "Epoch 562, Train Loss: 0.6336392164230347, Test Loss: 0.6200938820838928\n",
      "Epoch 563, Train Loss: 0.6334484219551086, Test Loss: 0.6199220418930054\n",
      "Epoch 564, Train Loss: 0.633258044719696, Test Loss: 0.6197381615638733\n",
      "Epoch 565, Train Loss: 0.6330679059028625, Test Loss: 0.6195735335350037\n",
      "Epoch 566, Train Loss: 0.6328781843185425, Test Loss: 0.6193928718566895\n",
      "Epoch 567, Train Loss: 0.6326888799667358, Test Loss: 0.6192217469215393\n",
      "Epoch 568, Train Loss: 0.6324998140335083, Test Loss: 0.6190471649169922\n",
      "Epoch 569, Train Loss: 0.6323109865188599, Test Loss: 0.6188674569129944\n",
      "Epoch 570, Train Loss: 0.6321223974227905, Test Loss: 0.6186923384666443\n",
      "Epoch 571, Train Loss: 0.6319340467453003, Test Loss: 0.6185215711593628\n",
      "Epoch 572, Train Loss: 0.6317458748817444, Test Loss: 0.6183456182479858\n",
      "Epoch 573, Train Loss: 0.6315581202507019, Test Loss: 0.6181649565696716\n",
      "Epoch 574, Train Loss: 0.6313706636428833, Test Loss: 0.6179955005645752\n",
      "Epoch 575, Train Loss: 0.6311831474304199, Test Loss: 0.61781245470047\n",
      "Epoch 576, Train Loss: 0.6309959888458252, Test Loss: 0.6176380515098572\n",
      "Epoch 577, Train Loss: 0.6308092474937439, Test Loss: 0.617470383644104\n",
      "Epoch 578, Train Loss: 0.6306226849555969, Test Loss: 0.6173030734062195\n",
      "Epoch 579, Train Loss: 0.6304363012313843, Test Loss: 0.6171335577964783\n",
      "Epoch 580, Train Loss: 0.6302502155303955, Test Loss: 0.6169589161872864\n",
      "Epoch 581, Train Loss: 0.6300643682479858, Test Loss: 0.6167739033699036\n",
      "Epoch 582, Train Loss: 0.629878580570221, Test Loss: 0.6166043877601624\n",
      "Epoch 583, Train Loss: 0.6296930909156799, Test Loss: 0.6164304614067078\n",
      "Epoch 584, Train Loss: 0.6295077204704285, Test Loss: 0.6162557601928711\n",
      "Epoch 585, Train Loss: 0.6293225288391113, Test Loss: 0.6160776615142822\n",
      "Epoch 586, Train Loss: 0.6291374564170837, Test Loss: 0.615902841091156\n",
      "Epoch 587, Train Loss: 0.6289525628089905, Test Loss: 0.6157289147377014\n",
      "Epoch 588, Train Loss: 0.6287678480148315, Test Loss: 0.6155489087104797\n",
      "Epoch 589, Train Loss: 0.6285830140113831, Test Loss: 0.6153834462165833\n",
      "Epoch 590, Train Loss: 0.6283983588218689, Test Loss: 0.6152194738388062\n",
      "Epoch 591, Train Loss: 0.6282137632369995, Test Loss: 0.6150587797164917\n",
      "Epoch 592, Train Loss: 0.6280294060707092, Test Loss: 0.6148837804794312\n",
      "Epoch 593, Train Loss: 0.6278449892997742, Test Loss: 0.6147135496139526\n",
      "Epoch 594, Train Loss: 0.6276609301567078, Test Loss: 0.6145405769348145\n",
      "Epoch 595, Train Loss: 0.6274768710136414, Test Loss: 0.6143735647201538\n",
      "Epoch 596, Train Loss: 0.6272929310798645, Test Loss: 0.6142082810401917\n",
      "Epoch 597, Train Loss: 0.6271090507507324, Test Loss: 0.6140319108963013\n",
      "Epoch 598, Train Loss: 0.6269253492355347, Test Loss: 0.6138738989830017\n",
      "Epoch 599, Train Loss: 0.6267417073249817, Test Loss: 0.6137052178382874\n",
      "Epoch 600, Train Loss: 0.6265581846237183, Test Loss: 0.6135358214378357\n",
      "Epoch 601, Train Loss: 0.6263746619224548, Test Loss: 0.6133712530136108\n",
      "Epoch 602, Train Loss: 0.626191258430481, Test Loss: 0.6132044792175293\n",
      "Epoch 603, Train Loss: 0.6260079741477966, Test Loss: 0.6130335927009583\n",
      "Epoch 604, Train Loss: 0.6258248686790466, Test Loss: 0.6128560304641724\n",
      "Epoch 605, Train Loss: 0.6256417036056519, Test Loss: 0.612693727016449\n",
      "Epoch 606, Train Loss: 0.6254585385322571, Test Loss: 0.612530529499054\n",
      "Epoch 607, Train Loss: 0.6252754330635071, Test Loss: 0.6123602986335754\n",
      "Epoch 608, Train Loss: 0.6250923275947571, Test Loss: 0.6122000813484192\n",
      "Epoch 609, Train Loss: 0.6249092817306519, Test Loss: 0.6120365262031555\n",
      "Epoch 610, Train Loss: 0.6247261166572571, Test Loss: 0.6118708252906799\n",
      "Epoch 611, Train Loss: 0.6245430111885071, Test Loss: 0.6117092370986938\n",
      "Epoch 612, Train Loss: 0.6243599653244019, Test Loss: 0.6115489602088928\n",
      "Epoch 613, Train Loss: 0.6241768598556519, Test Loss: 0.6113835573196411\n",
      "Epoch 614, Train Loss: 0.6239937543869019, Test Loss: 0.6112096309661865\n",
      "Epoch 615, Train Loss: 0.6238107681274414, Test Loss: 0.6110422611236572\n",
      "Epoch 616, Train Loss: 0.6236278414726257, Test Loss: 0.6108775734901428\n",
      "Epoch 617, Train Loss: 0.6234449744224548, Test Loss: 0.6107088327407837\n",
      "Epoch 618, Train Loss: 0.6232621073722839, Test Loss: 0.6105421781539917\n",
      "Epoch 619, Train Loss: 0.6230792999267578, Test Loss: 0.6103740930557251\n",
      "Epoch 620, Train Loss: 0.6228963732719421, Test Loss: 0.6102079153060913\n",
      "Epoch 621, Train Loss: 0.6227133870124817, Test Loss: 0.6100383996963501\n",
      "Epoch 622, Train Loss: 0.6225304007530212, Test Loss: 0.6098729372024536\n",
      "Epoch 623, Train Loss: 0.6223474144935608, Test Loss: 0.6097140908241272\n",
      "Epoch 624, Train Loss: 0.6221644878387451, Test Loss: 0.6095476746559143\n",
      "Epoch 625, Train Loss: 0.6219813823699951, Test Loss: 0.6093787550926208\n",
      "Epoch 626, Train Loss: 0.6217982769012451, Test Loss: 0.6092206835746765\n",
      "Epoch 627, Train Loss: 0.6216149926185608, Test Loss: 0.6090458035469055\n",
      "Epoch 628, Train Loss: 0.6214317083358765, Test Loss: 0.6088799238204956\n",
      "Epoch 629, Train Loss: 0.6212484240531921, Test Loss: 0.6087145209312439\n",
      "Epoch 630, Train Loss: 0.6210651993751526, Test Loss: 0.6085549592971802\n",
      "Epoch 631, Train Loss: 0.620881974697113, Test Loss: 0.6083835959434509\n",
      "Epoch 632, Train Loss: 0.620698869228363, Test Loss: 0.6082160472869873\n",
      "Epoch 633, Train Loss: 0.6205155849456787, Test Loss: 0.6080557107925415\n",
      "Epoch 634, Train Loss: 0.6203323006629944, Test Loss: 0.6078904271125793\n",
      "Epoch 635, Train Loss: 0.6201491355895996, Test Loss: 0.6077260971069336\n",
      "Epoch 636, Train Loss: 0.6199658513069153, Test Loss: 0.6075732111930847\n",
      "Epoch 637, Train Loss: 0.619782567024231, Test Loss: 0.607405960559845\n",
      "Epoch 638, Train Loss: 0.6195992827415466, Test Loss: 0.6072496175765991\n",
      "Epoch 639, Train Loss: 0.6194158792495728, Test Loss: 0.6070781946182251\n",
      "Epoch 640, Train Loss: 0.6192324757575989, Test Loss: 0.6069168448448181\n",
      "Epoch 641, Train Loss: 0.6190489530563354, Test Loss: 0.606749415397644\n",
      "Epoch 642, Train Loss: 0.6188653707504272, Test Loss: 0.6065784096717834\n",
      "Epoch 643, Train Loss: 0.618681788444519, Test Loss: 0.6064030528068542\n",
      "Epoch 644, Train Loss: 0.6184980869293213, Test Loss: 0.6062294244766235\n",
      "Epoch 645, Train Loss: 0.6183143854141235, Test Loss: 0.6060608625411987\n",
      "Epoch 646, Train Loss: 0.6181303262710571, Test Loss: 0.6058973073959351\n",
      "Epoch 647, Train Loss: 0.6179463267326355, Test Loss: 0.6057302355766296\n",
      "Epoch 648, Train Loss: 0.6177622079849243, Test Loss: 0.6055684089660645\n",
      "Epoch 649, Train Loss: 0.6175779700279236, Test Loss: 0.6053999662399292\n",
      "Epoch 650, Train Loss: 0.6173937320709229, Test Loss: 0.6052278280258179\n",
      "Epoch 651, Train Loss: 0.6172093749046326, Test Loss: 0.6050611138343811\n",
      "Epoch 652, Train Loss: 0.6170249581336975, Test Loss: 0.604901134967804\n",
      "Epoch 653, Train Loss: 0.6168405413627625, Test Loss: 0.6047400236129761\n",
      "Epoch 654, Train Loss: 0.6166558265686035, Test Loss: 0.6045733690261841\n",
      "Epoch 655, Train Loss: 0.6164711117744446, Test Loss: 0.6043950319290161\n",
      "Epoch 656, Train Loss: 0.6162862777709961, Test Loss: 0.6042269468307495\n",
      "Epoch 657, Train Loss: 0.6161015033721924, Test Loss: 0.6040754318237305\n",
      "Epoch 658, Train Loss: 0.6159166097640991, Test Loss: 0.6039043068885803\n",
      "Epoch 659, Train Loss: 0.6157315969467163, Test Loss: 0.6037395596504211\n",
      "Epoch 660, Train Loss: 0.6155465245246887, Test Loss: 0.6035688519477844\n",
      "Epoch 661, Train Loss: 0.6153613328933716, Test Loss: 0.6034011244773865\n",
      "Epoch 662, Train Loss: 0.6151761412620544, Test Loss: 0.6032342314720154\n",
      "Epoch 663, Train Loss: 0.6149909496307373, Test Loss: 0.6030657291412354\n",
      "Epoch 664, Train Loss: 0.6148055195808411, Test Loss: 0.602892279624939\n",
      "Epoch 665, Train Loss: 0.6146201491355896, Test Loss: 0.6027199625968933\n",
      "Epoch 666, Train Loss: 0.6144346594810486, Test Loss: 0.6025568842887878\n",
      "Epoch 667, Train Loss: 0.6142489314079285, Test Loss: 0.6023831963539124\n",
      "Epoch 668, Train Loss: 0.6140629053115845, Test Loss: 0.6022032499313354\n",
      "Epoch 669, Train Loss: 0.6138768196105957, Test Loss: 0.6020326018333435\n",
      "Epoch 670, Train Loss: 0.6136905550956726, Test Loss: 0.6018633842468262\n",
      "Epoch 671, Train Loss: 0.6135042309761047, Test Loss: 0.6016947031021118\n",
      "Epoch 672, Train Loss: 0.6133179664611816, Test Loss: 0.6015335917472839\n",
      "Epoch 673, Train Loss: 0.6131316423416138, Test Loss: 0.6013680696487427\n",
      "Epoch 674, Train Loss: 0.6129451394081116, Test Loss: 0.6011933088302612\n",
      "Epoch 675, Train Loss: 0.6127585768699646, Test Loss: 0.6010227203369141\n",
      "Epoch 676, Train Loss: 0.6125719547271729, Test Loss: 0.6008516550064087\n",
      "Epoch 677, Train Loss: 0.6123850345611572, Test Loss: 0.6006785035133362\n",
      "Epoch 678, Train Loss: 0.6121980547904968, Test Loss: 0.6005023717880249\n",
      "Epoch 679, Train Loss: 0.6120108962059021, Test Loss: 0.6003268361091614\n",
      "Epoch 680, Train Loss: 0.611823558807373, Test Loss: 0.6001567840576172\n",
      "Epoch 681, Train Loss: 0.6116361021995544, Test Loss: 0.599980354309082\n",
      "Epoch 682, Train Loss: 0.6114485263824463, Test Loss: 0.5998184084892273\n",
      "Epoch 683, Train Loss: 0.611260712146759, Test Loss: 0.5996383428573608\n",
      "Epoch 684, Train Loss: 0.6110726594924927, Test Loss: 0.5994588732719421\n",
      "Epoch 685, Train Loss: 0.6108846664428711, Test Loss: 0.5992786884307861\n",
      "Epoch 686, Train Loss: 0.6106964349746704, Test Loss: 0.599103569984436\n",
      "Epoch 687, Train Loss: 0.6105079650878906, Test Loss: 0.5989296436309814\n",
      "Epoch 688, Train Loss: 0.6103193163871765, Test Loss: 0.598752498626709\n",
      "Epoch 689, Train Loss: 0.6101305484771729, Test Loss: 0.5985767841339111\n",
      "Epoch 690, Train Loss: 0.6099415421485901, Test Loss: 0.5984045267105103\n",
      "Epoch 691, Train Loss: 0.6097524762153625, Test Loss: 0.5982315540313721\n",
      "Epoch 692, Train Loss: 0.6095631122589111, Test Loss: 0.5980595946311951\n",
      "Epoch 693, Train Loss: 0.609373927116394, Test Loss: 0.5979005098342896\n",
      "Epoch 694, Train Loss: 0.6091846227645874, Test Loss: 0.5977289080619812\n",
      "Epoch 695, Train Loss: 0.6089950203895569, Test Loss: 0.5975478887557983\n",
      "Epoch 696, Train Loss: 0.6088053584098816, Test Loss: 0.5973694324493408\n",
      "Epoch 697, Train Loss: 0.6086156368255615, Test Loss: 0.5972081422805786\n",
      "Epoch 698, Train Loss: 0.608425498008728, Test Loss: 0.5970288515090942\n",
      "Epoch 699, Train Loss: 0.608235239982605, Test Loss: 0.5968664288520813\n",
      "Epoch 700, Train Loss: 0.6080449223518372, Test Loss: 0.596701979637146\n",
      "Epoch 701, Train Loss: 0.607854425907135, Test Loss: 0.5965275764465332\n",
      "Epoch 702, Train Loss: 0.6076636910438538, Test Loss: 0.596350371837616\n",
      "Epoch 703, Train Loss: 0.6074727177619934, Test Loss: 0.5961767435073853\n",
      "Epoch 704, Train Loss: 0.607281506061554, Test Loss: 0.5959932804107666\n",
      "Epoch 705, Train Loss: 0.607090175151825, Test Loss: 0.5958160161972046\n",
      "Epoch 706, Train Loss: 0.6068987250328064, Test Loss: 0.5956444144248962\n",
      "Epoch 707, Train Loss: 0.606706976890564, Test Loss: 0.5954753756523132\n",
      "Epoch 708, Train Loss: 0.606515109539032, Test Loss: 0.5952916741371155\n",
      "Epoch 709, Train Loss: 0.6063228845596313, Test Loss: 0.5951150059700012\n",
      "Epoch 710, Train Loss: 0.6061305403709412, Test Loss: 0.5949473977088928\n",
      "Epoch 711, Train Loss: 0.6059378385543823, Test Loss: 0.5947772860527039\n",
      "Epoch 712, Train Loss: 0.6057449579238892, Test Loss: 0.594600260257721\n",
      "Epoch 713, Train Loss: 0.6055519580841064, Test Loss: 0.5944288372993469\n",
      "Epoch 714, Train Loss: 0.6053587198257446, Test Loss: 0.5942634344100952\n",
      "Epoch 715, Train Loss: 0.6051650643348694, Test Loss: 0.5940760970115662\n",
      "Epoch 716, Train Loss: 0.6049712300300598, Test Loss: 0.5938969254493713\n",
      "Epoch 717, Train Loss: 0.6047772169113159, Test Loss: 0.5937241911888123\n",
      "Epoch 718, Train Loss: 0.6045829653739929, Test Loss: 0.5935548543930054\n",
      "Epoch 719, Train Loss: 0.6043887138366699, Test Loss: 0.5933845639228821\n",
      "Epoch 720, Train Loss: 0.6041942238807678, Test Loss: 0.5932050943374634\n",
      "Epoch 721, Train Loss: 0.6039995551109314, Test Loss: 0.5930286049842834\n",
      "Epoch 722, Train Loss: 0.6038048267364502, Test Loss: 0.592864990234375\n",
      "Epoch 723, Train Loss: 0.6036099195480347, Test Loss: 0.5926874279975891\n",
      "Epoch 724, Train Loss: 0.6034147143363953, Test Loss: 0.5925225615501404\n",
      "Epoch 725, Train Loss: 0.6032191514968872, Test Loss: 0.5923367738723755\n",
      "Epoch 726, Train Loss: 0.6030235886573792, Test Loss: 0.5921535491943359\n",
      "Epoch 727, Train Loss: 0.6028278470039368, Test Loss: 0.5919687747955322\n",
      "Epoch 728, Train Loss: 0.6026318669319153, Test Loss: 0.591797411441803\n",
      "Epoch 729, Train Loss: 0.6024355292320251, Test Loss: 0.5916048884391785\n",
      "Epoch 730, Train Loss: 0.6022390127182007, Test Loss: 0.5914217829704285\n",
      "Epoch 731, Train Loss: 0.6020421385765076, Test Loss: 0.5912389159202576\n",
      "Epoch 732, Train Loss: 0.6018451452255249, Test Loss: 0.5910720229148865\n",
      "Epoch 733, Train Loss: 0.6016479730606079, Test Loss: 0.5908917188644409\n",
      "Epoch 734, Train Loss: 0.6014507412910461, Test Loss: 0.5907106995582581\n",
      "Epoch 735, Train Loss: 0.6012529730796814, Test Loss: 0.5905464887619019\n",
      "Epoch 736, Train Loss: 0.6010550260543823, Test Loss: 0.5903750061988831\n",
      "Epoch 737, Train Loss: 0.6008569002151489, Test Loss: 0.5901894569396973\n",
      "Epoch 738, Train Loss: 0.6006584763526917, Test Loss: 0.5900231599807739\n",
      "Epoch 739, Train Loss: 0.6004597544670105, Test Loss: 0.5898334980010986\n",
      "Epoch 740, Train Loss: 0.6002609133720398, Test Loss: 0.5896546244621277\n",
      "Epoch 741, Train Loss: 0.6000617742538452, Test Loss: 0.5894724130630493\n",
      "Epoch 742, Train Loss: 0.5998623967170715, Test Loss: 0.5892906188964844\n",
      "Epoch 743, Train Loss: 0.5996625423431396, Test Loss: 0.589111328125\n",
      "Epoch 744, Train Loss: 0.5994625687599182, Test Loss: 0.5889357924461365\n",
      "Epoch 745, Train Loss: 0.5992622375488281, Test Loss: 0.5887577533721924\n",
      "Epoch 746, Train Loss: 0.5990617275238037, Test Loss: 0.5885727405548096\n",
      "Epoch 747, Train Loss: 0.5988608598709106, Test Loss: 0.5883901715278625\n",
      "Epoch 748, Train Loss: 0.5986596941947937, Test Loss: 0.5882054567337036\n",
      "Epoch 749, Train Loss: 0.5984582901000977, Test Loss: 0.5880295038223267\n",
      "Epoch 750, Train Loss: 0.5982565879821777, Test Loss: 0.5878503918647766\n",
      "Epoch 751, Train Loss: 0.5980545282363892, Test Loss: 0.5876597166061401\n",
      "Epoch 752, Train Loss: 0.5978522300720215, Test Loss: 0.587479293346405\n",
      "Epoch 753, Train Loss: 0.5976496934890747, Test Loss: 0.5873009562492371\n",
      "Epoch 754, Train Loss: 0.597446858882904, Test Loss: 0.5871176719665527\n",
      "Epoch 755, Train Loss: 0.5972437858581543, Test Loss: 0.5869308114051819\n",
      "Epoch 756, Train Loss: 0.5970403552055359, Test Loss: 0.5867307186126709\n",
      "Epoch 757, Train Loss: 0.5968366265296936, Test Loss: 0.5865478515625\n",
      "Epoch 758, Train Loss: 0.5966324806213379, Test Loss: 0.5863564610481262\n",
      "Epoch 759, Train Loss: 0.5964281558990479, Test Loss: 0.5861539840698242\n",
      "Epoch 760, Train Loss: 0.5962234139442444, Test Loss: 0.5859635472297668\n",
      "Epoch 761, Train Loss: 0.5960183143615723, Test Loss: 0.5857855677604675\n",
      "Epoch 762, Train Loss: 0.5958129167556763, Test Loss: 0.5855997800827026\n",
      "Epoch 763, Train Loss: 0.5956072211265564, Test Loss: 0.5854068994522095\n",
      "Epoch 764, Train Loss: 0.5954010486602783, Test Loss: 0.585237979888916\n",
      "Epoch 765, Train Loss: 0.5951943397521973, Test Loss: 0.585055410861969\n",
      "Epoch 766, Train Loss: 0.5949873924255371, Test Loss: 0.5848733186721802\n",
      "Epoch 767, Train Loss: 0.5947801470756531, Test Loss: 0.5846693515777588\n",
      "Epoch 768, Train Loss: 0.5945724248886108, Test Loss: 0.5844860672950745\n",
      "Epoch 769, Train Loss: 0.5943642854690552, Test Loss: 0.5842990875244141\n",
      "Epoch 770, Train Loss: 0.5941557884216309, Test Loss: 0.584112823009491\n",
      "Epoch 771, Train Loss: 0.5939469337463379, Test Loss: 0.5839235782623291\n",
      "Epoch 772, Train Loss: 0.5937378406524658, Test Loss: 0.5837132334709167\n",
      "Epoch 773, Train Loss: 0.5935280919075012, Test Loss: 0.5835253000259399\n",
      "Epoch 774, Train Loss: 0.5933181643486023, Test Loss: 0.583343505859375\n",
      "Epoch 775, Train Loss: 0.5931077003479004, Test Loss: 0.5831517577171326\n",
      "Epoch 776, Train Loss: 0.5928967595100403, Test Loss: 0.5829713344573975\n",
      "Epoch 777, Train Loss: 0.5926854014396667, Test Loss: 0.5827935934066772\n",
      "Epoch 778, Train Loss: 0.5924736857414246, Test Loss: 0.582598865032196\n",
      "Epoch 779, Train Loss: 0.5922614336013794, Test Loss: 0.5824041962623596\n",
      "Epoch 780, Train Loss: 0.5920491218566895, Test Loss: 0.582213282585144\n",
      "Epoch 781, Train Loss: 0.5918363332748413, Test Loss: 0.5820165276527405\n",
      "Epoch 782, Train Loss: 0.5916233062744141, Test Loss: 0.5818138718605042\n",
      "Epoch 783, Train Loss: 0.5914098024368286, Test Loss: 0.5816231966018677\n",
      "Epoch 784, Train Loss: 0.5911960601806641, Test Loss: 0.5814287662506104\n",
      "Epoch 785, Train Loss: 0.5909816026687622, Test Loss: 0.5812318325042725\n",
      "Epoch 786, Train Loss: 0.590766966342926, Test Loss: 0.5810465812683105\n",
      "Epoch 787, Train Loss: 0.5905518531799316, Test Loss: 0.5808402299880981\n",
      "Epoch 788, Train Loss: 0.5903363823890686, Test Loss: 0.5806438326835632\n",
      "Epoch 789, Train Loss: 0.5901204943656921, Test Loss: 0.5804505944252014\n",
      "Epoch 790, Train Loss: 0.589904248714447, Test Loss: 0.5802633166313171\n",
      "Epoch 791, Train Loss: 0.5896875262260437, Test Loss: 0.5800658464431763\n",
      "Epoch 792, Train Loss: 0.589470386505127, Test Loss: 0.5798757076263428\n",
      "Epoch 793, Train Loss: 0.5892527103424072, Test Loss: 0.579684853553772\n",
      "Epoch 794, Train Loss: 0.5890342593193054, Test Loss: 0.5794727206230164\n",
      "Epoch 795, Train Loss: 0.5888155698776245, Test Loss: 0.5792734622955322\n",
      "Epoch 796, Train Loss: 0.5885965824127197, Test Loss: 0.5790656805038452\n",
      "Epoch 797, Train Loss: 0.5883771181106567, Test Loss: 0.5788492560386658\n",
      "Epoch 798, Train Loss: 0.5881572961807251, Test Loss: 0.5786497592926025\n",
      "Epoch 799, Train Loss: 0.58793705701828, Test Loss: 0.5784506797790527\n",
      "Epoch 800, Train Loss: 0.587716281414032, Test Loss: 0.5782471299171448\n",
      "Epoch 801, Train Loss: 0.5874950289726257, Test Loss: 0.5780417323112488\n",
      "Epoch 802, Train Loss: 0.5872734785079956, Test Loss: 0.5778323411941528\n",
      "Epoch 803, Train Loss: 0.5870513916015625, Test Loss: 0.5776208639144897\n",
      "Epoch 804, Train Loss: 0.586828887462616, Test Loss: 0.5774164795875549\n",
      "Epoch 805, Train Loss: 0.5866056680679321, Test Loss: 0.5772154927253723\n",
      "Epoch 806, Train Loss: 0.5863820910453796, Test Loss: 0.577026903629303\n",
      "Epoch 807, Train Loss: 0.586158037185669, Test Loss: 0.576817512512207\n",
      "Epoch 808, Train Loss: 0.5859334468841553, Test Loss: 0.5766012072563171\n",
      "Epoch 809, Train Loss: 0.5857083201408386, Test Loss: 0.5764046907424927\n",
      "Epoch 810, Train Loss: 0.5854828357696533, Test Loss: 0.576194703578949\n",
      "Epoch 811, Train Loss: 0.5852566957473755, Test Loss: 0.5759860873222351\n",
      "Epoch 812, Train Loss: 0.5850299000740051, Test Loss: 0.5757775902748108\n",
      "Epoch 813, Train Loss: 0.5848026275634766, Test Loss: 0.5755689740180969\n",
      "Epoch 814, Train Loss: 0.5845750570297241, Test Loss: 0.5753676891326904\n",
      "Epoch 815, Train Loss: 0.5843468308448792, Test Loss: 0.5751548409461975\n",
      "Epoch 816, Train Loss: 0.5841181874275208, Test Loss: 0.5749427676200867\n",
      "Epoch 817, Train Loss: 0.5838890075683594, Test Loss: 0.5747121572494507\n",
      "Epoch 818, Train Loss: 0.5836595296859741, Test Loss: 0.5745051503181458\n",
      "Epoch 819, Train Loss: 0.5834295749664307, Test Loss: 0.5742867588996887\n",
      "Epoch 820, Train Loss: 0.5831990242004395, Test Loss: 0.5740845203399658\n",
      "Epoch 821, Train Loss: 0.5829681754112244, Test Loss: 0.5738735795021057\n",
      "Epoch 822, Train Loss: 0.5827368497848511, Test Loss: 0.5736653804779053\n",
      "Epoch 823, Train Loss: 0.5825051665306091, Test Loss: 0.5734456181526184\n",
      "Epoch 824, Train Loss: 0.5822731256484985, Test Loss: 0.5732153654098511\n",
      "Epoch 825, Train Loss: 0.5820403099060059, Test Loss: 0.5730065107345581\n",
      "Epoch 826, Train Loss: 0.581807017326355, Test Loss: 0.5728008151054382\n",
      "Epoch 827, Train Loss: 0.5815733671188354, Test Loss: 0.5725913643836975\n",
      "Epoch 828, Train Loss: 0.5813391208648682, Test Loss: 0.5723783373832703\n",
      "Epoch 829, Train Loss: 0.5811046361923218, Test Loss: 0.5721771717071533\n",
      "Epoch 830, Train Loss: 0.5808696746826172, Test Loss: 0.5719470977783203\n",
      "Epoch 831, Train Loss: 0.5806342959403992, Test Loss: 0.5717343091964722\n",
      "Epoch 832, Train Loss: 0.5803986191749573, Test Loss: 0.5715123414993286\n",
      "Epoch 833, Train Loss: 0.580162525177002, Test Loss: 0.5712899565696716\n",
      "Epoch 834, Train Loss: 0.5799258351325989, Test Loss: 0.5710804462432861\n",
      "Epoch 835, Train Loss: 0.5796887874603271, Test Loss: 0.5708833336830139\n",
      "Epoch 836, Train Loss: 0.5794511437416077, Test Loss: 0.5706545114517212\n",
      "Epoch 837, Train Loss: 0.5792131423950195, Test Loss: 0.5704418420791626\n",
      "Epoch 838, Train Loss: 0.5789744853973389, Test Loss: 0.5702289342880249\n",
      "Epoch 839, Train Loss: 0.5787353515625, Test Loss: 0.5700027942657471\n",
      "Epoch 840, Train Loss: 0.5784957408905029, Test Loss: 0.5697857737541199\n",
      "Epoch 841, Train Loss: 0.5782556533813477, Test Loss: 0.5695511698722839\n",
      "Epoch 842, Train Loss: 0.5780152678489685, Test Loss: 0.5693041682243347\n",
      "Epoch 843, Train Loss: 0.5777742266654968, Test Loss: 0.5690953731536865\n",
      "Epoch 844, Train Loss: 0.5775325298309326, Test Loss: 0.5688751339912415\n",
      "Epoch 845, Train Loss: 0.5772905349731445, Test Loss: 0.5686835050582886\n",
      "Epoch 846, Train Loss: 0.5770478844642639, Test Loss: 0.5684536695480347\n",
      "Epoch 847, Train Loss: 0.5768045783042908, Test Loss: 0.5682264566421509\n",
      "Epoch 848, Train Loss: 0.5765607953071594, Test Loss: 0.5679988265037537\n",
      "Epoch 849, Train Loss: 0.5763165950775146, Test Loss: 0.5677675604820251\n",
      "Epoch 850, Train Loss: 0.5760717391967773, Test Loss: 0.5675516724586487\n",
      "Epoch 851, Train Loss: 0.5758260488510132, Test Loss: 0.5673198699951172\n",
      "Epoch 852, Train Loss: 0.5755799412727356, Test Loss: 0.5670974254608154\n",
      "Epoch 853, Train Loss: 0.5753334164619446, Test Loss: 0.5668902397155762\n",
      "Epoch 854, Train Loss: 0.5750861763954163, Test Loss: 0.5666695237159729\n",
      "Epoch 855, Train Loss: 0.5748382806777954, Test Loss: 0.5664511919021606\n",
      "Epoch 856, Train Loss: 0.5745894908905029, Test Loss: 0.5662112832069397\n",
      "Epoch 857, Train Loss: 0.5743403434753418, Test Loss: 0.5659804344177246\n",
      "Epoch 858, Train Loss: 0.5740910768508911, Test Loss: 0.5657657980918884\n",
      "Epoch 859, Train Loss: 0.5738410949707031, Test Loss: 0.5655439496040344\n",
      "Epoch 860, Train Loss: 0.5735904574394226, Test Loss: 0.5653127431869507\n",
      "Epoch 861, Train Loss: 0.5733394622802734, Test Loss: 0.5650951266288757\n",
      "Epoch 862, Train Loss: 0.5730876922607422, Test Loss: 0.5648691654205322\n",
      "Epoch 863, Train Loss: 0.5728349685668945, Test Loss: 0.5646302103996277\n",
      "Epoch 864, Train Loss: 0.5725818872451782, Test Loss: 0.5643913745880127\n",
      "Epoch 865, Train Loss: 0.5723282694816589, Test Loss: 0.5641420483589172\n",
      "Epoch 866, Train Loss: 0.5720742344856262, Test Loss: 0.5639105439186096\n",
      "Epoch 867, Train Loss: 0.5718196630477905, Test Loss: 0.5636708736419678\n",
      "Epoch 868, Train Loss: 0.5715643167495728, Test Loss: 0.563438892364502\n",
      "Epoch 869, Train Loss: 0.5713084936141968, Test Loss: 0.563208818435669\n",
      "Epoch 870, Train Loss: 0.5710520148277283, Test Loss: 0.5629709959030151\n",
      "Epoch 871, Train Loss: 0.5707949995994568, Test Loss: 0.5627419352531433\n",
      "Epoch 872, Train Loss: 0.570537269115448, Test Loss: 0.5625015497207642\n",
      "Epoch 873, Train Loss: 0.5702787041664124, Test Loss: 0.5622594356536865\n",
      "Epoch 874, Train Loss: 0.5700197815895081, Test Loss: 0.5620310306549072\n",
      "Epoch 875, Train Loss: 0.569760262966156, Test Loss: 0.5618029832839966\n",
      "Epoch 876, Train Loss: 0.5694997906684875, Test Loss: 0.5615549683570862\n",
      "Epoch 877, Train Loss: 0.5692387223243713, Test Loss: 0.5613111257553101\n",
      "Epoch 878, Train Loss: 0.5689771175384521, Test Loss: 0.5610697269439697\n",
      "Epoch 879, Train Loss: 0.5687147378921509, Test Loss: 0.5608454942703247\n",
      "Epoch 880, Train Loss: 0.5684515237808228, Test Loss: 0.5605903267860413\n",
      "Epoch 881, Train Loss: 0.5681878924369812, Test Loss: 0.5603475570678711\n",
      "Epoch 882, Train Loss: 0.5679236054420471, Test Loss: 0.5601229667663574\n",
      "Epoch 883, Train Loss: 0.5676588416099548, Test Loss: 0.5598894953727722\n",
      "Epoch 884, Train Loss: 0.5673936009407043, Test Loss: 0.5596558451652527\n",
      "Epoch 885, Train Loss: 0.5671278238296509, Test Loss: 0.5594216585159302\n",
      "Epoch 886, Train Loss: 0.5668612122535706, Test Loss: 0.5591628551483154\n",
      "Epoch 887, Train Loss: 0.5665944814682007, Test Loss: 0.5589159727096558\n",
      "Epoch 888, Train Loss: 0.5663270354270935, Test Loss: 0.558663010597229\n",
      "Epoch 889, Train Loss: 0.5660589933395386, Test Loss: 0.558400571346283\n",
      "Epoch 890, Train Loss: 0.565790593624115, Test Loss: 0.5581334829330444\n",
      "Epoch 891, Train Loss: 0.5655211210250854, Test Loss: 0.5578850507736206\n",
      "Epoch 892, Train Loss: 0.5652508735656738, Test Loss: 0.5576551556587219\n",
      "Epoch 893, Train Loss: 0.5649804472923279, Test Loss: 0.5574120879173279\n",
      "Epoch 894, Train Loss: 0.564709484577179, Test Loss: 0.5571745038032532\n",
      "Epoch 895, Train Loss: 0.5644378662109375, Test Loss: 0.5569337606430054\n",
      "Epoch 896, Train Loss: 0.5641657710075378, Test Loss: 0.5566881895065308\n",
      "Epoch 897, Train Loss: 0.5638930797576904, Test Loss: 0.5564250946044922\n",
      "Epoch 898, Train Loss: 0.5636199116706848, Test Loss: 0.5561838150024414\n",
      "Epoch 899, Train Loss: 0.5633462071418762, Test Loss: 0.555937647819519\n",
      "Epoch 900, Train Loss: 0.5630720853805542, Test Loss: 0.5556785464286804\n",
      "Epoch 901, Train Loss: 0.5627976655960083, Test Loss: 0.555427610874176\n",
      "Epoch 902, Train Loss: 0.5625227689743042, Test Loss: 0.5551603436470032\n",
      "Epoch 903, Train Loss: 0.5622472167015076, Test Loss: 0.5549171566963196\n",
      "Epoch 904, Train Loss: 0.5619712471961975, Test Loss: 0.554657518863678\n",
      "Epoch 905, Train Loss: 0.5616946816444397, Test Loss: 0.5544055104255676\n",
      "Epoch 906, Train Loss: 0.5614176988601685, Test Loss: 0.5541514158248901\n",
      "Epoch 907, Train Loss: 0.5611401200294495, Test Loss: 0.5539010167121887\n",
      "Epoch 908, Train Loss: 0.5608620643615723, Test Loss: 0.5536383390426636\n",
      "Epoch 909, Train Loss: 0.5605832934379578, Test Loss: 0.5533803105354309\n",
      "Epoch 910, Train Loss: 0.5603038668632507, Test Loss: 0.5531362295150757\n",
      "Epoch 911, Train Loss: 0.560023844242096, Test Loss: 0.5529013276100159\n",
      "Epoch 912, Train Loss: 0.5597426295280457, Test Loss: 0.5526412129402161\n",
      "Epoch 913, Train Loss: 0.5594608783721924, Test Loss: 0.5523898005485535\n",
      "Epoch 914, Train Loss: 0.5591785907745361, Test Loss: 0.5521292090415955\n",
      "Epoch 915, Train Loss: 0.5588957071304321, Test Loss: 0.5518803000450134\n",
      "Epoch 916, Train Loss: 0.5586121678352356, Test Loss: 0.5516139268875122\n",
      "Epoch 917, Train Loss: 0.5583279132843018, Test Loss: 0.5513509511947632\n",
      "Epoch 918, Train Loss: 0.5580430030822754, Test Loss: 0.551098108291626\n",
      "Epoch 919, Train Loss: 0.5577573776245117, Test Loss: 0.550835132598877\n",
      "Epoch 920, Train Loss: 0.5574712157249451, Test Loss: 0.5505920648574829\n",
      "Epoch 921, Train Loss: 0.5571846961975098, Test Loss: 0.5503368377685547\n",
      "Epoch 922, Train Loss: 0.556897759437561, Test Loss: 0.5500824451446533\n",
      "Epoch 923, Train Loss: 0.5566100478172302, Test Loss: 0.5498239398002625\n",
      "Epoch 924, Train Loss: 0.5563219785690308, Test Loss: 0.5495668053627014\n",
      "Epoch 925, Train Loss: 0.5560333728790283, Test Loss: 0.5493176579475403\n",
      "Epoch 926, Train Loss: 0.555743932723999, Test Loss: 0.5490502715110779\n",
      "Epoch 927, Train Loss: 0.555453896522522, Test Loss: 0.548782229423523\n",
      "Epoch 928, Train Loss: 0.5551633834838867, Test Loss: 0.5485272407531738\n",
      "Epoch 929, Train Loss: 0.5548722147941589, Test Loss: 0.5482692122459412\n",
      "Epoch 930, Train Loss: 0.5545809864997864, Test Loss: 0.5480312705039978\n",
      "Epoch 931, Train Loss: 0.5542889833450317, Test Loss: 0.5477577447891235\n",
      "Epoch 932, Train Loss: 0.5539965033531189, Test Loss: 0.5474924445152283\n",
      "Epoch 933, Train Loss: 0.5537036061286926, Test Loss: 0.5472242832183838\n",
      "Epoch 934, Train Loss: 0.5534101724624634, Test Loss: 0.5469390749931335\n",
      "Epoch 935, Train Loss: 0.5531162023544312, Test Loss: 0.5466848015785217\n",
      "Epoch 936, Train Loss: 0.552821934223175, Test Loss: 0.5464093089103699\n",
      "Epoch 937, Train Loss: 0.5525270104408264, Test Loss: 0.5461586117744446\n",
      "Epoch 938, Train Loss: 0.5522316098213196, Test Loss: 0.545883059501648\n",
      "Epoch 939, Train Loss: 0.5519357919692993, Test Loss: 0.5456101298332214\n",
      "Epoch 940, Train Loss: 0.5516394972801208, Test Loss: 0.5453516244888306\n",
      "Epoch 941, Train Loss: 0.5513426661491394, Test Loss: 0.5450830459594727\n",
      "Epoch 942, Train Loss: 0.5510454177856445, Test Loss: 0.5448083877563477\n",
      "Epoch 943, Train Loss: 0.5507476925849915, Test Loss: 0.5445526242256165\n",
      "Epoch 944, Train Loss: 0.5504493117332458, Test Loss: 0.5442779064178467\n",
      "Epoch 945, Train Loss: 0.550150454044342, Test Loss: 0.5440075397491455\n",
      "Epoch 946, Train Loss: 0.5498512387275696, Test Loss: 0.5437144637107849\n",
      "Epoch 947, Train Loss: 0.5495519042015076, Test Loss: 0.5434508323669434\n",
      "Epoch 948, Train Loss: 0.5492520332336426, Test Loss: 0.5431715846061707\n",
      "Epoch 949, Train Loss: 0.5489515662193298, Test Loss: 0.5428813695907593\n",
      "Epoch 950, Train Loss: 0.5486504435539246, Test Loss: 0.5426044464111328\n",
      "Epoch 951, Train Loss: 0.5483486652374268, Test Loss: 0.5423303246498108\n",
      "Epoch 952, Train Loss: 0.5480461716651917, Test Loss: 0.5420480966567993\n",
      "Epoch 953, Train Loss: 0.5477432012557983, Test Loss: 0.5417855381965637\n",
      "Epoch 954, Train Loss: 0.5474396347999573, Test Loss: 0.5415001511573792\n",
      "Epoch 955, Train Loss: 0.547135591506958, Test Loss: 0.5412179231643677\n",
      "Epoch 956, Train Loss: 0.5468310117721558, Test Loss: 0.5409680604934692\n",
      "Epoch 957, Train Loss: 0.5465258955955505, Test Loss: 0.5406809449195862\n",
      "Epoch 958, Train Loss: 0.5462202429771423, Test Loss: 0.5403963923454285\n",
      "Epoch 959, Train Loss: 0.5459141135215759, Test Loss: 0.5401186943054199\n",
      "Epoch 960, Train Loss: 0.5456073880195618, Test Loss: 0.5398173928260803\n",
      "Epoch 961, Train Loss: 0.5452997088432312, Test Loss: 0.5395377278327942\n",
      "Epoch 962, Train Loss: 0.5449919700622559, Test Loss: 0.5392513871192932\n",
      "Epoch 963, Train Loss: 0.5446836352348328, Test Loss: 0.5389766097068787\n",
      "Epoch 964, Train Loss: 0.544374942779541, Test Loss: 0.5386824011802673\n",
      "Epoch 965, Train Loss: 0.5440658330917358, Test Loss: 0.5384066104888916\n",
      "Epoch 966, Train Loss: 0.5437564849853516, Test Loss: 0.5381342768669128\n",
      "Epoch 967, Train Loss: 0.5434466600418091, Test Loss: 0.5378516912460327\n",
      "Epoch 968, Train Loss: 0.543136477470398, Test Loss: 0.537575364112854\n",
      "Epoch 969, Train Loss: 0.5428258180618286, Test Loss: 0.5372937321662903\n",
      "Epoch 970, Train Loss: 0.542514979839325, Test Loss: 0.5369865894317627\n",
      "Epoch 971, Train Loss: 0.5422031879425049, Test Loss: 0.5367228984832764\n",
      "Epoch 972, Train Loss: 0.5418912768363953, Test Loss: 0.5364357829093933\n",
      "Epoch 973, Train Loss: 0.5415791273117065, Test Loss: 0.5361412167549133\n",
      "Epoch 974, Train Loss: 0.5412666201591492, Test Loss: 0.5358549952507019\n",
      "Epoch 975, Train Loss: 0.5409539937973022, Test Loss: 0.535547137260437\n",
      "Epoch 976, Train Loss: 0.540640652179718, Test Loss: 0.5352705121040344\n",
      "Epoch 977, Train Loss: 0.5403265357017517, Test Loss: 0.5350023508071899\n",
      "Epoch 978, Train Loss: 0.5400119423866272, Test Loss: 0.5347179770469666\n",
      "Epoch 979, Train Loss: 0.5396972298622131, Test Loss: 0.5344395637512207\n",
      "Epoch 980, Train Loss: 0.5393819808959961, Test Loss: 0.5341634750366211\n",
      "Epoch 981, Train Loss: 0.5390663146972656, Test Loss: 0.5338928699493408\n",
      "Epoch 982, Train Loss: 0.538750171661377, Test Loss: 0.5336000323295593\n",
      "Epoch 983, Train Loss: 0.5384337306022644, Test Loss: 0.533309817314148\n",
      "Epoch 984, Train Loss: 0.5381171703338623, Test Loss: 0.5330334901809692\n",
      "Epoch 985, Train Loss: 0.5378003120422363, Test Loss: 0.5327564477920532\n",
      "Epoch 986, Train Loss: 0.5374826788902283, Test Loss: 0.5324636697769165\n",
      "Epoch 987, Train Loss: 0.5371646285057068, Test Loss: 0.532168984413147\n",
      "Epoch 988, Train Loss: 0.5368462204933167, Test Loss: 0.531862199306488\n",
      "Epoch 989, Train Loss: 0.5365275144577026, Test Loss: 0.5315622687339783\n",
      "Epoch 990, Train Loss: 0.5362085103988647, Test Loss: 0.5312510132789612\n",
      "Epoch 991, Train Loss: 0.535889208316803, Test Loss: 0.5309675335884094\n",
      "Epoch 992, Train Loss: 0.5355696082115173, Test Loss: 0.5306717753410339\n",
      "Epoch 993, Train Loss: 0.5352494120597839, Test Loss: 0.5303807258605957\n",
      "Epoch 994, Train Loss: 0.5349287986755371, Test Loss: 0.5300891399383545\n",
      "Epoch 995, Train Loss: 0.5346077084541321, Test Loss: 0.5298075079917908\n",
      "Epoch 996, Train Loss: 0.534286618232727, Test Loss: 0.529518723487854\n",
      "Epoch 997, Train Loss: 0.5339654088020325, Test Loss: 0.5292256474494934\n",
      "Epoch 998, Train Loss: 0.5336439609527588, Test Loss: 0.5289309620857239\n",
      "Epoch 999, Train Loss: 0.5333221554756165, Test Loss: 0.5286533832550049\n",
      "Epoch 1000, Train Loss: 0.5330001711845398, Test Loss: 0.5283569693565369\n",
      "Epoch 1001, Train Loss: 0.5326778888702393, Test Loss: 0.528050422668457\n",
      "Epoch 1002, Train Loss: 0.5323557257652283, Test Loss: 0.5277408957481384\n",
      "Epoch 1003, Train Loss: 0.5320330262184143, Test Loss: 0.5274463295936584\n",
      "Epoch 1004, Train Loss: 0.5317103266716003, Test Loss: 0.5271551012992859\n",
      "Epoch 1005, Train Loss: 0.5313873291015625, Test Loss: 0.5268697738647461\n",
      "Epoch 1006, Train Loss: 0.5310643315315247, Test Loss: 0.5265803337097168\n",
      "Epoch 1007, Train Loss: 0.5307410955429077, Test Loss: 0.5262894630432129\n",
      "Epoch 1008, Train Loss: 0.5304176211357117, Test Loss: 0.525997519493103\n",
      "Epoch 1009, Train Loss: 0.5300940275192261, Test Loss: 0.5257315635681152\n",
      "Epoch 1010, Train Loss: 0.5297699570655823, Test Loss: 0.5254285931587219\n",
      "Epoch 1011, Train Loss: 0.5294458866119385, Test Loss: 0.5251213908195496\n",
      "Epoch 1012, Train Loss: 0.529121458530426, Test Loss: 0.5248103141784668\n",
      "Epoch 1013, Train Loss: 0.5287966728210449, Test Loss: 0.5245164632797241\n",
      "Epoch 1014, Train Loss: 0.5284716486930847, Test Loss: 0.5242190957069397\n",
      "Epoch 1015, Train Loss: 0.5281462669372559, Test Loss: 0.5239273905754089\n",
      "Epoch 1016, Train Loss: 0.5278210639953613, Test Loss: 0.5236194133758545\n",
      "Epoch 1017, Train Loss: 0.5274956226348877, Test Loss: 0.5233249664306641\n",
      "Epoch 1018, Train Loss: 0.5271700024604797, Test Loss: 0.5230394601821899\n",
      "Epoch 1019, Train Loss: 0.5268443822860718, Test Loss: 0.5227400064468384\n",
      "Epoch 1020, Train Loss: 0.5265184640884399, Test Loss: 0.5224249362945557\n",
      "Epoch 1021, Train Loss: 0.5261924266815186, Test Loss: 0.5221333503723145\n",
      "Epoch 1022, Train Loss: 0.5258663892745972, Test Loss: 0.5218583941459656\n",
      "Epoch 1023, Train Loss: 0.5255402326583862, Test Loss: 0.5215680599212646\n",
      "Epoch 1024, Train Loss: 0.5252136588096619, Test Loss: 0.5212664008140564\n",
      "Epoch 1025, Train Loss: 0.5248867869377136, Test Loss: 0.520970344543457\n",
      "Epoch 1026, Train Loss: 0.524559736251831, Test Loss: 0.5206802487373352\n",
      "Epoch 1027, Train Loss: 0.5242324471473694, Test Loss: 0.5203731656074524\n",
      "Epoch 1028, Train Loss: 0.5239050984382629, Test Loss: 0.5200697779655457\n",
      "Epoch 1029, Train Loss: 0.523577868938446, Test Loss: 0.5197882652282715\n",
      "Epoch 1030, Train Loss: 0.5232502222061157, Test Loss: 0.5194783806800842\n",
      "Epoch 1031, Train Loss: 0.5229226350784302, Test Loss: 0.5191816091537476\n",
      "Epoch 1032, Train Loss: 0.5225948691368103, Test Loss: 0.5188830494880676\n",
      "Epoch 1033, Train Loss: 0.5222671627998352, Test Loss: 0.5185657739639282\n",
      "Epoch 1034, Train Loss: 0.521939218044281, Test Loss: 0.5182739496231079\n",
      "Epoch 1035, Train Loss: 0.5216108560562134, Test Loss: 0.5179997682571411\n",
      "Epoch 1036, Train Loss: 0.521282970905304, Test Loss: 0.5177242755889893\n",
      "Epoch 1037, Train Loss: 0.5209544897079468, Test Loss: 0.5173924565315247\n",
      "Epoch 1038, Train Loss: 0.5206258893013, Test Loss: 0.5171046257019043\n",
      "Epoch 1039, Train Loss: 0.5202972292900085, Test Loss: 0.5167999863624573\n",
      "Epoch 1040, Train Loss: 0.5199685096740723, Test Loss: 0.5164920091629028\n",
      "Epoch 1041, Train Loss: 0.5196396708488464, Test Loss: 0.5162004232406616\n",
      "Epoch 1042, Train Loss: 0.5193106532096863, Test Loss: 0.515905499458313\n",
      "Epoch 1043, Train Loss: 0.5189816355705261, Test Loss: 0.5156000256538391\n",
      "Epoch 1044, Train Loss: 0.5186522006988525, Test Loss: 0.5153194665908813\n",
      "Epoch 1045, Train Loss: 0.5183226466178894, Test Loss: 0.5150157809257507\n",
      "Epoch 1046, Train Loss: 0.5179932117462158, Test Loss: 0.5147112011909485\n",
      "Epoch 1047, Train Loss: 0.5176635980606079, Test Loss: 0.5144014358520508\n",
      "Epoch 1048, Train Loss: 0.5173338651657104, Test Loss: 0.5141075849533081\n",
      "Epoch 1049, Train Loss: 0.5170041918754578, Test Loss: 0.5138140320777893\n",
      "Epoch 1050, Train Loss: 0.5166745185852051, Test Loss: 0.5135014653205872\n",
      "Epoch 1051, Train Loss: 0.5163450241088867, Test Loss: 0.5132004022598267\n",
      "Epoch 1052, Train Loss: 0.5160152912139893, Test Loss: 0.5129278302192688\n",
      "Epoch 1053, Train Loss: 0.5156856179237366, Test Loss: 0.5126415491104126\n",
      "Epoch 1054, Train Loss: 0.5153562426567078, Test Loss: 0.5123583078384399\n",
      "Epoch 1055, Train Loss: 0.5150267481803894, Test Loss: 0.51206374168396\n",
      "Epoch 1056, Train Loss: 0.5146974325180054, Test Loss: 0.5118000507354736\n",
      "Epoch 1057, Train Loss: 0.5143678784370422, Test Loss: 0.5114912986755371\n",
      "Epoch 1058, Train Loss: 0.514038622379303, Test Loss: 0.5111892819404602\n",
      "Epoch 1059, Train Loss: 0.5137093663215637, Test Loss: 0.5108829736709595\n",
      "Epoch 1060, Train Loss: 0.5133801102638245, Test Loss: 0.5105922818183899\n",
      "Epoch 1061, Train Loss: 0.5130507946014404, Test Loss: 0.5102914571762085\n",
      "Epoch 1062, Train Loss: 0.5127218961715698, Test Loss: 0.5100064873695374\n",
      "Epoch 1063, Train Loss: 0.5123929977416992, Test Loss: 0.5096926093101501\n",
      "Epoch 1064, Train Loss: 0.5120639801025391, Test Loss: 0.5094015598297119\n",
      "Epoch 1065, Train Loss: 0.5117350816726685, Test Loss: 0.5091130137443542\n",
      "Epoch 1066, Train Loss: 0.5114061236381531, Test Loss: 0.5088155269622803\n",
      "Epoch 1067, Train Loss: 0.511077344417572, Test Loss: 0.5085263252258301\n",
      "Epoch 1068, Train Loss: 0.5107488632202148, Test Loss: 0.5082544088363647\n",
      "Epoch 1069, Train Loss: 0.5104201436042786, Test Loss: 0.5079578161239624\n",
      "Epoch 1070, Train Loss: 0.5100915431976318, Test Loss: 0.5076626539230347\n",
      "Epoch 1071, Train Loss: 0.5097628831863403, Test Loss: 0.5073687434196472\n",
      "Epoch 1072, Train Loss: 0.5094343423843384, Test Loss: 0.5070834755897522\n",
      "Epoch 1073, Train Loss: 0.5091060400009155, Test Loss: 0.5068091750144958\n",
      "Epoch 1074, Train Loss: 0.5087777376174927, Test Loss: 0.5065187811851501\n",
      "Epoch 1075, Train Loss: 0.508449912071228, Test Loss: 0.5062212347984314\n",
      "Epoch 1076, Train Loss: 0.5081225037574768, Test Loss: 0.5059284567832947\n",
      "Epoch 1077, Train Loss: 0.5077956914901733, Test Loss: 0.5056310892105103\n",
      "Epoch 1078, Train Loss: 0.5074693560600281, Test Loss: 0.5053192973136902\n",
      "Epoch 1079, Train Loss: 0.507143497467041, Test Loss: 0.5050197243690491\n",
      "Epoch 1080, Train Loss: 0.5068173408508301, Test Loss: 0.5047481060028076\n",
      "Epoch 1081, Train Loss: 0.5064913630485535, Test Loss: 0.5044800639152527\n",
      "Epoch 1082, Train Loss: 0.5061656832695007, Test Loss: 0.5042148232460022\n",
      "Epoch 1083, Train Loss: 0.5058402419090271, Test Loss: 0.5039240121841431\n",
      "Epoch 1084, Train Loss: 0.5055149793624878, Test Loss: 0.5036421418190002\n",
      "Epoch 1085, Train Loss: 0.505189836025238, Test Loss: 0.5033654570579529\n",
      "Epoch 1086, Train Loss: 0.5048647522926331, Test Loss: 0.503060519695282\n",
      "Epoch 1087, Train Loss: 0.504539966583252, Test Loss: 0.502763032913208\n",
      "Epoch 1088, Train Loss: 0.5042154788970947, Test Loss: 0.5024704337120056\n",
      "Epoch 1089, Train Loss: 0.5038909912109375, Test Loss: 0.5021923780441284\n",
      "Epoch 1090, Train Loss: 0.5035668015480042, Test Loss: 0.5019062161445618\n",
      "Epoch 1091, Train Loss: 0.5032424926757812, Test Loss: 0.5016435384750366\n",
      "Epoch 1092, Train Loss: 0.5029187202453613, Test Loss: 0.501377522945404\n",
      "Epoch 1093, Train Loss: 0.5025951862335205, Test Loss: 0.501076877117157\n",
      "Epoch 1094, Train Loss: 0.5022717714309692, Test Loss: 0.5008124113082886\n",
      "Epoch 1095, Train Loss: 0.501948893070221, Test Loss: 0.5005486011505127\n",
      "Epoch 1096, Train Loss: 0.5016262531280518, Test Loss: 0.5002733469009399\n",
      "Epoch 1097, Train Loss: 0.5013038516044617, Test Loss: 0.49999767541885376\n",
      "Epoch 1098, Train Loss: 0.5009816288948059, Test Loss: 0.499713659286499\n",
      "Epoch 1099, Train Loss: 0.5006598830223083, Test Loss: 0.4994506239891052\n",
      "Epoch 1100, Train Loss: 0.5003381371498108, Test Loss: 0.49916893243789673\n",
      "Epoch 1101, Train Loss: 0.500016450881958, Test Loss: 0.4988488256931305\n",
      "Epoch 1102, Train Loss: 0.4996950030326843, Test Loss: 0.4985739290714264\n",
      "Epoch 1103, Train Loss: 0.4993739128112793, Test Loss: 0.4983260929584503\n",
      "Epoch 1104, Train Loss: 0.49905309081077576, Test Loss: 0.49805933237075806\n",
      "Epoch 1105, Train Loss: 0.49873262643814087, Test Loss: 0.49778589606285095\n",
      "Epoch 1106, Train Loss: 0.4984126389026642, Test Loss: 0.4975180923938751\n",
      "Epoch 1107, Train Loss: 0.49809321761131287, Test Loss: 0.4972638189792633\n",
      "Epoch 1108, Train Loss: 0.49777382612228394, Test Loss: 0.49697431921958923\n",
      "Epoch 1109, Train Loss: 0.4974548816680908, Test Loss: 0.4966682493686676\n",
      "Epoch 1110, Train Loss: 0.4971369504928589, Test Loss: 0.49636343121528625\n",
      "Epoch 1111, Train Loss: 0.49681881070137024, Test Loss: 0.4961036443710327\n",
      "Epoch 1112, Train Loss: 0.4965011775493622, Test Loss: 0.49583694338798523\n",
      "Epoch 1113, Train Loss: 0.4961841106414795, Test Loss: 0.49556514620780945\n",
      "Epoch 1114, Train Loss: 0.49586722254753113, Test Loss: 0.49529439210891724\n",
      "Epoch 1115, Train Loss: 0.49555063247680664, Test Loss: 0.495044082403183\n",
      "Epoch 1116, Train Loss: 0.49523434042930603, Test Loss: 0.4947906732559204\n",
      "Epoch 1117, Train Loss: 0.4949185252189636, Test Loss: 0.49449777603149414\n",
      "Epoch 1118, Train Loss: 0.4946030378341675, Test Loss: 0.49422353506088257\n",
      "Epoch 1119, Train Loss: 0.4942881166934967, Test Loss: 0.4939456284046173\n",
      "Epoch 1120, Train Loss: 0.4939735233783722, Test Loss: 0.4936748147010803\n",
      "Epoch 1121, Train Loss: 0.49365928769111633, Test Loss: 0.49341750144958496\n",
      "Epoch 1122, Train Loss: 0.4933457374572754, Test Loss: 0.49314451217651367\n",
      "Epoch 1123, Train Loss: 0.49303242564201355, Test Loss: 0.49290674924850464\n",
      "Epoch 1124, Train Loss: 0.4927195906639099, Test Loss: 0.4926321506500244\n",
      "Epoch 1125, Train Loss: 0.49240708351135254, Test Loss: 0.49238234758377075\n",
      "Epoch 1126, Train Loss: 0.49209481477737427, Test Loss: 0.4921002984046936\n",
      "Epoch 1127, Train Loss: 0.4917830526828766, Test Loss: 0.4918309152126312\n",
      "Epoch 1128, Train Loss: 0.4914717376232147, Test Loss: 0.49158093333244324\n",
      "Epoch 1129, Train Loss: 0.49116069078445435, Test Loss: 0.49131160974502563\n",
      "Epoch 1130, Train Loss: 0.49084997177124023, Test Loss: 0.491051584482193\n",
      "Epoch 1131, Train Loss: 0.49053987860679626, Test Loss: 0.49079975485801697\n",
      "Epoch 1132, Train Loss: 0.4902304410934448, Test Loss: 0.49055716395378113\n",
      "Epoch 1133, Train Loss: 0.489920973777771, Test Loss: 0.49028605222702026\n",
      "Epoch 1134, Train Loss: 0.489612340927124, Test Loss: 0.49002382159233093\n",
      "Epoch 1135, Train Loss: 0.489303857088089, Test Loss: 0.48973995447158813\n",
      "Epoch 1136, Train Loss: 0.4889959394931793, Test Loss: 0.4894905388355255\n",
      "Epoch 1137, Train Loss: 0.48868852853775024, Test Loss: 0.48922014236450195\n",
      "Epoch 1138, Train Loss: 0.48838168382644653, Test Loss: 0.4889356195926666\n",
      "Epoch 1139, Train Loss: 0.48807501792907715, Test Loss: 0.48869433999061584\n",
      "Epoch 1140, Train Loss: 0.48776888847351074, Test Loss: 0.48845624923706055\n",
      "Epoch 1141, Train Loss: 0.4874632656574249, Test Loss: 0.4882003664970398\n",
      "Epoch 1142, Train Loss: 0.4871580898761749, Test Loss: 0.4879385530948639\n",
      "Epoch 1143, Train Loss: 0.48685330152511597, Test Loss: 0.48768237233161926\n",
      "Epoch 1144, Train Loss: 0.48654884099960327, Test Loss: 0.4874252676963806\n",
      "Epoch 1145, Train Loss: 0.48624494671821594, Test Loss: 0.48719340562820435\n",
      "Epoch 1146, Train Loss: 0.485941618680954, Test Loss: 0.4869404733181\n",
      "Epoch 1147, Train Loss: 0.48563826084136963, Test Loss: 0.4866599440574646\n",
      "Epoch 1148, Train Loss: 0.48533540964126587, Test Loss: 0.4863951504230499\n",
      "Epoch 1149, Train Loss: 0.4850330352783203, Test Loss: 0.48614829778671265\n",
      "Epoch 1150, Train Loss: 0.48473113775253296, Test Loss: 0.4858994483947754\n",
      "Epoch 1151, Train Loss: 0.4844295382499695, Test Loss: 0.48564884066581726\n",
      "Epoch 1152, Train Loss: 0.4841284155845642, Test Loss: 0.48539823293685913\n",
      "Epoch 1153, Train Loss: 0.4838278889656067, Test Loss: 0.48515456914901733\n",
      "Epoch 1154, Train Loss: 0.48352760076522827, Test Loss: 0.4848842918872833\n",
      "Epoch 1155, Train Loss: 0.4832281172275543, Test Loss: 0.4846425950527191\n",
      "Epoch 1156, Train Loss: 0.4829290509223938, Test Loss: 0.4843772053718567\n",
      "Epoch 1157, Train Loss: 0.4826306402683258, Test Loss: 0.4841085374355316\n",
      "Epoch 1158, Train Loss: 0.4823327958583832, Test Loss: 0.4838666021823883\n",
      "Epoch 1159, Train Loss: 0.48203548789024353, Test Loss: 0.48360171914100647\n",
      "Epoch 1160, Train Loss: 0.48173847794532776, Test Loss: 0.4833562672138214\n",
      "Epoch 1161, Train Loss: 0.4814419746398926, Test Loss: 0.4831308424472809\n",
      "Epoch 1162, Train Loss: 0.48114585876464844, Test Loss: 0.48289811611175537\n",
      "Epoch 1163, Train Loss: 0.4808505177497864, Test Loss: 0.48267003893852234\n",
      "Epoch 1164, Train Loss: 0.48055538535118103, Test Loss: 0.4824344217777252\n",
      "Epoch 1165, Train Loss: 0.48026081919670105, Test Loss: 0.4821958541870117\n",
      "Epoch 1166, Train Loss: 0.47996604442596436, Test Loss: 0.48193132877349854\n",
      "Epoch 1167, Train Loss: 0.4796719551086426, Test Loss: 0.48166725039482117\n",
      "Epoch 1168, Train Loss: 0.47937867045402527, Test Loss: 0.48143893480300903\n",
      "Epoch 1169, Train Loss: 0.4790852665901184, Test Loss: 0.4811756908893585\n",
      "Epoch 1170, Train Loss: 0.47879260778427124, Test Loss: 0.4809107780456543\n",
      "Epoch 1171, Train Loss: 0.47850045561790466, Test Loss: 0.4806791841983795\n",
      "Epoch 1172, Train Loss: 0.4782089293003082, Test Loss: 0.480446994304657\n",
      "Epoch 1173, Train Loss: 0.47791817784309387, Test Loss: 0.4802490174770355\n",
      "Epoch 1174, Train Loss: 0.47762739658355713, Test Loss: 0.47999611496925354\n",
      "Epoch 1175, Train Loss: 0.47733762860298157, Test Loss: 0.4797872006893158\n",
      "Epoch 1176, Train Loss: 0.4770478308200836, Test Loss: 0.47954779863357544\n",
      "Epoch 1177, Train Loss: 0.4767588675022125, Test Loss: 0.4793119728565216\n",
      "Epoch 1178, Train Loss: 0.4764702320098877, Test Loss: 0.47906067967414856\n",
      "Epoch 1179, Train Loss: 0.4761825203895569, Test Loss: 0.4788453280925751\n",
      "Epoch 1180, Train Loss: 0.4758949875831604, Test Loss: 0.4785923957824707\n",
      "Epoch 1181, Train Loss: 0.4756079316139221, Test Loss: 0.4783427119255066\n",
      "Epoch 1182, Train Loss: 0.4753214418888092, Test Loss: 0.47810250520706177\n",
      "Epoch 1183, Train Loss: 0.4750353991985321, Test Loss: 0.4778555631637573\n",
      "Epoch 1184, Train Loss: 0.47474992275238037, Test Loss: 0.47760623693466187\n",
      "Epoch 1185, Train Loss: 0.4744649827480316, Test Loss: 0.4773700535297394\n",
      "Epoch 1186, Train Loss: 0.4741804599761963, Test Loss: 0.4771353304386139\n",
      "Epoch 1187, Train Loss: 0.473896324634552, Test Loss: 0.47690412402153015\n",
      "Epoch 1188, Train Loss: 0.4736124575138092, Test Loss: 0.47668159008026123\n",
      "Epoch 1189, Train Loss: 0.47332918643951416, Test Loss: 0.4764552116394043\n",
      "Epoch 1190, Train Loss: 0.473046213388443, Test Loss: 0.4762430489063263\n",
      "Epoch 1191, Train Loss: 0.4727638065814972, Test Loss: 0.47600623965263367\n",
      "Epoch 1192, Train Loss: 0.47248178720474243, Test Loss: 0.47578689455986023\n",
      "Epoch 1193, Train Loss: 0.4722002148628235, Test Loss: 0.47556978464126587\n",
      "Epoch 1194, Train Loss: 0.4719192385673523, Test Loss: 0.4753478169441223\n",
      "Epoch 1195, Train Loss: 0.47163882851600647, Test Loss: 0.4751443564891815\n",
      "Epoch 1196, Train Loss: 0.47135841846466064, Test Loss: 0.4749084711074829\n",
      "Epoch 1197, Train Loss: 0.47107893228530884, Test Loss: 0.474710077047348\n",
      "Epoch 1198, Train Loss: 0.4707993268966675, Test Loss: 0.474469929933548\n",
      "Epoch 1199, Train Loss: 0.47052040696144104, Test Loss: 0.474234014749527\n",
      "Epoch 1200, Train Loss: 0.4702420234680176, Test Loss: 0.47398433089256287\n",
      "Epoch 1201, Train Loss: 0.4699641466140747, Test Loss: 0.47375959157943726\n",
      "Epoch 1202, Train Loss: 0.46968671679496765, Test Loss: 0.47354644536972046\n",
      "Epoch 1203, Train Loss: 0.4694098234176636, Test Loss: 0.47331973910331726\n",
      "Epoch 1204, Train Loss: 0.46913331747055054, Test Loss: 0.4730791449546814\n",
      "Epoch 1205, Train Loss: 0.4688572287559509, Test Loss: 0.47284945845603943\n",
      "Epoch 1206, Train Loss: 0.4685814380645752, Test Loss: 0.4726153612136841\n",
      "Epoch 1207, Train Loss: 0.46830618381500244, Test Loss: 0.472385972738266\n",
      "Epoch 1208, Train Loss: 0.4680310785770416, Test Loss: 0.4721596837043762\n",
      "Epoch 1209, Train Loss: 0.46775707602500916, Test Loss: 0.4719175398349762\n",
      "Epoch 1210, Train Loss: 0.46748247742652893, Test Loss: 0.4717242419719696\n",
      "Epoch 1211, Train Loss: 0.46720901131629944, Test Loss: 0.47149935364723206\n",
      "Epoch 1212, Train Loss: 0.4669354557991028, Test Loss: 0.4713039994239807\n",
      "Epoch 1213, Train Loss: 0.46666303277015686, Test Loss: 0.47105929255485535\n",
      "Epoch 1214, Train Loss: 0.46639031171798706, Test Loss: 0.4708719253540039\n",
      "Epoch 1215, Train Loss: 0.46611833572387695, Test Loss: 0.47065749764442444\n",
      "Epoch 1216, Train Loss: 0.4658466875553131, Test Loss: 0.47043418884277344\n",
      "Epoch 1217, Train Loss: 0.46557557582855225, Test Loss: 0.4702432453632355\n",
      "Epoch 1218, Train Loss: 0.46530476212501526, Test Loss: 0.4700080454349518\n",
      "Epoch 1219, Train Loss: 0.46503445506095886, Test Loss: 0.46977561712265015\n",
      "Epoch 1220, Train Loss: 0.4647645354270935, Test Loss: 0.46956175565719604\n",
      "Epoch 1221, Train Loss: 0.46449509263038635, Test Loss: 0.4693485200405121\n",
      "Epoch 1222, Train Loss: 0.46422627568244934, Test Loss: 0.469105988740921\n",
      "Epoch 1223, Train Loss: 0.46395787596702576, Test Loss: 0.4688948392868042\n",
      "Epoch 1224, Train Loss: 0.4636896848678589, Test Loss: 0.4686998128890991\n",
      "Epoch 1225, Train Loss: 0.4634220600128174, Test Loss: 0.468476265668869\n",
      "Epoch 1226, Train Loss: 0.46315479278564453, Test Loss: 0.4682740569114685\n",
      "Epoch 1227, Train Loss: 0.4628883898258209, Test Loss: 0.4680464565753937\n",
      "Epoch 1228, Train Loss: 0.46262240409851074, Test Loss: 0.46786099672317505\n",
      "Epoch 1229, Train Loss: 0.46235743165016174, Test Loss: 0.4676164388656616\n",
      "Epoch 1230, Train Loss: 0.46209272742271423, Test Loss: 0.4674204885959625\n",
      "Epoch 1231, Train Loss: 0.46182847023010254, Test Loss: 0.46723702549934387\n",
      "Epoch 1232, Train Loss: 0.4615650177001953, Test Loss: 0.4670281708240509\n",
      "Epoch 1233, Train Loss: 0.4613019824028015, Test Loss: 0.46682068705558777\n",
      "Epoch 1234, Train Loss: 0.46103954315185547, Test Loss: 0.466631680727005\n",
      "Epoch 1235, Train Loss: 0.46077755093574524, Test Loss: 0.46642735600471497\n",
      "Epoch 1236, Train Loss: 0.4605161249637604, Test Loss: 0.4662034511566162\n",
      "Epoch 1237, Train Loss: 0.46025532484054565, Test Loss: 0.4659864902496338\n",
      "Epoch 1238, Train Loss: 0.45999494194984436, Test Loss: 0.46578091382980347\n",
      "Epoch 1239, Train Loss: 0.45973488688468933, Test Loss: 0.4655921459197998\n",
      "Epoch 1240, Train Loss: 0.45947548747062683, Test Loss: 0.46539631485939026\n",
      "Epoch 1241, Train Loss: 0.4592165946960449, Test Loss: 0.46519383788108826\n",
      "Epoch 1242, Train Loss: 0.45895814895629883, Test Loss: 0.4649863839149475\n",
      "Epoch 1243, Train Loss: 0.4587007164955139, Test Loss: 0.4647533893585205\n",
      "Epoch 1244, Train Loss: 0.45844271779060364, Test Loss: 0.46457362174987793\n",
      "Epoch 1245, Train Loss: 0.45818549394607544, Test Loss: 0.4643990695476532\n",
      "Epoch 1246, Train Loss: 0.45792919397354126, Test Loss: 0.46420690417289734\n",
      "Epoch 1247, Train Loss: 0.4576733112335205, Test Loss: 0.4640021026134491\n",
      "Epoch 1248, Train Loss: 0.4574177861213684, Test Loss: 0.46379023790359497\n",
      "Epoch 1249, Train Loss: 0.4571627080440521, Test Loss: 0.46360230445861816\n",
      "Epoch 1250, Train Loss: 0.45690828561782837, Test Loss: 0.4634241461753845\n",
      "Epoch 1251, Train Loss: 0.45665442943573, Test Loss: 0.4632369875907898\n",
      "Epoch 1252, Train Loss: 0.45640063285827637, Test Loss: 0.4630175232887268\n",
      "Epoch 1253, Train Loss: 0.4561476409435272, Test Loss: 0.46282443404197693\n",
      "Epoch 1254, Train Loss: 0.45589497685432434, Test Loss: 0.4626000225543976\n",
      "Epoch 1255, Train Loss: 0.45564281940460205, Test Loss: 0.4623998701572418\n",
      "Epoch 1256, Train Loss: 0.45539116859436035, Test Loss: 0.4622075855731964\n",
      "Epoch 1257, Train Loss: 0.45513978600502014, Test Loss: 0.46201789379119873\n",
      "Epoch 1258, Train Loss: 0.4548890292644501, Test Loss: 0.4618142247200012\n",
      "Epoch 1259, Train Loss: 0.45463839173316956, Test Loss: 0.46163854002952576\n",
      "Epoch 1260, Train Loss: 0.45438843965530396, Test Loss: 0.46142280101776123\n",
      "Epoch 1261, Train Loss: 0.454138845205307, Test Loss: 0.4612150490283966\n",
      "Epoch 1262, Train Loss: 0.4538893699645996, Test Loss: 0.4610583186149597\n",
      "Epoch 1263, Train Loss: 0.45364052057266235, Test Loss: 0.46089425683021545\n",
      "Epoch 1264, Train Loss: 0.4533917307853699, Test Loss: 0.46069490909576416\n",
      "Epoch 1265, Train Loss: 0.45314347743988037, Test Loss: 0.46050843596458435\n",
      "Epoch 1266, Train Loss: 0.4528953731060028, Test Loss: 0.46031373739242554\n",
      "Epoch 1267, Train Loss: 0.45264777541160583, Test Loss: 0.46012091636657715\n",
      "Epoch 1268, Train Loss: 0.45240065455436707, Test Loss: 0.45990845561027527\n",
      "Epoch 1269, Train Loss: 0.4521538019180298, Test Loss: 0.45974263548851013\n",
      "Epoch 1270, Train Loss: 0.4519072473049164, Test Loss: 0.4595428705215454\n",
      "Epoch 1271, Train Loss: 0.4516613483428955, Test Loss: 0.4593522250652313\n",
      "Epoch 1272, Train Loss: 0.45141589641571045, Test Loss: 0.45920175313949585\n",
      "Epoch 1273, Train Loss: 0.4511707127094269, Test Loss: 0.4590070843696594\n",
      "Epoch 1274, Train Loss: 0.4509264826774597, Test Loss: 0.45884522795677185\n",
      "Epoch 1275, Train Loss: 0.4506816267967224, Test Loss: 0.4586603045463562\n",
      "Epoch 1276, Train Loss: 0.45043736696243286, Test Loss: 0.4584554135799408\n",
      "Epoch 1277, Train Loss: 0.45019349455833435, Test Loss: 0.45826202630996704\n",
      "Epoch 1278, Train Loss: 0.4499499201774597, Test Loss: 0.45809075236320496\n",
      "Epoch 1279, Train Loss: 0.44970637559890747, Test Loss: 0.45790135860443115\n",
      "Epoch 1280, Train Loss: 0.44946345686912537, Test Loss: 0.45771992206573486\n",
      "Epoch 1281, Train Loss: 0.44922101497650146, Test Loss: 0.4575526714324951\n",
      "Epoch 1282, Train Loss: 0.4489784836769104, Test Loss: 0.4573395252227783\n",
      "Epoch 1283, Train Loss: 0.44873642921447754, Test Loss: 0.457145094871521\n",
      "Epoch 1284, Train Loss: 0.4484947621822357, Test Loss: 0.4569600820541382\n",
      "Epoch 1285, Train Loss: 0.44825348258018494, Test Loss: 0.4567584991455078\n",
      "Epoch 1286, Train Loss: 0.4480125308036804, Test Loss: 0.4565904438495636\n",
      "Epoch 1287, Train Loss: 0.4477718770503998, Test Loss: 0.45641762018203735\n",
      "Epoch 1288, Train Loss: 0.44753193855285645, Test Loss: 0.45622000098228455\n",
      "Epoch 1289, Train Loss: 0.44729310274124146, Test Loss: 0.4559885859489441\n",
      "Epoch 1290, Train Loss: 0.4470534324645996, Test Loss: 0.45583075284957886\n",
      "Epoch 1291, Train Loss: 0.44681447744369507, Test Loss: 0.4556671380996704\n",
      "Epoch 1292, Train Loss: 0.4465760886669159, Test Loss: 0.45547598600387573\n",
      "Epoch 1293, Train Loss: 0.44633811712265015, Test Loss: 0.4552912414073944\n",
      "Epoch 1294, Train Loss: 0.4461001753807068, Test Loss: 0.4551284611225128\n",
      "Epoch 1295, Train Loss: 0.44586268067359924, Test Loss: 0.45495912432670593\n",
      "Epoch 1296, Train Loss: 0.44562551379203796, Test Loss: 0.4547842741012573\n",
      "Epoch 1297, Train Loss: 0.4453887343406677, Test Loss: 0.4546177089214325\n",
      "Epoch 1298, Train Loss: 0.4451521337032318, Test Loss: 0.4544293284416199\n",
      "Epoch 1299, Train Loss: 0.4449155032634735, Test Loss: 0.4542516767978668\n",
      "Epoch 1300, Train Loss: 0.4446793496608734, Test Loss: 0.4540571868419647\n",
      "Epoch 1301, Train Loss: 0.44444361329078674, Test Loss: 0.4538939893245697\n",
      "Epoch 1302, Train Loss: 0.44420793652534485, Test Loss: 0.45372071862220764\n",
      "Epoch 1303, Train Loss: 0.4439725875854492, Test Loss: 0.4535364508628845\n",
      "Epoch 1304, Train Loss: 0.44373783469200134, Test Loss: 0.4533744752407074\n",
      "Epoch 1305, Train Loss: 0.44350332021713257, Test Loss: 0.4531933665275574\n",
      "Epoch 1306, Train Loss: 0.4432690739631653, Test Loss: 0.4529789686203003\n",
      "Epoch 1307, Train Loss: 0.4430352449417114, Test Loss: 0.45280852913856506\n",
      "Epoch 1308, Train Loss: 0.4428018629550934, Test Loss: 0.45262610912323\n",
      "Epoch 1309, Train Loss: 0.44256919622421265, Test Loss: 0.45248857140541077\n",
      "Epoch 1310, Train Loss: 0.44233641028404236, Test Loss: 0.45229780673980713\n",
      "Epoch 1311, Train Loss: 0.4421042203903198, Test Loss: 0.4521133601665497\n",
      "Epoch 1312, Train Loss: 0.44187232851982117, Test Loss: 0.45194417238235474\n",
      "Epoch 1313, Train Loss: 0.44164109230041504, Test Loss: 0.4517991542816162\n",
      "Epoch 1314, Train Loss: 0.44141054153442383, Test Loss: 0.4516444802284241\n",
      "Epoch 1315, Train Loss: 0.4411796033382416, Test Loss: 0.4514695703983307\n",
      "Epoch 1316, Train Loss: 0.44094812870025635, Test Loss: 0.45124495029449463\n",
      "Epoch 1317, Train Loss: 0.4407179355621338, Test Loss: 0.45103779435157776\n",
      "Epoch 1318, Train Loss: 0.4404877722263336, Test Loss: 0.45086196064949036\n",
      "Epoch 1319, Train Loss: 0.44025760889053345, Test Loss: 0.45069950819015503\n",
      "Epoch 1320, Train Loss: 0.4400283992290497, Test Loss: 0.45050016045570374\n",
      "Epoch 1321, Train Loss: 0.43979889154434204, Test Loss: 0.45034798979759216\n",
      "Epoch 1322, Train Loss: 0.4395701587200165, Test Loss: 0.45016244053840637\n",
      "Epoch 1323, Train Loss: 0.4393414258956909, Test Loss: 0.4499938189983368\n",
      "Epoch 1324, Train Loss: 0.439113050699234, Test Loss: 0.44982558488845825\n",
      "Epoch 1325, Train Loss: 0.43888533115386963, Test Loss: 0.4496437609195709\n",
      "Epoch 1326, Train Loss: 0.43865758180618286, Test Loss: 0.4494778513908386\n",
      "Epoch 1327, Train Loss: 0.43843087553977966, Test Loss: 0.44928157329559326\n",
      "Epoch 1328, Train Loss: 0.4382028579711914, Test Loss: 0.4491455852985382\n",
      "Epoch 1329, Train Loss: 0.43797633051872253, Test Loss: 0.44896557927131653\n",
      "Epoch 1330, Train Loss: 0.43775030970573425, Test Loss: 0.4487746059894562\n",
      "Epoch 1331, Train Loss: 0.4375242590904236, Test Loss: 0.4486030340194702\n",
      "Epoch 1332, Train Loss: 0.4372985064983368, Test Loss: 0.4484102725982666\n",
      "Epoch 1333, Train Loss: 0.4370719790458679, Test Loss: 0.4482554793357849\n",
      "Epoch 1334, Train Loss: 0.43684592843055725, Test Loss: 0.448089063167572\n",
      "Epoch 1335, Train Loss: 0.4366202652454376, Test Loss: 0.44792163372039795\n",
      "Epoch 1336, Train Loss: 0.4363948404788971, Test Loss: 0.4477580189704895\n",
      "Epoch 1337, Train Loss: 0.43616950511932373, Test Loss: 0.4475993812084198\n",
      "Epoch 1338, Train Loss: 0.4359447658061981, Test Loss: 0.44740164279937744\n",
      "Epoch 1339, Train Loss: 0.4357200562953949, Test Loss: 0.4472764730453491\n",
      "Epoch 1340, Train Loss: 0.43549519777297974, Test Loss: 0.44706839323043823\n",
      "Epoch 1341, Train Loss: 0.4352705776691437, Test Loss: 0.446892648935318\n",
      "Epoch 1342, Train Loss: 0.43504634499549866, Test Loss: 0.44672757387161255\n",
      "Epoch 1343, Train Loss: 0.43482232093811035, Test Loss: 0.4465762674808502\n",
      "Epoch 1344, Train Loss: 0.43459823727607727, Test Loss: 0.44638699293136597\n",
      "Epoch 1345, Train Loss: 0.43437448143959045, Test Loss: 0.44622179865837097\n",
      "Epoch 1346, Train Loss: 0.43415114283561707, Test Loss: 0.44603821635246277\n",
      "Epoch 1347, Train Loss: 0.43392831087112427, Test Loss: 0.4458453059196472\n",
      "Epoch 1348, Train Loss: 0.4337051510810852, Test Loss: 0.4456869959831238\n",
      "Epoch 1349, Train Loss: 0.4334833323955536, Test Loss: 0.4454916715621948\n",
      "Epoch 1350, Train Loss: 0.4332604706287384, Test Loss: 0.44533565640449524\n",
      "Epoch 1351, Train Loss: 0.43303748965263367, Test Loss: 0.44520303606987\n",
      "Epoch 1352, Train Loss: 0.4328155219554901, Test Loss: 0.44503355026245117\n",
      "Epoch 1353, Train Loss: 0.43259382247924805, Test Loss: 0.4448498785495758\n",
      "Epoch 1354, Train Loss: 0.4323720932006836, Test Loss: 0.44468703866004944\n",
      "Epoch 1355, Train Loss: 0.4321506917476654, Test Loss: 0.4445224106311798\n",
      "Epoch 1356, Train Loss: 0.4319292902946472, Test Loss: 0.4443647563457489\n",
      "Epoch 1357, Train Loss: 0.4317081868648529, Test Loss: 0.4441748261451721\n",
      "Epoch 1358, Train Loss: 0.4314876198768616, Test Loss: 0.4440418481826782\n",
      "Epoch 1359, Train Loss: 0.4312671720981598, Test Loss: 0.443888783454895\n",
      "Epoch 1360, Train Loss: 0.4310470223426819, Test Loss: 0.4437296390533447\n",
      "Epoch 1361, Train Loss: 0.43082642555236816, Test Loss: 0.44354525208473206\n",
      "Epoch 1362, Train Loss: 0.43060654401779175, Test Loss: 0.4433871805667877\n",
      "Epoch 1363, Train Loss: 0.43038642406463623, Test Loss: 0.44321122765541077\n",
      "Epoch 1364, Train Loss: 0.430166631937027, Test Loss: 0.4430323839187622\n",
      "Epoch 1365, Train Loss: 0.4299467206001282, Test Loss: 0.44283613562583923\n",
      "Epoch 1366, Train Loss: 0.4297271966934204, Test Loss: 0.4426746964454651\n",
      "Epoch 1367, Train Loss: 0.4295077919960022, Test Loss: 0.44251367449760437\n",
      "Epoch 1368, Train Loss: 0.4292886555194855, Test Loss: 0.4423259496688843\n",
      "Epoch 1369, Train Loss: 0.4290699064731598, Test Loss: 0.4421439468860626\n",
      "Epoch 1370, Train Loss: 0.428850919008255, Test Loss: 0.4419839084148407\n",
      "Epoch 1371, Train Loss: 0.4286324679851532, Test Loss: 0.4417937695980072\n",
      "Epoch 1372, Train Loss: 0.4284135103225708, Test Loss: 0.4416598677635193\n",
      "Epoch 1373, Train Loss: 0.4281950294971466, Test Loss: 0.4415088891983032\n",
      "Epoch 1374, Train Loss: 0.4279765784740448, Test Loss: 0.4413191080093384\n",
      "Epoch 1375, Train Loss: 0.4277583956718445, Test Loss: 0.4411555826663971\n",
      "Epoch 1376, Train Loss: 0.42754024267196655, Test Loss: 0.44100815057754517\n",
      "Epoch 1377, Train Loss: 0.42732253670692444, Test Loss: 0.44087350368499756\n",
      "Epoch 1378, Train Loss: 0.4271044135093689, Test Loss: 0.440673291683197\n",
      "Epoch 1379, Train Loss: 0.4268868863582611, Test Loss: 0.44048872590065\n",
      "Epoch 1380, Train Loss: 0.4266694486141205, Test Loss: 0.4403490126132965\n",
      "Epoch 1381, Train Loss: 0.42645224928855896, Test Loss: 0.44019967317581177\n",
      "Epoch 1382, Train Loss: 0.42623549699783325, Test Loss: 0.4400501847267151\n",
      "Epoch 1383, Train Loss: 0.42601871490478516, Test Loss: 0.43987518548965454\n",
      "Epoch 1384, Train Loss: 0.4258030652999878, Test Loss: 0.43975797295570374\n",
      "Epoch 1385, Train Loss: 0.4255858361721039, Test Loss: 0.4395478069782257\n",
      "Epoch 1386, Train Loss: 0.42536941170692444, Test Loss: 0.43939223885536194\n",
      "Epoch 1387, Train Loss: 0.4251531660556793, Test Loss: 0.4392264783382416\n",
      "Epoch 1388, Train Loss: 0.42493709921836853, Test Loss: 0.4390692710876465\n",
      "Epoch 1389, Train Loss: 0.4247213304042816, Test Loss: 0.43893325328826904\n",
      "Epoch 1390, Train Loss: 0.4245056211948395, Test Loss: 0.43876954913139343\n",
      "Epoch 1391, Train Loss: 0.4242900609970093, Test Loss: 0.43859973549842834\n",
      "Epoch 1392, Train Loss: 0.42407453060150146, Test Loss: 0.4384288489818573\n",
      "Epoch 1393, Train Loss: 0.4238588511943817, Test Loss: 0.43824928998947144\n",
      "Epoch 1394, Train Loss: 0.42364346981048584, Test Loss: 0.4381032884120941\n",
      "Epoch 1395, Train Loss: 0.4234280288219452, Test Loss: 0.43793708086013794\n",
      "Epoch 1396, Train Loss: 0.42321228981018066, Test Loss: 0.4377608597278595\n",
      "Epoch 1397, Train Loss: 0.4229969084262848, Test Loss: 0.4375636577606201\n",
      "Epoch 1398, Train Loss: 0.42278170585632324, Test Loss: 0.4374019503593445\n",
      "Epoch 1399, Train Loss: 0.4225672781467438, Test Loss: 0.43720486760139465\n",
      "Epoch 1400, Train Loss: 0.4223518967628479, Test Loss: 0.43704721331596375\n",
      "Epoch 1401, Train Loss: 0.4221371114253998, Test Loss: 0.436882883310318\n",
      "Epoch 1402, Train Loss: 0.4219221770763397, Test Loss: 0.4367354214191437\n",
      "Epoch 1403, Train Loss: 0.421707421541214, Test Loss: 0.43658211827278137\n",
      "Epoch 1404, Train Loss: 0.42149320244789124, Test Loss: 0.4364033639431\n",
      "Epoch 1405, Train Loss: 0.42127907276153564, Test Loss: 0.4362224340438843\n",
      "Epoch 1406, Train Loss: 0.42106446623802185, Test Loss: 0.43607136607170105\n",
      "Epoch 1407, Train Loss: 0.42085060477256775, Test Loss: 0.43590736389160156\n",
      "Epoch 1408, Train Loss: 0.4206358790397644, Test Loss: 0.4357820749282837\n",
      "Epoch 1409, Train Loss: 0.4204217791557312, Test Loss: 0.4356466829776764\n",
      "Epoch 1410, Train Loss: 0.42020800709724426, Test Loss: 0.43550679087638855\n",
      "Epoch 1411, Train Loss: 0.4199938476085663, Test Loss: 0.4353446960449219\n",
      "Epoch 1412, Train Loss: 0.41977956891059875, Test Loss: 0.43517738580703735\n",
      "Epoch 1413, Train Loss: 0.4195658564567566, Test Loss: 0.4350510835647583\n",
      "Epoch 1414, Train Loss: 0.419351726770401, Test Loss: 0.4348670244216919\n",
      "Epoch 1415, Train Loss: 0.4191378653049469, Test Loss: 0.4347045421600342\n",
      "Epoch 1416, Train Loss: 0.4189245104789734, Test Loss: 0.4345634877681732\n",
      "Epoch 1417, Train Loss: 0.4187106192111969, Test Loss: 0.4343853294849396\n",
      "Epoch 1418, Train Loss: 0.41849714517593384, Test Loss: 0.4342087507247925\n",
      "Epoch 1419, Train Loss: 0.41828402876853943, Test Loss: 0.4340369403362274\n",
      "Epoch 1420, Train Loss: 0.418070912361145, Test Loss: 0.43387722969055176\n",
      "Epoch 1421, Train Loss: 0.4178575277328491, Test Loss: 0.4337266981601715\n",
      "Epoch 1422, Train Loss: 0.41764405369758606, Test Loss: 0.4335904121398926\n",
      "Epoch 1423, Train Loss: 0.41743117570877075, Test Loss: 0.43342024087905884\n",
      "Epoch 1424, Train Loss: 0.4172184467315674, Test Loss: 0.4332517981529236\n",
      "Epoch 1425, Train Loss: 0.417005330324173, Test Loss: 0.43310320377349854\n",
      "Epoch 1426, Train Loss: 0.41679254174232483, Test Loss: 0.432945191860199\n",
      "Epoch 1427, Train Loss: 0.4165799021720886, Test Loss: 0.4328225553035736\n",
      "Epoch 1428, Train Loss: 0.41636738181114197, Test Loss: 0.43266499042510986\n",
      "Epoch 1429, Train Loss: 0.41615501046180725, Test Loss: 0.4324987530708313\n",
      "Epoch 1430, Train Loss: 0.41594263911247253, Test Loss: 0.43236178159713745\n",
      "Epoch 1431, Train Loss: 0.41573047637939453, Test Loss: 0.4322061836719513\n",
      "Epoch 1432, Train Loss: 0.415518194437027, Test Loss: 0.43205568194389343\n",
      "Epoch 1433, Train Loss: 0.41530629992485046, Test Loss: 0.4319157302379608\n",
      "Epoch 1434, Train Loss: 0.41509509086608887, Test Loss: 0.43177512288093567\n",
      "Epoch 1435, Train Loss: 0.4148821532726288, Test Loss: 0.43157094717025757\n",
      "Epoch 1436, Train Loss: 0.4146706163883209, Test Loss: 0.4314414858818054\n",
      "Epoch 1437, Train Loss: 0.41445839405059814, Test Loss: 0.4312572777271271\n",
      "Epoch 1438, Train Loss: 0.41424673795700073, Test Loss: 0.43108928203582764\n",
      "Epoch 1439, Train Loss: 0.414035439491272, Test Loss: 0.4309046268463135\n",
      "Epoch 1440, Train Loss: 0.41382381319999695, Test Loss: 0.43077340722084045\n",
      "Epoch 1441, Train Loss: 0.4136123061180115, Test Loss: 0.4306197166442871\n",
      "Epoch 1442, Train Loss: 0.41340091824531555, Test Loss: 0.4304429292678833\n",
      "Epoch 1443, Train Loss: 0.41318994760513306, Test Loss: 0.4302711486816406\n",
      "Epoch 1444, Train Loss: 0.4129784405231476, Test Loss: 0.430158406496048\n",
      "Epoch 1445, Train Loss: 0.41276755928993225, Test Loss: 0.4299902617931366\n",
      "Epoch 1446, Train Loss: 0.4125567674636841, Test Loss: 0.4298344850540161\n",
      "Epoch 1447, Train Loss: 0.41234567761421204, Test Loss: 0.42970895767211914\n",
      "Epoch 1448, Train Loss: 0.4121347963809967, Test Loss: 0.4295419156551361\n",
      "Epoch 1449, Train Loss: 0.411923885345459, Test Loss: 0.4293990731239319\n",
      "Epoch 1450, Train Loss: 0.4117133319377899, Test Loss: 0.4292757213115692\n",
      "Epoch 1451, Train Loss: 0.4115025997161865, Test Loss: 0.4291185736656189\n",
      "Epoch 1452, Train Loss: 0.4112917482852936, Test Loss: 0.42894673347473145\n",
      "Epoch 1453, Train Loss: 0.4110807478427887, Test Loss: 0.4287863075733185\n",
      "Epoch 1454, Train Loss: 0.4108714163303375, Test Loss: 0.4286734163761139\n",
      "Epoch 1455, Train Loss: 0.4106593728065491, Test Loss: 0.4284931421279907\n",
      "Epoch 1456, Train Loss: 0.4104484021663666, Test Loss: 0.4283541440963745\n",
      "Epoch 1457, Train Loss: 0.410237580537796, Test Loss: 0.4281921684741974\n",
      "Epoch 1458, Train Loss: 0.4100262522697449, Test Loss: 0.4280249774456024\n",
      "Epoch 1459, Train Loss: 0.4098154604434967, Test Loss: 0.4278644621372223\n",
      "Epoch 1460, Train Loss: 0.409604012966156, Test Loss: 0.42768794298171997\n",
      "Epoch 1461, Train Loss: 0.40939316153526306, Test Loss: 0.42755207419395447\n",
      "Epoch 1462, Train Loss: 0.40918195247650146, Test Loss: 0.42738327383995056\n",
      "Epoch 1463, Train Loss: 0.4089711010456085, Test Loss: 0.4272390305995941\n",
      "Epoch 1464, Train Loss: 0.40875986218452454, Test Loss: 0.42706915736198425\n",
      "Epoch 1465, Train Loss: 0.40854886174201965, Test Loss: 0.4269118010997772\n",
      "Epoch 1466, Train Loss: 0.40833792090415955, Test Loss: 0.426739901304245\n",
      "Epoch 1467, Train Loss: 0.40812721848487854, Test Loss: 0.42656904458999634\n",
      "Epoch 1468, Train Loss: 0.40791574120521545, Test Loss: 0.42642495036125183\n",
      "Epoch 1469, Train Loss: 0.40770456194877625, Test Loss: 0.4262794256210327\n",
      "Epoch 1470, Train Loss: 0.4074932038784027, Test Loss: 0.42613485455513\n",
      "Epoch 1471, Train Loss: 0.4072817862033844, Test Loss: 0.4259767532348633\n",
      "Epoch 1472, Train Loss: 0.40707045793533325, Test Loss: 0.4258303940296173\n",
      "Epoch 1473, Train Loss: 0.4068591594696045, Test Loss: 0.42566144466400146\n",
      "Epoch 1474, Train Loss: 0.40664801001548767, Test Loss: 0.4255138635635376\n",
      "Epoch 1475, Train Loss: 0.4064362347126007, Test Loss: 0.42533794045448303\n",
      "Epoch 1476, Train Loss: 0.4062250554561615, Test Loss: 0.4252009689807892\n",
      "Epoch 1477, Train Loss: 0.40601345896720886, Test Loss: 0.42503854632377625\n",
      "Epoch 1478, Train Loss: 0.40580183267593384, Test Loss: 0.42488062381744385\n",
      "Epoch 1479, Train Loss: 0.405590295791626, Test Loss: 0.4247038662433624\n",
      "Epoch 1480, Train Loss: 0.4053783416748047, Test Loss: 0.4245840609073639\n",
      "Epoch 1481, Train Loss: 0.4051666557788849, Test Loss: 0.4244222640991211\n",
      "Epoch 1482, Train Loss: 0.4049549102783203, Test Loss: 0.4242478609085083\n",
      "Epoch 1483, Train Loss: 0.4047435224056244, Test Loss: 0.42407119274139404\n",
      "Epoch 1484, Train Loss: 0.40453192591667175, Test Loss: 0.42390546202659607\n",
      "Epoch 1485, Train Loss: 0.4043196141719818, Test Loss: 0.4237573742866516\n",
      "Epoch 1486, Train Loss: 0.4041074514389038, Test Loss: 0.42365556955337524\n",
      "Epoch 1487, Train Loss: 0.40389546751976013, Test Loss: 0.42346882820129395\n",
      "Epoch 1488, Train Loss: 0.4036836624145508, Test Loss: 0.4233115017414093\n",
      "Epoch 1489, Train Loss: 0.40347179770469666, Test Loss: 0.4231598675251007\n",
      "Epoch 1490, Train Loss: 0.40325990319252014, Test Loss: 0.42299866676330566\n",
      "Epoch 1491, Train Loss: 0.40304774045944214, Test Loss: 0.42285314202308655\n",
      "Epoch 1492, Train Loss: 0.402835875749588, Test Loss: 0.4226905107498169\n",
      "Epoch 1493, Train Loss: 0.4026239514350891, Test Loss: 0.42252424359321594\n",
      "Epoch 1494, Train Loss: 0.4024117887020111, Test Loss: 0.4223683476448059\n",
      "Epoch 1495, Train Loss: 0.4021994471549988, Test Loss: 0.4222061336040497\n",
      "Epoch 1496, Train Loss: 0.4019874930381775, Test Loss: 0.4220515191555023\n",
      "Epoch 1497, Train Loss: 0.4017750322818756, Test Loss: 0.42190995812416077\n",
      "Epoch 1498, Train Loss: 0.4015622138977051, Test Loss: 0.42174214124679565\n",
      "Epoch 1499, Train Loss: 0.40134918689727783, Test Loss: 0.4215874969959259\n",
      "Epoch 1500, Train Loss: 0.40113621950149536, Test Loss: 0.42148488759994507\n",
      "Epoch 1501, Train Loss: 0.4009234309196472, Test Loss: 0.4213166832923889\n",
      "Epoch 1502, Train Loss: 0.40071070194244385, Test Loss: 0.42115554213523865\n",
      "Epoch 1503, Train Loss: 0.4004978835582733, Test Loss: 0.4210003614425659\n",
      "Epoch 1504, Train Loss: 0.400285005569458, Test Loss: 0.42085492610931396\n",
      "Epoch 1505, Train Loss: 0.4000720977783203, Test Loss: 0.4206913709640503\n",
      "Epoch 1506, Train Loss: 0.39985916018486023, Test Loss: 0.4205346405506134\n",
      "Epoch 1507, Train Loss: 0.399646133184433, Test Loss: 0.4203720986843109\n",
      "Epoch 1508, Train Loss: 0.39943304657936096, Test Loss: 0.42025333642959595\n",
      "Epoch 1509, Train Loss: 0.39921998977661133, Test Loss: 0.4200979173183441\n",
      "Epoch 1510, Train Loss: 0.39900723099708557, Test Loss: 0.41992178559303284\n",
      "Epoch 1511, Train Loss: 0.3987939953804016, Test Loss: 0.4197810888290405\n",
      "Epoch 1512, Train Loss: 0.3985806703567505, Test Loss: 0.41963526606559753\n",
      "Epoch 1513, Train Loss: 0.39836758375167847, Test Loss: 0.4194897711277008\n",
      "Epoch 1514, Train Loss: 0.3981545567512512, Test Loss: 0.4193407893180847\n",
      "Epoch 1515, Train Loss: 0.397940993309021, Test Loss: 0.4191693663597107\n",
      "Epoch 1516, Train Loss: 0.3977276086807251, Test Loss: 0.4190199077129364\n",
      "Epoch 1517, Train Loss: 0.3975137770175934, Test Loss: 0.4188151955604553\n",
      "Epoch 1518, Train Loss: 0.39730003476142883, Test Loss: 0.4186549484729767\n",
      "Epoch 1519, Train Loss: 0.3970862627029419, Test Loss: 0.41852420568466187\n",
      "Epoch 1520, Train Loss: 0.39687299728393555, Test Loss: 0.41841158270835876\n",
      "Epoch 1521, Train Loss: 0.3966582715511322, Test Loss: 0.41823458671569824\n",
      "Epoch 1522, Train Loss: 0.39644452929496765, Test Loss: 0.4180872440338135\n",
      "Epoch 1523, Train Loss: 0.3962293863296509, Test Loss: 0.4178869128227234\n",
      "Epoch 1524, Train Loss: 0.3960149884223938, Test Loss: 0.4176981747150421\n",
      "Epoch 1525, Train Loss: 0.39579999446868896, Test Loss: 0.4175329804420471\n",
      "Epoch 1526, Train Loss: 0.39558514952659607, Test Loss: 0.41735661029815674\n",
      "Epoch 1527, Train Loss: 0.3953697681427002, Test Loss: 0.4172172248363495\n",
      "Epoch 1528, Train Loss: 0.3951547145843506, Test Loss: 0.4170823097229004\n",
      "Epoch 1529, Train Loss: 0.3949396014213562, Test Loss: 0.41694337129592896\n",
      "Epoch 1530, Train Loss: 0.39472439885139465, Test Loss: 0.4167898893356323\n",
      "Epoch 1531, Train Loss: 0.39451009035110474, Test Loss: 0.4166751205921173\n",
      "Epoch 1532, Train Loss: 0.3942939043045044, Test Loss: 0.4164823591709137\n",
      "Epoch 1533, Train Loss: 0.39407777786254883, Test Loss: 0.41631019115448\n",
      "Epoch 1534, Train Loss: 0.3938620984554291, Test Loss: 0.41613686084747314\n",
      "Epoch 1535, Train Loss: 0.3936462700366974, Test Loss: 0.4159741997718811\n",
      "Epoch 1536, Train Loss: 0.39343026280403137, Test Loss: 0.4158330261707306\n",
      "Epoch 1537, Train Loss: 0.3932141661643982, Test Loss: 0.41568270325660706\n",
      "Epoch 1538, Train Loss: 0.39299848675727844, Test Loss: 0.41556316614151\n",
      "Epoch 1539, Train Loss: 0.392781525850296, Test Loss: 0.41537991166114807\n",
      "Epoch 1540, Train Loss: 0.39256489276885986, Test Loss: 0.4151672124862671\n",
      "Epoch 1541, Train Loss: 0.3923482298851013, Test Loss: 0.4150024950504303\n",
      "Epoch 1542, Train Loss: 0.3921315371990204, Test Loss: 0.4148299992084503\n",
      "Epoch 1543, Train Loss: 0.3919144570827484, Test Loss: 0.41471269726753235\n",
      "Epoch 1544, Train Loss: 0.39169758558273315, Test Loss: 0.4145558774471283\n",
      "Epoch 1545, Train Loss: 0.3914799392223358, Test Loss: 0.414368599653244\n",
      "Epoch 1546, Train Loss: 0.391262412071228, Test Loss: 0.4142252206802368\n",
      "Epoch 1547, Train Loss: 0.39104482531547546, Test Loss: 0.4140833914279938\n",
      "Epoch 1548, Train Loss: 0.3908271789550781, Test Loss: 0.41392096877098083\n",
      "Epoch 1549, Train Loss: 0.3906094431877136, Test Loss: 0.41377633810043335\n",
      "Epoch 1550, Train Loss: 0.39039120078086853, Test Loss: 0.41359296441078186\n",
      "Epoch 1551, Train Loss: 0.39017364382743835, Test Loss: 0.4134577214717865\n",
      "Epoch 1552, Train Loss: 0.3899550437927246, Test Loss: 0.4132750630378723\n",
      "Epoch 1553, Train Loss: 0.38973599672317505, Test Loss: 0.41309604048728943\n",
      "Epoch 1554, Train Loss: 0.38951820135116577, Test Loss: 0.4129759669303894\n",
      "Epoch 1555, Train Loss: 0.3892984688282013, Test Loss: 0.41277506947517395\n",
      "Epoch 1556, Train Loss: 0.38907942175865173, Test Loss: 0.4125947952270508\n",
      "Epoch 1557, Train Loss: 0.3888604938983917, Test Loss: 0.4124186635017395\n",
      "Epoch 1558, Train Loss: 0.3886415660381317, Test Loss: 0.41228920221328735\n",
      "Epoch 1559, Train Loss: 0.38842201232910156, Test Loss: 0.41210034489631653\n",
      "Epoch 1560, Train Loss: 0.38820216059684753, Test Loss: 0.41191866993904114\n",
      "Epoch 1561, Train Loss: 0.387982577085495, Test Loss: 0.41179847717285156\n",
      "Epoch 1562, Train Loss: 0.3877629339694977, Test Loss: 0.4116573631763458\n",
      "Epoch 1563, Train Loss: 0.3875424563884735, Test Loss: 0.41147518157958984\n",
      "Epoch 1564, Train Loss: 0.3873216211795807, Test Loss: 0.4112944006919861\n",
      "Epoch 1565, Train Loss: 0.38710078597068787, Test Loss: 0.4111289978027344\n",
      "Epoch 1566, Train Loss: 0.3868797719478607, Test Loss: 0.4109588861465454\n",
      "Epoch 1567, Train Loss: 0.386658638715744, Test Loss: 0.41081321239471436\n",
      "Epoch 1568, Train Loss: 0.3864375054836273, Test Loss: 0.41062894463539124\n",
      "Epoch 1569, Train Loss: 0.386216402053833, Test Loss: 0.41044485569000244\n",
      "Epoch 1570, Train Loss: 0.38599416613578796, Test Loss: 0.4103234112262726\n",
      "Epoch 1571, Train Loss: 0.38577237725257874, Test Loss: 0.410179078578949\n",
      "Epoch 1572, Train Loss: 0.3855505883693695, Test Loss: 0.41001856327056885\n",
      "Epoch 1573, Train Loss: 0.3853282034397125, Test Loss: 0.4098595380783081\n",
      "Epoch 1574, Train Loss: 0.3851054906845093, Test Loss: 0.40968242287635803\n",
      "Epoch 1575, Train Loss: 0.3848828971385956, Test Loss: 0.4094725251197815\n",
      "Epoch 1576, Train Loss: 0.38466012477874756, Test Loss: 0.40932804346084595\n",
      "Epoch 1577, Train Loss: 0.38443711400032043, Test Loss: 0.40914198756217957\n",
      "Epoch 1578, Train Loss: 0.38421425223350525, Test Loss: 0.4089736044406891\n",
      "Epoch 1579, Train Loss: 0.3839912414550781, Test Loss: 0.4088036119937897\n",
      "Epoch 1580, Train Loss: 0.3837679326534271, Test Loss: 0.40867096185684204\n",
      "Epoch 1581, Train Loss: 0.3835446834564209, Test Loss: 0.40844446420669556\n",
      "Epoch 1582, Train Loss: 0.3833206295967102, Test Loss: 0.40831634402275085\n",
      "Epoch 1583, Train Loss: 0.3830968737602234, Test Loss: 0.4081405997276306\n",
      "Epoch 1584, Train Loss: 0.3828730583190918, Test Loss: 0.40801286697387695\n",
      "Epoch 1585, Train Loss: 0.3826484680175781, Test Loss: 0.40784958004951477\n",
      "Epoch 1586, Train Loss: 0.38242438435554504, Test Loss: 0.4076918065547943\n",
      "Epoch 1587, Train Loss: 0.382198691368103, Test Loss: 0.4074837863445282\n",
      "Epoch 1588, Train Loss: 0.381973534822464, Test Loss: 0.4073193073272705\n",
      "Epoch 1589, Train Loss: 0.38174861669540405, Test Loss: 0.4070888161659241\n",
      "Epoch 1590, Train Loss: 0.38152241706848145, Test Loss: 0.4069387912750244\n",
      "Epoch 1591, Train Loss: 0.3812964856624603, Test Loss: 0.4067820608615875\n",
      "Epoch 1592, Train Loss: 0.3810703158378601, Test Loss: 0.40664565563201904\n",
      "Epoch 1593, Train Loss: 0.3808441162109375, Test Loss: 0.40646329522132874\n",
      "Epoch 1594, Train Loss: 0.3806179165840149, Test Loss: 0.40631356835365295\n",
      "Epoch 1595, Train Loss: 0.38039156794548035, Test Loss: 0.40616393089294434\n",
      "Epoch 1596, Train Loss: 0.3801637887954712, Test Loss: 0.40593621134757996\n",
      "Epoch 1597, Train Loss: 0.37993666529655457, Test Loss: 0.40579086542129517\n",
      "Epoch 1598, Train Loss: 0.37970954179763794, Test Loss: 0.40564581751823425\n",
      "Epoch 1599, Train Loss: 0.3794819414615631, Test Loss: 0.40546444058418274\n",
      "Epoch 1600, Train Loss: 0.3792543113231659, Test Loss: 0.40527424216270447\n",
      "Epoch 1601, Train Loss: 0.3790268003940582, Test Loss: 0.405141681432724\n",
      "Epoch 1602, Train Loss: 0.3787994384765625, Test Loss: 0.40493348240852356\n",
      "Epoch 1603, Train Loss: 0.3785715401172638, Test Loss: 0.4047694206237793\n",
      "Epoch 1604, Train Loss: 0.3783443570137024, Test Loss: 0.4045754373073578\n",
      "Epoch 1605, Train Loss: 0.37811627984046936, Test Loss: 0.40437549352645874\n",
      "Epoch 1606, Train Loss: 0.37788739800453186, Test Loss: 0.4042122960090637\n",
      "Epoch 1607, Train Loss: 0.3776590824127197, Test Loss: 0.4040849804878235\n",
      "Epoch 1608, Train Loss: 0.37743034958839417, Test Loss: 0.4038728177547455\n",
      "Epoch 1609, Train Loss: 0.37720149755477905, Test Loss: 0.40374138951301575\n",
      "Epoch 1610, Train Loss: 0.3769727647304535, Test Loss: 0.4035714268684387\n",
      "Epoch 1611, Train Loss: 0.3767439126968384, Test Loss: 0.4033539295196533\n",
      "Epoch 1612, Train Loss: 0.3765144944190979, Test Loss: 0.4031982719898224\n",
      "Epoch 1613, Train Loss: 0.37628528475761414, Test Loss: 0.4030720591545105\n",
      "Epoch 1614, Train Loss: 0.37605592608451843, Test Loss: 0.4029235541820526\n",
      "Epoch 1615, Train Loss: 0.3758258819580078, Test Loss: 0.4027376174926758\n",
      "Epoch 1616, Train Loss: 0.37559613585472107, Test Loss: 0.4025685489177704\n",
      "Epoch 1617, Train Loss: 0.3753657639026642, Test Loss: 0.4023648202419281\n",
      "Epoch 1618, Train Loss: 0.37513577938079834, Test Loss: 0.4022206664085388\n",
      "Epoch 1619, Train Loss: 0.3749063313007355, Test Loss: 0.4020720422267914\n",
      "Epoch 1620, Train Loss: 0.37467536330223083, Test Loss: 0.4018701910972595\n",
      "Epoch 1621, Train Loss: 0.3744443655014038, Test Loss: 0.4016736149787903\n",
      "Epoch 1622, Train Loss: 0.3742130994796753, Test Loss: 0.4015042185783386\n",
      "Epoch 1623, Train Loss: 0.37398308515548706, Test Loss: 0.4013858139514923\n",
      "Epoch 1624, Train Loss: 0.3737517297267914, Test Loss: 0.40119507908821106\n",
      "Epoch 1625, Train Loss: 0.3735191822052002, Test Loss: 0.4009804129600525\n",
      "Epoch 1626, Train Loss: 0.3732873797416687, Test Loss: 0.4007965922355652\n",
      "Epoch 1627, Train Loss: 0.3730556070804596, Test Loss: 0.400625079870224\n",
      "Epoch 1628, Train Loss: 0.37282392382621765, Test Loss: 0.40044790506362915\n",
      "Epoch 1629, Train Loss: 0.3725918233394623, Test Loss: 0.40026727318763733\n",
      "Epoch 1630, Train Loss: 0.3723595440387726, Test Loss: 0.40009889006614685\n",
      "Epoch 1631, Train Loss: 0.3721277415752411, Test Loss: 0.39989832043647766\n",
      "Epoch 1632, Train Loss: 0.3718952536582947, Test Loss: 0.3997138738632202\n",
      "Epoch 1633, Train Loss: 0.3716640770435333, Test Loss: 0.399507075548172\n",
      "Epoch 1634, Train Loss: 0.371427983045578, Test Loss: 0.3994079828262329\n",
      "Epoch 1635, Train Loss: 0.3711944818496704, Test Loss: 0.3992459177970886\n",
      "Epoch 1636, Train Loss: 0.3709602952003479, Test Loss: 0.39906659722328186\n",
      "Epoch 1637, Train Loss: 0.3707262873649597, Test Loss: 0.39892682433128357\n",
      "Epoch 1638, Train Loss: 0.3704915940761566, Test Loss: 0.3987351655960083\n",
      "Epoch 1639, Train Loss: 0.37025654315948486, Test Loss: 0.3985244333744049\n",
      "Epoch 1640, Train Loss: 0.3700215220451355, Test Loss: 0.3983493745326996\n",
      "Epoch 1641, Train Loss: 0.36978617310523987, Test Loss: 0.39815112948417664\n",
      "Epoch 1642, Train Loss: 0.3695501387119293, Test Loss: 0.39802321791648865\n",
      "Epoch 1643, Train Loss: 0.3693144619464874, Test Loss: 0.3978167772293091\n",
      "Epoch 1644, Train Loss: 0.3690781891345978, Test Loss: 0.3976491391658783\n",
      "Epoch 1645, Train Loss: 0.3688415586948395, Test Loss: 0.3974821865558624\n",
      "Epoch 1646, Train Loss: 0.3686048686504364, Test Loss: 0.39731302857398987\n",
      "Epoch 1647, Train Loss: 0.36836814880371094, Test Loss: 0.3970852792263031\n",
      "Epoch 1648, Train Loss: 0.36813101172447205, Test Loss: 0.3969142735004425\n",
      "Epoch 1649, Train Loss: 0.36789456009864807, Test Loss: 0.396704763174057\n",
      "Epoch 1650, Train Loss: 0.3676562011241913, Test Loss: 0.3965565860271454\n",
      "Epoch 1651, Train Loss: 0.3674185872077942, Test Loss: 0.396391361951828\n",
      "Epoch 1652, Train Loss: 0.3671807646751404, Test Loss: 0.3961830139160156\n",
      "Epoch 1653, Train Loss: 0.36694255471229553, Test Loss: 0.3960137665271759\n",
      "Epoch 1654, Train Loss: 0.36670368909835815, Test Loss: 0.39585769176483154\n",
      "Epoch 1655, Train Loss: 0.36646533012390137, Test Loss: 0.395683228969574\n",
      "Epoch 1656, Train Loss: 0.366226464509964, Test Loss: 0.3955182433128357\n",
      "Epoch 1657, Train Loss: 0.3659871518611908, Test Loss: 0.39533060789108276\n",
      "Epoch 1658, Train Loss: 0.3657475411891937, Test Loss: 0.39511799812316895\n",
      "Epoch 1659, Train Loss: 0.36550790071487427, Test Loss: 0.3949204385280609\n",
      "Epoch 1660, Train Loss: 0.36526820063591003, Test Loss: 0.39471691846847534\n",
      "Epoch 1661, Train Loss: 0.3650280237197876, Test Loss: 0.3945605158805847\n",
      "Epoch 1662, Train Loss: 0.36478757858276367, Test Loss: 0.39438629150390625\n",
      "Epoch 1663, Train Loss: 0.36454713344573975, Test Loss: 0.39421844482421875\n",
      "Epoch 1664, Train Loss: 0.3643063008785248, Test Loss: 0.3940436542034149\n",
      "Epoch 1665, Train Loss: 0.3640652000904083, Test Loss: 0.3938141167163849\n",
      "Epoch 1666, Train Loss: 0.3638239800930023, Test Loss: 0.3936137855052948\n",
      "Epoch 1667, Train Loss: 0.36358168721199036, Test Loss: 0.39347365498542786\n",
      "Epoch 1668, Train Loss: 0.3633396029472351, Test Loss: 0.39328858256340027\n",
      "Epoch 1669, Train Loss: 0.3630986511707306, Test Loss: 0.3930474519729614\n",
      "Epoch 1670, Train Loss: 0.3628552556037903, Test Loss: 0.3928903639316559\n",
      "Epoch 1671, Train Loss: 0.3626120090484619, Test Loss: 0.3927217125892639\n",
      "Epoch 1672, Train Loss: 0.36236897110939026, Test Loss: 0.39251428842544556\n",
      "Epoch 1673, Train Loss: 0.3621257543563843, Test Loss: 0.39235880970954895\n",
      "Epoch 1674, Train Loss: 0.36188191175460815, Test Loss: 0.392174631357193\n",
      "Epoch 1675, Train Loss: 0.3616381883621216, Test Loss: 0.3919573724269867\n",
      "Epoch 1676, Train Loss: 0.3613942265510559, Test Loss: 0.3917533755302429\n",
      "Epoch 1677, Train Loss: 0.3611500859260559, Test Loss: 0.3915724456310272\n",
      "Epoch 1678, Train Loss: 0.3609054684638977, Test Loss: 0.39139309525489807\n",
      "Epoch 1679, Train Loss: 0.36066076159477234, Test Loss: 0.39123740792274475\n",
      "Epoch 1680, Train Loss: 0.36041688919067383, Test Loss: 0.39107999205589294\n",
      "Epoch 1681, Train Loss: 0.36017078161239624, Test Loss: 0.39088010787963867\n",
      "Epoch 1682, Train Loss: 0.3599258363246918, Test Loss: 0.39072078466415405\n",
      "Epoch 1683, Train Loss: 0.3596798777580261, Test Loss: 0.3905431628227234\n",
      "Epoch 1684, Train Loss: 0.35943272709846497, Test Loss: 0.3903152048587799\n",
      "Epoch 1685, Train Loss: 0.359186053276062, Test Loss: 0.39012661576271057\n",
      "Epoch 1686, Train Loss: 0.35893964767456055, Test Loss: 0.3899771273136139\n",
      "Epoch 1687, Train Loss: 0.3586924076080322, Test Loss: 0.3897368609905243\n",
      "Epoch 1688, Train Loss: 0.35844525694847107, Test Loss: 0.38955703377723694\n",
      "Epoch 1689, Train Loss: 0.35819771885871887, Test Loss: 0.3893532156944275\n",
      "Epoch 1690, Train Loss: 0.35794979333877563, Test Loss: 0.3891749382019043\n",
      "Epoch 1691, Train Loss: 0.35770168900489807, Test Loss: 0.388994038105011\n",
      "Epoch 1692, Train Loss: 0.3574534058570862, Test Loss: 0.3887600898742676\n",
      "Epoch 1693, Train Loss: 0.357204794883728, Test Loss: 0.3885544538497925\n",
      "Epoch 1694, Train Loss: 0.35695528984069824, Test Loss: 0.38837793469429016\n",
      "Epoch 1695, Train Loss: 0.35670575499534607, Test Loss: 0.3881814777851105\n",
      "Epoch 1696, Train Loss: 0.3564559519290924, Test Loss: 0.3879967927932739\n",
      "Epoch 1697, Train Loss: 0.3562055826187134, Test Loss: 0.3877962529659271\n",
      "Epoch 1698, Train Loss: 0.35595637559890747, Test Loss: 0.387558251619339\n",
      "Epoch 1699, Train Loss: 0.3557046055793762, Test Loss: 0.38738760352134705\n",
      "Epoch 1700, Train Loss: 0.3554535508155823, Test Loss: 0.3871924877166748\n",
      "Epoch 1701, Train Loss: 0.3552015721797943, Test Loss: 0.3870293200016022\n",
      "Epoch 1702, Train Loss: 0.3549502193927765, Test Loss: 0.3868160545825958\n",
      "Epoch 1703, Train Loss: 0.35469821095466614, Test Loss: 0.38662612438201904\n",
      "Epoch 1704, Train Loss: 0.3544459939002991, Test Loss: 0.38644129037857056\n",
      "Epoch 1705, Train Loss: 0.35419392585754395, Test Loss: 0.38622137904167175\n",
      "Epoch 1706, Train Loss: 0.3539407253265381, Test Loss: 0.38605400919914246\n",
      "Epoch 1707, Train Loss: 0.35368791222572327, Test Loss: 0.38587507605552673\n",
      "Epoch 1708, Train Loss: 0.353434294462204, Test Loss: 0.3856413662433624\n",
      "Epoch 1709, Train Loss: 0.35318079590797424, Test Loss: 0.38547462224960327\n",
      "Epoch 1710, Train Loss: 0.3529268503189087, Test Loss: 0.3852679431438446\n",
      "Epoch 1711, Train Loss: 0.35267266631126404, Test Loss: 0.3850894570350647\n",
      "Epoch 1712, Train Loss: 0.35241785645484924, Test Loss: 0.38486799597740173\n",
      "Epoch 1713, Train Loss: 0.3521628677845001, Test Loss: 0.38466382026672363\n",
      "Epoch 1714, Train Loss: 0.3519074320793152, Test Loss: 0.3844732940196991\n",
      "Epoch 1715, Train Loss: 0.3516519069671631, Test Loss: 0.38426247239112854\n",
      "Epoch 1716, Train Loss: 0.35139602422714233, Test Loss: 0.384074866771698\n",
      "Epoch 1717, Train Loss: 0.35114142298698425, Test Loss: 0.38383960723876953\n",
      "Epoch 1718, Train Loss: 0.350883811712265, Test Loss: 0.3836681842803955\n",
      "Epoch 1719, Train Loss: 0.35062721371650696, Test Loss: 0.3835057020187378\n",
      "Epoch 1720, Train Loss: 0.35037046670913696, Test Loss: 0.3833129405975342\n",
      "Epoch 1721, Train Loss: 0.3501133918762207, Test Loss: 0.38313043117523193\n",
      "Epoch 1722, Train Loss: 0.34985604882240295, Test Loss: 0.38292890787124634\n",
      "Epoch 1723, Train Loss: 0.3495980501174927, Test Loss: 0.3827032744884491\n",
      "Epoch 1724, Train Loss: 0.3493400812149048, Test Loss: 0.38248592615127563\n",
      "Epoch 1725, Train Loss: 0.3490820527076721, Test Loss: 0.3823189437389374\n",
      "Epoch 1726, Train Loss: 0.34882357716560364, Test Loss: 0.38207748532295227\n",
      "Epoch 1727, Train Loss: 0.3485645353794098, Test Loss: 0.38190770149230957\n",
      "Epoch 1728, Train Loss: 0.3483060300350189, Test Loss: 0.3817394971847534\n",
      "Epoch 1729, Train Loss: 0.34804660081863403, Test Loss: 0.3815327286720276\n",
      "Epoch 1730, Train Loss: 0.3477879464626312, Test Loss: 0.3813507854938507\n",
      "Epoch 1731, Train Loss: 0.3475280702114105, Test Loss: 0.38113465905189514\n",
      "Epoch 1732, Train Loss: 0.34726783633232117, Test Loss: 0.380912184715271\n",
      "Epoch 1733, Train Loss: 0.34700778126716614, Test Loss: 0.3807142376899719\n",
      "Epoch 1734, Train Loss: 0.34674757719039917, Test Loss: 0.38052406907081604\n",
      "Epoch 1735, Train Loss: 0.34648680686950684, Test Loss: 0.38033050298690796\n",
      "Epoch 1736, Train Loss: 0.34622737765312195, Test Loss: 0.38014623522758484\n",
      "Epoch 1737, Train Loss: 0.3459637761116028, Test Loss: 0.3799069821834564\n",
      "Epoch 1738, Train Loss: 0.3457012474536896, Test Loss: 0.37965449690818787\n",
      "Epoch 1739, Train Loss: 0.3454394042491913, Test Loss: 0.3794758915901184\n",
      "Epoch 1740, Train Loss: 0.34517672657966614, Test Loss: 0.37926849722862244\n",
      "Epoch 1741, Train Loss: 0.3449142575263977, Test Loss: 0.3790423572063446\n",
      "Epoch 1742, Train Loss: 0.34465107321739197, Test Loss: 0.3788333833217621\n",
      "Epoch 1743, Train Loss: 0.3443881869316101, Test Loss: 0.37858107686042786\n",
      "Epoch 1744, Train Loss: 0.34412404894828796, Test Loss: 0.3783859610557556\n",
      "Epoch 1745, Train Loss: 0.34386029839515686, Test Loss: 0.37823325395584106\n",
      "Epoch 1746, Train Loss: 0.3435954451560974, Test Loss: 0.3779948949813843\n",
      "Epoch 1747, Train Loss: 0.34333106875419617, Test Loss: 0.37780821323394775\n",
      "Epoch 1748, Train Loss: 0.34306591749191284, Test Loss: 0.37758222222328186\n",
      "Epoch 1749, Train Loss: 0.342800498008728, Test Loss: 0.3773236870765686\n",
      "Epoch 1750, Train Loss: 0.3425348699092865, Test Loss: 0.3771105408668518\n",
      "Epoch 1751, Train Loss: 0.34226900339126587, Test Loss: 0.3769046664237976\n",
      "Epoch 1752, Train Loss: 0.3420025110244751, Test Loss: 0.37673309445381165\n",
      "Epoch 1753, Train Loss: 0.3417358696460724, Test Loss: 0.37650448083877563\n",
      "Epoch 1754, Train Loss: 0.34146901965141296, Test Loss: 0.3762882351875305\n",
      "Epoch 1755, Train Loss: 0.34120187163352966, Test Loss: 0.3760833144187927\n",
      "Epoch 1756, Train Loss: 0.3409345746040344, Test Loss: 0.3758595883846283\n",
      "Epoch 1757, Train Loss: 0.340667188167572, Test Loss: 0.37569350004196167\n",
      "Epoch 1758, Train Loss: 0.3403998017311096, Test Loss: 0.3755170404911041\n",
      "Epoch 1759, Train Loss: 0.3401308059692383, Test Loss: 0.37530049681663513\n",
      "Epoch 1760, Train Loss: 0.3398619294166565, Test Loss: 0.3750816881656647\n",
      "Epoch 1761, Train Loss: 0.33959251642227173, Test Loss: 0.3748306930065155\n",
      "Epoch 1762, Train Loss: 0.33932340145111084, Test Loss: 0.37458351254463196\n",
      "Epoch 1763, Train Loss: 0.3390549421310425, Test Loss: 0.374345600605011\n",
      "Epoch 1764, Train Loss: 0.33878374099731445, Test Loss: 0.3741653561592102\n",
      "Epoch 1765, Train Loss: 0.33851343393325806, Test Loss: 0.3739682137966156\n",
      "Epoch 1766, Train Loss: 0.3382427394390106, Test Loss: 0.3737684488296509\n",
      "Epoch 1767, Train Loss: 0.33797186613082886, Test Loss: 0.3735598623752594\n",
      "Epoch 1768, Train Loss: 0.3377006947994232, Test Loss: 0.3733665943145752\n",
      "Epoch 1769, Train Loss: 0.33742913603782654, Test Loss: 0.37314534187316895\n",
      "Epoch 1770, Train Loss: 0.337156742811203, Test Loss: 0.37290525436401367\n",
      "Epoch 1771, Train Loss: 0.33688464760780334, Test Loss: 0.3726636469364166\n",
      "Epoch 1772, Train Loss: 0.3366115093231201, Test Loss: 0.37248075008392334\n",
      "Epoch 1773, Train Loss: 0.33633869886398315, Test Loss: 0.37227559089660645\n",
      "Epoch 1774, Train Loss: 0.33606499433517456, Test Loss: 0.37203237414360046\n",
      "Epoch 1775, Train Loss: 0.3357917070388794, Test Loss: 0.37186628580093384\n",
      "Epoch 1776, Train Loss: 0.33551862835884094, Test Loss: 0.3716745376586914\n",
      "Epoch 1777, Train Loss: 0.3352428078651428, Test Loss: 0.37141042947769165\n",
      "Epoch 1778, Train Loss: 0.3349681496620178, Test Loss: 0.37119343876838684\n",
      "Epoch 1779, Train Loss: 0.3346928656101227, Test Loss: 0.37102144956588745\n",
      "Epoch 1780, Train Loss: 0.3344171941280365, Test Loss: 0.3708034157752991\n",
      "Epoch 1781, Train Loss: 0.334141343832016, Test Loss: 0.3705421984195709\n",
      "Epoch 1782, Train Loss: 0.3338652551174164, Test Loss: 0.370304673910141\n",
      "Epoch 1783, Train Loss: 0.3335883915424347, Test Loss: 0.37010467052459717\n",
      "Epoch 1784, Train Loss: 0.3333115875720978, Test Loss: 0.3698815405368805\n",
      "Epoch 1785, Train Loss: 0.3330339193344116, Test Loss: 0.36967310309410095\n",
      "Epoch 1786, Train Loss: 0.3327561318874359, Test Loss: 0.36946120858192444\n",
      "Epoch 1787, Train Loss: 0.33247846364974976, Test Loss: 0.3692675828933716\n",
      "Epoch 1788, Train Loss: 0.33220174908638, Test Loss: 0.3690774142742157\n",
      "Epoch 1789, Train Loss: 0.33192163705825806, Test Loss: 0.36883020401000977\n",
      "Epoch 1790, Train Loss: 0.33164182305336, Test Loss: 0.3686017394065857\n",
      "Epoch 1791, Train Loss: 0.3313620388507843, Test Loss: 0.3683570325374603\n",
      "Epoch 1792, Train Loss: 0.3310818672180176, Test Loss: 0.36814120411872864\n",
      "Epoch 1793, Train Loss: 0.33080214262008667, Test Loss: 0.36787450313568115\n",
      "Epoch 1794, Train Loss: 0.3305218517780304, Test Loss: 0.36769264936447144\n",
      "Epoch 1795, Train Loss: 0.33024170994758606, Test Loss: 0.3674732744693756\n",
      "Epoch 1796, Train Loss: 0.3299616277217865, Test Loss: 0.3672632575035095\n",
      "Epoch 1797, Train Loss: 0.32968032360076904, Test Loss: 0.36701536178588867\n",
      "Epoch 1798, Train Loss: 0.329399049282074, Test Loss: 0.3668041229248047\n",
      "Epoch 1799, Train Loss: 0.3291163444519043, Test Loss: 0.36653587222099304\n",
      "Epoch 1800, Train Loss: 0.3288353383541107, Test Loss: 0.3663438856601715\n",
      "Epoch 1801, Train Loss: 0.3285520076751709, Test Loss: 0.36605024337768555\n",
      "Epoch 1802, Train Loss: 0.3282700479030609, Test Loss: 0.36580517888069153\n",
      "Epoch 1803, Train Loss: 0.3279865086078644, Test Loss: 0.36560195684432983\n",
      "Epoch 1804, Train Loss: 0.3277033865451813, Test Loss: 0.3653659224510193\n",
      "Epoch 1805, Train Loss: 0.32741913199424744, Test Loss: 0.3651433289051056\n",
      "Epoch 1806, Train Loss: 0.3271349370479584, Test Loss: 0.36492908000946045\n",
      "Epoch 1807, Train Loss: 0.3268508315086365, Test Loss: 0.3646828830242157\n",
      "Epoch 1808, Train Loss: 0.3265663683414459, Test Loss: 0.3644482493400574\n",
      "Epoch 1809, Train Loss: 0.326281875371933, Test Loss: 0.3642575442790985\n",
      "Epoch 1810, Train Loss: 0.3259962797164917, Test Loss: 0.3640215992927551\n",
      "Epoch 1811, Train Loss: 0.3257105350494385, Test Loss: 0.36380094289779663\n",
      "Epoch 1812, Train Loss: 0.3254242539405823, Test Loss: 0.3635757863521576\n",
      "Epoch 1813, Train Loss: 0.3251372277736664, Test Loss: 0.363328754901886\n",
      "Epoch 1814, Train Loss: 0.32485097646713257, Test Loss: 0.36312010884284973\n",
      "Epoch 1815, Train Loss: 0.32456648349761963, Test Loss: 0.3629419803619385\n",
      "Epoch 1816, Train Loss: 0.3242766857147217, Test Loss: 0.36267730593681335\n",
      "Epoch 1817, Train Loss: 0.32399091124534607, Test Loss: 0.3624720573425293\n",
      "Epoch 1818, Train Loss: 0.323700487613678, Test Loss: 0.3621983230113983\n",
      "Epoch 1819, Train Loss: 0.323410302400589, Test Loss: 0.3619289696216583\n",
      "Epoch 1820, Train Loss: 0.3231208622455597, Test Loss: 0.36168164014816284\n",
      "Epoch 1821, Train Loss: 0.32283180952072144, Test Loss: 0.36144086718559265\n",
      "Epoch 1822, Train Loss: 0.3225416839122772, Test Loss: 0.3612024784088135\n",
      "Epoch 1823, Train Loss: 0.3222526013851166, Test Loss: 0.3609215319156647\n",
      "Epoch 1824, Train Loss: 0.3219611644744873, Test Loss: 0.3607235252857208\n",
      "Epoch 1825, Train Loss: 0.32166939973831177, Test Loss: 0.36048707365989685\n",
      "Epoch 1826, Train Loss: 0.3213777542114258, Test Loss: 0.3602565824985504\n",
      "Epoch 1827, Train Loss: 0.32108569145202637, Test Loss: 0.36005154252052307\n",
      "Epoch 1828, Train Loss: 0.3207937777042389, Test Loss: 0.3597835302352905\n",
      "Epoch 1829, Train Loss: 0.3205009698867798, Test Loss: 0.35957685112953186\n",
      "Epoch 1830, Train Loss: 0.32020828127861023, Test Loss: 0.35935255885124207\n",
      "Epoch 1831, Train Loss: 0.3199152946472168, Test Loss: 0.35910657048225403\n",
      "Epoch 1832, Train Loss: 0.3196219503879547, Test Loss: 0.35886797308921814\n",
      "Epoch 1833, Train Loss: 0.3193283677101135, Test Loss: 0.3585861921310425\n",
      "Epoch 1834, Train Loss: 0.3190344274044037, Test Loss: 0.3583866059780121\n",
      "Epoch 1835, Train Loss: 0.3187404274940491, Test Loss: 0.35815683007240295\n",
      "Epoch 1836, Train Loss: 0.31844598054885864, Test Loss: 0.35790547728538513\n",
      "Epoch 1837, Train Loss: 0.3181510865688324, Test Loss: 0.35764482617378235\n",
      "Epoch 1838, Train Loss: 0.3178557753562927, Test Loss: 0.35739630460739136\n",
      "Epoch 1839, Train Loss: 0.317560076713562, Test Loss: 0.35717713832855225\n",
      "Epoch 1840, Train Loss: 0.31726446747779846, Test Loss: 0.35689330101013184\n",
      "Epoch 1841, Train Loss: 0.31696817278862, Test Loss: 0.35664844512939453\n",
      "Epoch 1842, Train Loss: 0.31667158007621765, Test Loss: 0.3564392030239105\n",
      "Epoch 1843, Train Loss: 0.3163740932941437, Test Loss: 0.3561623990535736\n",
      "Epoch 1844, Train Loss: 0.31607648730278015, Test Loss: 0.3559202551841736\n",
      "Epoch 1845, Train Loss: 0.31577855348587036, Test Loss: 0.3556709289550781\n",
      "Epoch 1846, Train Loss: 0.31548023223876953, Test Loss: 0.35542812943458557\n",
      "Epoch 1847, Train Loss: 0.3151823878288269, Test Loss: 0.3551563322544098\n",
      "Epoch 1848, Train Loss: 0.3148842453956604, Test Loss: 0.3549060523509979\n",
      "Epoch 1849, Train Loss: 0.31458571553230286, Test Loss: 0.354667991399765\n",
      "Epoch 1850, Train Loss: 0.31428632140159607, Test Loss: 0.35440605878829956\n",
      "Epoch 1851, Train Loss: 0.3139858543872833, Test Loss: 0.3541988134384155\n",
      "Epoch 1852, Train Loss: 0.31368592381477356, Test Loss: 0.3539745509624481\n",
      "Epoch 1853, Train Loss: 0.31338587403297424, Test Loss: 0.3537144660949707\n",
      "Epoch 1854, Train Loss: 0.31308722496032715, Test Loss: 0.3535304367542267\n",
      "Epoch 1855, Train Loss: 0.312784880399704, Test Loss: 0.3532169461250305\n",
      "Epoch 1856, Train Loss: 0.31248414516448975, Test Loss: 0.3529413044452667\n",
      "Epoch 1857, Train Loss: 0.31218233704566956, Test Loss: 0.3526972532272339\n",
      "Epoch 1858, Train Loss: 0.311880499124527, Test Loss: 0.35247981548309326\n",
      "Epoch 1859, Train Loss: 0.31157824397087097, Test Loss: 0.3522230088710785\n",
      "Epoch 1860, Train Loss: 0.3112756609916687, Test Loss: 0.3519444763660431\n",
      "Epoch 1861, Train Loss: 0.31097275018692017, Test Loss: 0.3516823947429657\n",
      "Epoch 1862, Train Loss: 0.31066974997520447, Test Loss: 0.35143905878067017\n",
      "Epoch 1863, Train Loss: 0.3103662133216858, Test Loss: 0.3512040376663208\n",
      "Epoch 1864, Train Loss: 0.3100615441799164, Test Loss: 0.3509358763694763\n",
      "Epoch 1865, Train Loss: 0.3097570538520813, Test Loss: 0.3506523072719574\n",
      "Epoch 1866, Train Loss: 0.30945277214050293, Test Loss: 0.35042670369148254\n",
      "Epoch 1867, Train Loss: 0.30914852023124695, Test Loss: 0.3501113951206207\n",
      "Epoch 1868, Train Loss: 0.30884358286857605, Test Loss: 0.3498358428478241\n",
      "Epoch 1869, Train Loss: 0.30853748321533203, Test Loss: 0.34960493445396423\n",
      "Epoch 1870, Train Loss: 0.3082332909107208, Test Loss: 0.34932783246040344\n",
      "Epoch 1871, Train Loss: 0.30792608857154846, Test Loss: 0.34909310936927795\n",
      "Epoch 1872, Train Loss: 0.3076203167438507, Test Loss: 0.3488299250602722\n",
      "Epoch 1873, Train Loss: 0.30731451511383057, Test Loss: 0.34857475757598877\n",
      "Epoch 1874, Train Loss: 0.30700844526290894, Test Loss: 0.3483107089996338\n",
      "Epoch 1875, Train Loss: 0.30670180916786194, Test Loss: 0.34806832671165466\n",
      "Epoch 1876, Train Loss: 0.306395024061203, Test Loss: 0.34783855080604553\n",
      "Epoch 1877, Train Loss: 0.3060871958732605, Test Loss: 0.34756380319595337\n",
      "Epoch 1878, Train Loss: 0.30577966570854187, Test Loss: 0.3473232090473175\n",
      "Epoch 1879, Train Loss: 0.3054722845554352, Test Loss: 0.34707221388816833\n",
      "Epoch 1880, Train Loss: 0.30516356229782104, Test Loss: 0.34675318002700806\n",
      "Epoch 1881, Train Loss: 0.3048553168773651, Test Loss: 0.3464997708797455\n",
      "Epoch 1882, Train Loss: 0.3045472800731659, Test Loss: 0.3462079167366028\n",
      "Epoch 1883, Train Loss: 0.30423837900161743, Test Loss: 0.34596994519233704\n",
      "Epoch 1884, Train Loss: 0.30393001437187195, Test Loss: 0.34568679332733154\n",
      "Epoch 1885, Train Loss: 0.3036200702190399, Test Loss: 0.34545645117759705\n",
      "Epoch 1886, Train Loss: 0.30331116914749146, Test Loss: 0.3451809883117676\n",
      "Epoch 1887, Train Loss: 0.3030012845993042, Test Loss: 0.3449491262435913\n",
      "Epoch 1888, Train Loss: 0.3026914894580841, Test Loss: 0.3446789085865021\n",
      "Epoch 1889, Train Loss: 0.3023824989795685, Test Loss: 0.34439557790756226\n",
      "Epoch 1890, Train Loss: 0.3020704388618469, Test Loss: 0.3441522419452667\n",
      "Epoch 1891, Train Loss: 0.3017599880695343, Test Loss: 0.34385254979133606\n",
      "Epoch 1892, Train Loss: 0.3014488220214844, Test Loss: 0.34357550740242004\n",
      "Epoch 1893, Train Loss: 0.3011372685432434, Test Loss: 0.3432992696762085\n",
      "Epoch 1894, Train Loss: 0.30082428455352783, Test Loss: 0.3430556356906891\n",
      "Epoch 1895, Train Loss: 0.30051276087760925, Test Loss: 0.3427409827709198\n",
      "Epoch 1896, Train Loss: 0.30019819736480713, Test Loss: 0.3424888849258423\n",
      "Epoch 1897, Train Loss: 0.2998840808868408, Test Loss: 0.3422527611255646\n",
      "Epoch 1898, Train Loss: 0.2995699346065521, Test Loss: 0.3419658839702606\n",
      "Epoch 1899, Train Loss: 0.2992558479309082, Test Loss: 0.3416864275932312\n",
      "Epoch 1900, Train Loss: 0.2989402711391449, Test Loss: 0.3414459526538849\n",
      "Epoch 1901, Train Loss: 0.2986246943473816, Test Loss: 0.3412058353424072\n",
      "Epoch 1902, Train Loss: 0.29830867052078247, Test Loss: 0.3409508168697357\n",
      "Epoch 1903, Train Loss: 0.29799264669418335, Test Loss: 0.34067392349243164\n",
      "Epoch 1904, Train Loss: 0.2976759076118469, Test Loss: 0.3403817415237427\n",
      "Epoch 1905, Train Loss: 0.29735925793647766, Test Loss: 0.34009042382240295\n",
      "Epoch 1906, Train Loss: 0.29704150557518005, Test Loss: 0.3398405611515045\n",
      "Epoch 1907, Train Loss: 0.2967260479927063, Test Loss: 0.3396264314651489\n",
      "Epoch 1908, Train Loss: 0.2964099049568176, Test Loss: 0.3393750786781311\n",
      "Epoch 1909, Train Loss: 0.2960898280143738, Test Loss: 0.33907297253608704\n",
      "Epoch 1910, Train Loss: 0.2957685589790344, Test Loss: 0.33873823285102844\n",
      "Epoch 1911, Train Loss: 0.2954496741294861, Test Loss: 0.3384682536125183\n",
      "Epoch 1912, Train Loss: 0.2951304018497467, Test Loss: 0.33819490671157837\n",
      "Epoch 1913, Train Loss: 0.29481086134910583, Test Loss: 0.33792245388031006\n",
      "Epoch 1914, Train Loss: 0.294491708278656, Test Loss: 0.3375965356826782\n",
      "Epoch 1915, Train Loss: 0.29417142271995544, Test Loss: 0.33731451630592346\n",
      "Epoch 1916, Train Loss: 0.29385140538215637, Test Loss: 0.33702099323272705\n",
      "Epoch 1917, Train Loss: 0.2935301959514618, Test Loss: 0.33677804470062256\n",
      "Epoch 1918, Train Loss: 0.2932090759277344, Test Loss: 0.33647090196609497\n",
      "Epoch 1919, Train Loss: 0.2928890585899353, Test Loss: 0.33615389466285706\n",
      "Epoch 1920, Train Loss: 0.2925663888454437, Test Loss: 0.33588942885398865\n",
      "Epoch 1921, Train Loss: 0.2922442555427551, Test Loss: 0.3356511890888214\n",
      "Epoch 1922, Train Loss: 0.29192206263542175, Test Loss: 0.33533892035484314\n",
      "Epoch 1923, Train Loss: 0.29159921407699585, Test Loss: 0.33509308099746704\n",
      "Epoch 1924, Train Loss: 0.29127612709999084, Test Loss: 0.33479657769203186\n",
      "Epoch 1925, Train Loss: 0.2909545600414276, Test Loss: 0.33455973863601685\n",
      "Epoch 1926, Train Loss: 0.29063037037849426, Test Loss: 0.33426764607429504\n",
      "Epoch 1927, Train Loss: 0.2903062105178833, Test Loss: 0.3339667022228241\n",
      "Epoch 1928, Train Loss: 0.2899820804595947, Test Loss: 0.33371424674987793\n",
      "Epoch 1929, Train Loss: 0.28965723514556885, Test Loss: 0.33345934748649597\n",
      "Epoch 1930, Train Loss: 0.28933185338974, Test Loss: 0.3331333100795746\n",
      "Epoch 1931, Train Loss: 0.2890063524246216, Test Loss: 0.33284786343574524\n",
      "Epoch 1932, Train Loss: 0.28868216276168823, Test Loss: 0.3326111435890198\n",
      "Epoch 1933, Train Loss: 0.2883550524711609, Test Loss: 0.33228549361228943\n",
      "Epoch 1934, Train Loss: 0.28802838921546936, Test Loss: 0.3319873511791229\n",
      "Epoch 1935, Train Loss: 0.28770315647125244, Test Loss: 0.3316482603549957\n",
      "Epoch 1936, Train Loss: 0.2873747646808624, Test Loss: 0.33140039443969727\n",
      "Epoch 1937, Train Loss: 0.28704750537872314, Test Loss: 0.3310999572277069\n",
      "Epoch 1938, Train Loss: 0.2867203652858734, Test Loss: 0.3308192193508148\n",
      "Epoch 1939, Train Loss: 0.2863931953907013, Test Loss: 0.3305337727069855\n",
      "Epoch 1940, Train Loss: 0.28606587648391724, Test Loss: 0.3302397131919861\n",
      "Epoch 1941, Train Loss: 0.2857385575771332, Test Loss: 0.32996121048927307\n",
      "Epoch 1942, Train Loss: 0.2854097783565521, Test Loss: 0.32969948649406433\n",
      "Epoch 1943, Train Loss: 0.2850811779499054, Test Loss: 0.3294060528278351\n",
      "Epoch 1944, Train Loss: 0.28475239872932434, Test Loss: 0.3290869891643524\n",
      "Epoch 1945, Train Loss: 0.2844231128692627, Test Loss: 0.32880592346191406\n",
      "Epoch 1946, Train Loss: 0.2840937674045563, Test Loss: 0.32852983474731445\n",
      "Epoch 1947, Train Loss: 0.2837637960910797, Test Loss: 0.32822999358177185\n",
      "Epoch 1948, Train Loss: 0.2834332585334778, Test Loss: 0.32793769240379333\n",
      "Epoch 1949, Train Loss: 0.283102810382843, Test Loss: 0.32766541838645935\n",
      "Epoch 1950, Train Loss: 0.2827715277671814, Test Loss: 0.3273623287677765\n",
      "Epoch 1951, Train Loss: 0.2824406325817108, Test Loss: 0.3270905613899231\n",
      "Epoch 1952, Train Loss: 0.2821092903614044, Test Loss: 0.32679077982902527\n",
      "Epoch 1953, Train Loss: 0.28177735209465027, Test Loss: 0.3264457583427429\n",
      "Epoch 1954, Train Loss: 0.2814457416534424, Test Loss: 0.3261302709579468\n",
      "Epoch 1955, Train Loss: 0.2811141014099121, Test Loss: 0.3258150517940521\n",
      "Epoch 1956, Train Loss: 0.2807835340499878, Test Loss: 0.32562002539634705\n",
      "Epoch 1957, Train Loss: 0.28045058250427246, Test Loss: 0.3253111243247986\n",
      "Epoch 1958, Train Loss: 0.2801162898540497, Test Loss: 0.3249647617340088\n",
      "Epoch 1959, Train Loss: 0.27978357672691345, Test Loss: 0.32466840744018555\n",
      "Epoch 1960, Train Loss: 0.27945059537887573, Test Loss: 0.32439324259757996\n",
      "Epoch 1961, Train Loss: 0.27911844849586487, Test Loss: 0.32412949204444885\n",
      "Epoch 1962, Train Loss: 0.2787849009037018, Test Loss: 0.3238256871700287\n",
      "Epoch 1963, Train Loss: 0.27845001220703125, Test Loss: 0.3234758973121643\n",
      "Epoch 1964, Train Loss: 0.27811628580093384, Test Loss: 0.3231646716594696\n",
      "Epoch 1965, Train Loss: 0.2777826189994812, Test Loss: 0.32282644510269165\n",
      "Epoch 1966, Train Loss: 0.2774464786052704, Test Loss: 0.32257264852523804\n",
      "Epoch 1967, Train Loss: 0.2771117091178894, Test Loss: 0.3223317265510559\n",
      "Epoch 1968, Train Loss: 0.2767755389213562, Test Loss: 0.3220227360725403\n",
      "Epoch 1969, Train Loss: 0.2764394283294678, Test Loss: 0.32169246673583984\n",
      "Epoch 1970, Train Loss: 0.2761031985282898, Test Loss: 0.3214089870452881\n",
      "Epoch 1971, Train Loss: 0.27576661109924316, Test Loss: 0.321101576089859\n",
      "Epoch 1972, Train Loss: 0.27543023228645325, Test Loss: 0.3207731544971466\n",
      "Epoch 1973, Train Loss: 0.275093138217926, Test Loss: 0.320486843585968\n",
      "Epoch 1974, Train Loss: 0.27475705742836, Test Loss: 0.3201536536216736\n",
      "Epoch 1975, Train Loss: 0.27441859245300293, Test Loss: 0.31987789273262024\n",
      "Epoch 1976, Train Loss: 0.27408087253570557, Test Loss: 0.319575697183609\n",
      "Epoch 1977, Train Loss: 0.2737453877925873, Test Loss: 0.3192913830280304\n",
      "Epoch 1978, Train Loss: 0.2734052836894989, Test Loss: 0.31898006796836853\n",
      "Epoch 1979, Train Loss: 0.2730656862258911, Test Loss: 0.31864333152770996\n",
      "Epoch 1980, Train Loss: 0.27272629737854004, Test Loss: 0.3183354437351227\n",
      "Epoch 1981, Train Loss: 0.2723870277404785, Test Loss: 0.31806808710098267\n",
      "Epoch 1982, Train Loss: 0.27204713225364685, Test Loss: 0.3177095949649811\n",
      "Epoch 1983, Train Loss: 0.271708220243454, Test Loss: 0.3173612952232361\n",
      "Epoch 1984, Train Loss: 0.2713663876056671, Test Loss: 0.31707963347435\n",
      "Epoch 1985, Train Loss: 0.2710267901420593, Test Loss: 0.31674328446388245\n",
      "Epoch 1986, Train Loss: 0.2706845998764038, Test Loss: 0.31649884581565857\n",
      "Epoch 1987, Train Loss: 0.27034321427345276, Test Loss: 0.3162190616130829\n",
      "Epoch 1988, Train Loss: 0.27000173926353455, Test Loss: 0.3159240186214447\n",
      "Epoch 1989, Train Loss: 0.2696598768234253, Test Loss: 0.31562262773513794\n",
      "Epoch 1990, Train Loss: 0.2693175673484802, Test Loss: 0.3153162896633148\n",
      "Epoch 1991, Train Loss: 0.2689763903617859, Test Loss: 0.3149499297142029\n",
      "Epoch 1992, Train Loss: 0.2686323821544647, Test Loss: 0.3146609663963318\n",
      "Epoch 1993, Train Loss: 0.26829054951667786, Test Loss: 0.31431344151496887\n",
      "Epoch 1994, Train Loss: 0.2679453492164612, Test Loss: 0.31402820348739624\n",
      "Epoch 1995, Train Loss: 0.2676007151603699, Test Loss: 0.3137546181678772\n",
      "Epoch 1996, Train Loss: 0.2672564685344696, Test Loss: 0.3134778141975403\n",
      "Epoch 1997, Train Loss: 0.2669109106063843, Test Loss: 0.313122034072876\n",
      "Epoch 1998, Train Loss: 0.26656579971313477, Test Loss: 0.31280606985092163\n",
      "Epoch 1999, Train Loss: 0.2662208378314972, Test Loss: 0.3124643862247467\n",
      "Epoch 2000, Train Loss: 0.2658747434616089, Test Loss: 0.31215113401412964\n",
      "Epoch 2001, Train Loss: 0.26552844047546387, Test Loss: 0.3118356466293335\n",
      "Epoch 2002, Train Loss: 0.2651817202568054, Test Loss: 0.31151390075683594\n",
      "Epoch 2003, Train Loss: 0.2648349106311798, Test Loss: 0.31121352314949036\n",
      "Epoch 2004, Train Loss: 0.26448750495910645, Test Loss: 0.3108603060245514\n",
      "Epoch 2005, Train Loss: 0.26413989067077637, Test Loss: 0.3105716109275818\n",
      "Epoch 2006, Train Loss: 0.26379141211509705, Test Loss: 0.31024783849716187\n",
      "Epoch 2007, Train Loss: 0.26344332098960876, Test Loss: 0.30992498993873596\n",
      "Epoch 2008, Train Loss: 0.26309457421302795, Test Loss: 0.30960914492607117\n",
      "Epoch 2009, Train Loss: 0.26274555921554565, Test Loss: 0.30931249260902405\n",
      "Epoch 2010, Train Loss: 0.2623971700668335, Test Loss: 0.30904126167297363\n",
      "Epoch 2011, Train Loss: 0.2620466947555542, Test Loss: 0.30869582295417786\n",
      "Epoch 2012, Train Loss: 0.2616965174674988, Test Loss: 0.30835074186325073\n",
      "Epoch 2013, Train Loss: 0.2613464593887329, Test Loss: 0.30801746249198914\n",
      "Epoch 2014, Train Loss: 0.2609959542751312, Test Loss: 0.3076837360858917\n",
      "Epoch 2015, Train Loss: 0.2606450617313385, Test Loss: 0.3073827624320984\n",
      "Epoch 2016, Train Loss: 0.2602936029434204, Test Loss: 0.30705827474594116\n",
      "Epoch 2017, Train Loss: 0.2599421739578247, Test Loss: 0.3067537248134613\n",
      "Epoch 2018, Train Loss: 0.2595904469490051, Test Loss: 0.30645325779914856\n",
      "Epoch 2019, Train Loss: 0.2592373192310333, Test Loss: 0.3061085045337677\n",
      "Epoch 2020, Train Loss: 0.25888484716415405, Test Loss: 0.3057405352592468\n",
      "Epoch 2021, Train Loss: 0.2585313320159912, Test Loss: 0.305447518825531\n",
      "Epoch 2022, Train Loss: 0.2581782042980194, Test Loss: 0.30508357286453247\n",
      "Epoch 2023, Train Loss: 0.2578263282775879, Test Loss: 0.30479878187179565\n",
      "Epoch 2024, Train Loss: 0.25747188925743103, Test Loss: 0.30448493361473083\n",
      "Epoch 2025, Train Loss: 0.2571173310279846, Test Loss: 0.30413928627967834\n",
      "Epoch 2026, Train Loss: 0.25676220655441284, Test Loss: 0.30382445454597473\n",
      "Epoch 2027, Train Loss: 0.25641071796417236, Test Loss: 0.3035748302936554\n",
      "Epoch 2028, Train Loss: 0.25605157017707825, Test Loss: 0.3031938970088959\n",
      "Epoch 2029, Train Loss: 0.25569647550582886, Test Loss: 0.30288049578666687\n",
      "Epoch 2030, Train Loss: 0.2553367018699646, Test Loss: 0.30246660113334656\n",
      "Epoch 2031, Train Loss: 0.2549801766872406, Test Loss: 0.30208566784858704\n",
      "Epoch 2032, Train Loss: 0.2546226382255554, Test Loss: 0.30176374316215515\n",
      "Epoch 2033, Train Loss: 0.25426414608955383, Test Loss: 0.3014662563800812\n",
      "Epoch 2034, Train Loss: 0.2539055347442627, Test Loss: 0.30112364888191223\n",
      "Epoch 2035, Train Loss: 0.25354719161987305, Test Loss: 0.3007733225822449\n",
      "Epoch 2036, Train Loss: 0.25318852066993713, Test Loss: 0.300409197807312\n",
      "Epoch 2037, Train Loss: 0.2528282403945923, Test Loss: 0.30013319849967957\n",
      "Epoch 2038, Train Loss: 0.2524685859680176, Test Loss: 0.29983729124069214\n",
      "Epoch 2039, Train Loss: 0.2521088719367981, Test Loss: 0.2995174825191498\n",
      "Epoch 2040, Train Loss: 0.2517487406730652, Test Loss: 0.29918172955513\n",
      "Epoch 2041, Train Loss: 0.25138726830482483, Test Loss: 0.29881080985069275\n",
      "Epoch 2042, Train Loss: 0.25102701783180237, Test Loss: 0.2984323799610138\n",
      "Epoch 2043, Train Loss: 0.2506650388240814, Test Loss: 0.29813671112060547\n",
      "Epoch 2044, Train Loss: 0.25030332803726196, Test Loss: 0.29779618978500366\n",
      "Epoch 2045, Train Loss: 0.2499420940876007, Test Loss: 0.2974739372730255\n",
      "Epoch 2046, Train Loss: 0.24957971274852753, Test Loss: 0.29708969593048096\n",
      "Epoch 2047, Train Loss: 0.2492169886827469, Test Loss: 0.2967693507671356\n",
      "Epoch 2048, Train Loss: 0.24885499477386475, Test Loss: 0.29649078845977783\n",
      "Epoch 2049, Train Loss: 0.24849167466163635, Test Loss: 0.29614782333374023\n",
      "Epoch 2050, Train Loss: 0.24812908470630646, Test Loss: 0.2957541346549988\n",
      "Epoch 2051, Train Loss: 0.24776515364646912, Test Loss: 0.2954346537590027\n",
      "Epoch 2052, Train Loss: 0.24740147590637207, Test Loss: 0.2951095700263977\n",
      "Epoch 2053, Train Loss: 0.24703779816627502, Test Loss: 0.2947442829608917\n",
      "Epoch 2054, Train Loss: 0.24667392671108246, Test Loss: 0.2944585382938385\n",
      "Epoch 2055, Train Loss: 0.24630896747112274, Test Loss: 0.2941029965877533\n",
      "Epoch 2056, Train Loss: 0.24594371020793915, Test Loss: 0.2937536835670471\n",
      "Epoch 2057, Train Loss: 0.24557870626449585, Test Loss: 0.2934136688709259\n",
      "Epoch 2058, Train Loss: 0.2452133595943451, Test Loss: 0.2930631935596466\n",
      "Epoch 2059, Train Loss: 0.2448481321334839, Test Loss: 0.29270943999290466\n",
      "Epoch 2060, Train Loss: 0.24448227882385254, Test Loss: 0.29237520694732666\n",
      "Epoch 2061, Train Loss: 0.24411676824092865, Test Loss: 0.2919995188713074\n",
      "Epoch 2062, Train Loss: 0.2437507063150406, Test Loss: 0.29163700342178345\n",
      "Epoch 2063, Train Loss: 0.24338465929031372, Test Loss: 0.29129090905189514\n",
      "Epoch 2064, Train Loss: 0.24301771819591522, Test Loss: 0.2909460663795471\n",
      "Epoch 2065, Train Loss: 0.2426496148109436, Test Loss: 0.2906256318092346\n",
      "Epoch 2066, Train Loss: 0.24228231608867645, Test Loss: 0.29031848907470703\n",
      "Epoch 2067, Train Loss: 0.24191510677337646, Test Loss: 0.28996583819389343\n",
      "Epoch 2068, Train Loss: 0.2415490746498108, Test Loss: 0.28966614603996277\n",
      "Epoch 2069, Train Loss: 0.24117973446846008, Test Loss: 0.2893218398094177\n",
      "Epoch 2070, Train Loss: 0.24081313610076904, Test Loss: 0.28900033235549927\n",
      "Epoch 2071, Train Loss: 0.24044300615787506, Test Loss: 0.2885574996471405\n",
      "Epoch 2072, Train Loss: 0.24007461965084076, Test Loss: 0.2882140278816223\n",
      "Epoch 2073, Train Loss: 0.23970478773117065, Test Loss: 0.2879122197628021\n",
      "Epoch 2074, Train Loss: 0.23933610320091248, Test Loss: 0.2875714898109436\n",
      "Epoch 2075, Train Loss: 0.2389668971300125, Test Loss: 0.28723111748695374\n",
      "Epoch 2076, Train Loss: 0.23859785497188568, Test Loss: 0.28689226508140564\n",
      "Epoch 2077, Train Loss: 0.2382267266511917, Test Loss: 0.28652888536453247\n",
      "Epoch 2078, Train Loss: 0.23785634338855743, Test Loss: 0.28614088892936707\n",
      "Epoch 2079, Train Loss: 0.23748627305030823, Test Loss: 0.28583475947380066\n",
      "Epoch 2080, Train Loss: 0.23711653053760529, Test Loss: 0.2854931652545929\n",
      "Epoch 2081, Train Loss: 0.23674528300762177, Test Loss: 0.28512710332870483\n",
      "Epoch 2082, Train Loss: 0.23637467622756958, Test Loss: 0.28480201959609985\n",
      "Epoch 2083, Train Loss: 0.23600398004055023, Test Loss: 0.2844708263874054\n",
      "Epoch 2084, Train Loss: 0.23563286662101746, Test Loss: 0.28413036465644836\n",
      "Epoch 2085, Train Loss: 0.23526151478290558, Test Loss: 0.2837466597557068\n",
      "Epoch 2086, Train Loss: 0.23488979041576385, Test Loss: 0.2833777368068695\n",
      "Epoch 2087, Train Loss: 0.2345183789730072, Test Loss: 0.2830117642879486\n",
      "Epoch 2088, Train Loss: 0.234145849943161, Test Loss: 0.2826860249042511\n",
      "Epoch 2089, Train Loss: 0.2337733805179596, Test Loss: 0.2823507785797119\n",
      "Epoch 2090, Train Loss: 0.23340100049972534, Test Loss: 0.28201305866241455\n",
      "Epoch 2091, Train Loss: 0.2330285906791687, Test Loss: 0.2816581130027771\n",
      "Epoch 2092, Train Loss: 0.23265491425991058, Test Loss: 0.2812972664833069\n",
      "Epoch 2093, Train Loss: 0.2322813868522644, Test Loss: 0.28090980648994446\n",
      "Epoch 2094, Train Loss: 0.23191004991531372, Test Loss: 0.2805289924144745\n",
      "Epoch 2095, Train Loss: 0.23153527081012726, Test Loss: 0.28019967675209045\n",
      "Epoch 2096, Train Loss: 0.2311612069606781, Test Loss: 0.2798691391944885\n",
      "Epoch 2097, Train Loss: 0.23078761994838715, Test Loss: 0.2795099914073944\n",
      "Epoch 2098, Train Loss: 0.23041367530822754, Test Loss: 0.27917253971099854\n",
      "Epoch 2099, Train Loss: 0.2300395369529724, Test Loss: 0.2788086533546448\n",
      "Epoch 2100, Train Loss: 0.22966621816158295, Test Loss: 0.2784440517425537\n",
      "Epoch 2101, Train Loss: 0.2292913943529129, Test Loss: 0.27808240056037903\n",
      "Epoch 2102, Train Loss: 0.22891642153263092, Test Loss: 0.2777351140975952\n",
      "Epoch 2103, Train Loss: 0.22854140400886536, Test Loss: 0.27736610174179077\n",
      "Epoch 2104, Train Loss: 0.22816748917102814, Test Loss: 0.2770349383354187\n",
      "Epoch 2105, Train Loss: 0.22779101133346558, Test Loss: 0.27663514018058777\n",
      "Epoch 2106, Train Loss: 0.22741614282131195, Test Loss: 0.2763107717037201\n",
      "Epoch 2107, Train Loss: 0.22703976929187775, Test Loss: 0.27594098448753357\n",
      "Epoch 2108, Train Loss: 0.22666428983211517, Test Loss: 0.27557072043418884\n",
      "Epoch 2109, Train Loss: 0.2262883335351944, Test Loss: 0.2752389907836914\n",
      "Epoch 2110, Train Loss: 0.2259121686220169, Test Loss: 0.2748921513557434\n",
      "Epoch 2111, Train Loss: 0.22553673386573792, Test Loss: 0.2744773328304291\n",
      "Epoch 2112, Train Loss: 0.22516068816184998, Test Loss: 0.2741336524486542\n",
      "Epoch 2113, Train Loss: 0.2247837334871292, Test Loss: 0.27376535534858704\n",
      "Epoch 2114, Train Loss: 0.22440634667873383, Test Loss: 0.2734021544456482\n",
      "Epoch 2115, Train Loss: 0.22402851283550262, Test Loss: 0.2730618417263031\n",
      "Epoch 2116, Train Loss: 0.22365052998065948, Test Loss: 0.2726837396621704\n",
      "Epoch 2117, Train Loss: 0.22327259182929993, Test Loss: 0.27233320474624634\n",
      "Epoch 2118, Train Loss: 0.22289510071277618, Test Loss: 0.27198469638824463\n",
      "Epoch 2119, Train Loss: 0.22251597046852112, Test Loss: 0.27160799503326416\n",
      "Epoch 2120, Train Loss: 0.22213715314865112, Test Loss: 0.2712301015853882\n",
      "Epoch 2121, Train Loss: 0.22175858914852142, Test Loss: 0.2708471715450287\n",
      "Epoch 2122, Train Loss: 0.221379816532135, Test Loss: 0.2704988121986389\n",
      "Epoch 2123, Train Loss: 0.2210010588169098, Test Loss: 0.2701775133609772\n",
      "Epoch 2124, Train Loss: 0.22062242031097412, Test Loss: 0.2698231041431427\n",
      "Epoch 2125, Train Loss: 0.22024358808994293, Test Loss: 0.2694215774536133\n",
      "Epoch 2126, Train Loss: 0.21986481547355652, Test Loss: 0.2690522074699402\n",
      "Epoch 2127, Train Loss: 0.2194855958223343, Test Loss: 0.2686687707901001\n",
      "Epoch 2128, Train Loss: 0.21910715103149414, Test Loss: 0.2682870626449585\n",
      "Epoch 2129, Train Loss: 0.21873046457767487, Test Loss: 0.26791098713874817\n",
      "Epoch 2130, Train Loss: 0.21834857761859894, Test Loss: 0.26755568385124207\n",
      "Epoch 2131, Train Loss: 0.21796815097332, Test Loss: 0.26718243956565857\n",
      "Epoch 2132, Train Loss: 0.21758721768856049, Test Loss: 0.2668515145778656\n",
      "Epoch 2133, Train Loss: 0.21720795333385468, Test Loss: 0.2664417624473572\n",
      "Epoch 2134, Train Loss: 0.21682727336883545, Test Loss: 0.2661252021789551\n",
      "Epoch 2135, Train Loss: 0.21644769608974457, Test Loss: 0.265750527381897\n",
      "Epoch 2136, Train Loss: 0.2160673886537552, Test Loss: 0.2653529644012451\n",
      "Epoch 2137, Train Loss: 0.21568672358989716, Test Loss: 0.2649795114994049\n",
      "Epoch 2138, Train Loss: 0.21530605852603912, Test Loss: 0.2645910084247589\n",
      "Epoch 2139, Train Loss: 0.2149263471364975, Test Loss: 0.2642580270767212\n",
      "Epoch 2140, Train Loss: 0.21454565227031708, Test Loss: 0.2638670802116394\n",
      "Epoch 2141, Train Loss: 0.2141646146774292, Test Loss: 0.2634652256965637\n",
      "Epoch 2142, Train Loss: 0.21378479897975922, Test Loss: 0.26307961344718933\n",
      "Epoch 2143, Train Loss: 0.21340356767177582, Test Loss: 0.26272091269493103\n",
      "Epoch 2144, Train Loss: 0.21302281320095062, Test Loss: 0.26235431432724\n",
      "Epoch 2145, Train Loss: 0.2126423418521881, Test Loss: 0.26199275255203247\n",
      "Epoch 2146, Train Loss: 0.21226021647453308, Test Loss: 0.261627733707428\n",
      "Epoch 2147, Train Loss: 0.21187928318977356, Test Loss: 0.26123619079589844\n",
      "Epoch 2148, Train Loss: 0.211499884724617, Test Loss: 0.26082366704940796\n",
      "Epoch 2149, Train Loss: 0.21111781895160675, Test Loss: 0.26045235991477966\n",
      "Epoch 2150, Train Loss: 0.2107343226671219, Test Loss: 0.2601085603237152\n",
      "Epoch 2151, Train Loss: 0.21035194396972656, Test Loss: 0.2597658932209015\n",
      "Epoch 2152, Train Loss: 0.20997068285942078, Test Loss: 0.2593820095062256\n",
      "Epoch 2153, Train Loss: 0.20958808064460754, Test Loss: 0.25899773836135864\n",
      "Epoch 2154, Train Loss: 0.2092057466506958, Test Loss: 0.25863805413246155\n",
      "Epoch 2155, Train Loss: 0.20882359147071838, Test Loss: 0.2582605183124542\n",
      "Epoch 2156, Train Loss: 0.2084418386220932, Test Loss: 0.2579059302806854\n",
      "Epoch 2157, Train Loss: 0.20805907249450684, Test Loss: 0.2575106918811798\n",
      "Epoch 2158, Train Loss: 0.20767638087272644, Test Loss: 0.25711116194725037\n",
      "Epoch 2159, Train Loss: 0.2072940170764923, Test Loss: 0.25673389434814453\n",
      "Epoch 2160, Train Loss: 0.20691196620464325, Test Loss: 0.2563435435295105\n",
      "Epoch 2161, Train Loss: 0.20652885735034943, Test Loss: 0.2559744715690613\n",
      "Epoch 2162, Train Loss: 0.20614571869373322, Test Loss: 0.25562500953674316\n",
      "Epoch 2163, Train Loss: 0.20576290786266327, Test Loss: 0.2552732229232788\n",
      "Epoch 2164, Train Loss: 0.20537970960140228, Test Loss: 0.25488951802253723\n",
      "Epoch 2165, Train Loss: 0.20499759912490845, Test Loss: 0.2544941008090973\n",
      "Epoch 2166, Train Loss: 0.20461304485797882, Test Loss: 0.25415122509002686\n",
      "Epoch 2167, Train Loss: 0.2042296826839447, Test Loss: 0.2537602186203003\n",
      "Epoch 2168, Train Loss: 0.20384591817855835, Test Loss: 0.25338420271873474\n",
      "Epoch 2169, Train Loss: 0.2034624069929123, Test Loss: 0.2529766857624054\n",
      "Epoch 2170, Train Loss: 0.20307837426662445, Test Loss: 0.25262323021888733\n",
      "Epoch 2171, Train Loss: 0.20269423723220825, Test Loss: 0.2522607147693634\n",
      "Epoch 2172, Train Loss: 0.20231015980243683, Test Loss: 0.2518449127674103\n",
      "Epoch 2173, Train Loss: 0.20192758738994598, Test Loss: 0.25142717361450195\n",
      "Epoch 2174, Train Loss: 0.2015416920185089, Test Loss: 0.25106796622276306\n",
      "Epoch 2175, Train Loss: 0.20115719735622406, Test Loss: 0.2506854832172394\n",
      "Epoch 2176, Train Loss: 0.20077216625213623, Test Loss: 0.2502908408641815\n",
      "Epoch 2177, Train Loss: 0.20038674771785736, Test Loss: 0.24992354214191437\n",
      "Epoch 2178, Train Loss: 0.20000211894512177, Test Loss: 0.24961702525615692\n",
      "Epoch 2179, Train Loss: 0.19961604475975037, Test Loss: 0.24923217296600342\n",
      "Epoch 2180, Train Loss: 0.19922901690006256, Test Loss: 0.24880611896514893\n",
      "Epoch 2181, Train Loss: 0.19884325563907623, Test Loss: 0.24842019379138947\n",
      "Epoch 2182, Train Loss: 0.19845785200595856, Test Loss: 0.24802544713020325\n",
      "Epoch 2183, Train Loss: 0.19807223975658417, Test Loss: 0.24766401946544647\n",
      "Epoch 2184, Train Loss: 0.19768619537353516, Test Loss: 0.24727822840213776\n",
      "Epoch 2185, Train Loss: 0.19730010628700256, Test Loss: 0.2469034492969513\n",
      "Epoch 2186, Train Loss: 0.19691458344459534, Test Loss: 0.24649223685264587\n",
      "Epoch 2187, Train Loss: 0.19652795791625977, Test Loss: 0.24614308774471283\n",
      "Epoch 2188, Train Loss: 0.19614160060882568, Test Loss: 0.245756134390831\n",
      "Epoch 2189, Train Loss: 0.19575591385364532, Test Loss: 0.24534547328948975\n",
      "Epoch 2190, Train Loss: 0.19536948204040527, Test Loss: 0.24496351182460785\n",
      "Epoch 2191, Train Loss: 0.19498398900032043, Test Loss: 0.2445818930864334\n",
      "Epoch 2192, Train Loss: 0.19459840655326843, Test Loss: 0.2441922277212143\n",
      "Epoch 2193, Train Loss: 0.19421330094337463, Test Loss: 0.24380379915237427\n",
      "Epoch 2194, Train Loss: 0.19382822513580322, Test Loss: 0.2434329092502594\n",
      "Epoch 2195, Train Loss: 0.19344334304332733, Test Loss: 0.24301430583000183\n",
      "Epoch 2196, Train Loss: 0.19305777549743652, Test Loss: 0.2426588237285614\n",
      "Epoch 2197, Train Loss: 0.1926734447479248, Test Loss: 0.2422867715358734\n",
      "Epoch 2198, Train Loss: 0.19228839874267578, Test Loss: 0.24189001321792603\n",
      "Epoch 2199, Train Loss: 0.19190393388271332, Test Loss: 0.2414911687374115\n",
      "Epoch 2200, Train Loss: 0.19151899218559265, Test Loss: 0.24113701283931732\n",
      "Epoch 2201, Train Loss: 0.1911334991455078, Test Loss: 0.24073438346385956\n",
      "Epoch 2202, Train Loss: 0.19074831902980804, Test Loss: 0.24035175144672394\n",
      "Epoch 2203, Train Loss: 0.1903655230998993, Test Loss: 0.23998743295669556\n",
      "Epoch 2204, Train Loss: 0.18997938930988312, Test Loss: 0.23958136141300201\n",
      "Epoch 2205, Train Loss: 0.18959525227546692, Test Loss: 0.23920515179634094\n",
      "Epoch 2206, Train Loss: 0.18921072781085968, Test Loss: 0.23881253600120544\n",
      "Epoch 2207, Train Loss: 0.1888280063867569, Test Loss: 0.2383846938610077\n",
      "Epoch 2208, Train Loss: 0.18844208121299744, Test Loss: 0.23805102705955505\n",
      "Epoch 2209, Train Loss: 0.18805831670761108, Test Loss: 0.2376757264137268\n",
      "Epoch 2210, Train Loss: 0.18767590820789337, Test Loss: 0.2373095601797104\n",
      "Epoch 2211, Train Loss: 0.18729005753993988, Test Loss: 0.23689508438110352\n",
      "Epoch 2212, Train Loss: 0.18690623342990875, Test Loss: 0.23646967113018036\n",
      "Epoch 2213, Train Loss: 0.18652114272117615, Test Loss: 0.23608621954917908\n",
      "Epoch 2214, Train Loss: 0.18613633513450623, Test Loss: 0.23570185899734497\n",
      "Epoch 2215, Train Loss: 0.18575267493724823, Test Loss: 0.23533594608306885\n",
      "Epoch 2216, Train Loss: 0.18536797165870667, Test Loss: 0.2349119782447815\n",
      "Epoch 2217, Train Loss: 0.18498554825782776, Test Loss: 0.23450934886932373\n",
      "Epoch 2218, Train Loss: 0.18459975719451904, Test Loss: 0.23415204882621765\n",
      "Epoch 2219, Train Loss: 0.18421609699726105, Test Loss: 0.23374497890472412\n",
      "Epoch 2220, Train Loss: 0.1838323175907135, Test Loss: 0.23334649205207825\n",
      "Epoch 2221, Train Loss: 0.1834487020969391, Test Loss: 0.23293934762477875\n",
      "Epoch 2222, Train Loss: 0.18306440114974976, Test Loss: 0.2325599044561386\n",
      "Epoch 2223, Train Loss: 0.1826811283826828, Test Loss: 0.23218725621700287\n",
      "Epoch 2224, Train Loss: 0.1822975128889084, Test Loss: 0.2317955493927002\n",
      "Epoch 2225, Train Loss: 0.1819150745868683, Test Loss: 0.23138774931430817\n",
      "Epoch 2226, Train Loss: 0.18153244256973267, Test Loss: 0.2310066521167755\n",
      "Epoch 2227, Train Loss: 0.18115119636058807, Test Loss: 0.23061522841453552\n",
      "Epoch 2228, Train Loss: 0.18076975643634796, Test Loss: 0.23023179173469543\n",
      "Epoch 2229, Train Loss: 0.18038822710514069, Test Loss: 0.2298443466424942\n",
      "Epoch 2230, Train Loss: 0.18000757694244385, Test Loss: 0.22943390905857086\n",
      "Epoch 2231, Train Loss: 0.17962729930877686, Test Loss: 0.22903501987457275\n",
      "Epoch 2232, Train Loss: 0.17924433946609497, Test Loss: 0.22868284583091736\n",
      "Epoch 2233, Train Loss: 0.1788635104894638, Test Loss: 0.22830712795257568\n",
      "Epoch 2234, Train Loss: 0.178482323884964, Test Loss: 0.22792182862758636\n",
      "Epoch 2235, Train Loss: 0.1781020313501358, Test Loss: 0.2275538593530655\n",
      "Epoch 2236, Train Loss: 0.17772144079208374, Test Loss: 0.22716258466243744\n",
      "Epoch 2237, Train Loss: 0.17734135687351227, Test Loss: 0.22674573957920074\n",
      "Epoch 2238, Train Loss: 0.17696250975131989, Test Loss: 0.2263525128364563\n",
      "Epoch 2239, Train Loss: 0.1765831708908081, Test Loss: 0.22600018978118896\n",
      "Epoch 2240, Train Loss: 0.17620530724525452, Test Loss: 0.2255754917860031\n",
      "Epoch 2241, Train Loss: 0.17582698166370392, Test Loss: 0.2251880168914795\n",
      "Epoch 2242, Train Loss: 0.1754489541053772, Test Loss: 0.22485290467739105\n",
      "Epoch 2243, Train Loss: 0.17507049441337585, Test Loss: 0.224466934800148\n",
      "Epoch 2244, Train Loss: 0.17469418048858643, Test Loss: 0.22408099472522736\n",
      "Epoch 2245, Train Loss: 0.17431776225566864, Test Loss: 0.2236984670162201\n",
      "Epoch 2246, Train Loss: 0.17394201457500458, Test Loss: 0.22330519556999207\n",
      "Epoch 2247, Train Loss: 0.1735663115978241, Test Loss: 0.22291694581508636\n",
      "Epoch 2248, Train Loss: 0.1731913983821869, Test Loss: 0.22251974046230316\n",
      "Epoch 2249, Train Loss: 0.17281736433506012, Test Loss: 0.22211961448192596\n",
      "Epoch 2250, Train Loss: 0.17244239151477814, Test Loss: 0.22176344692707062\n",
      "Epoch 2251, Train Loss: 0.1720687448978424, Test Loss: 0.22138385474681854\n",
      "Epoch 2252, Train Loss: 0.17169564962387085, Test Loss: 0.22101236879825592\n",
      "Epoch 2253, Train Loss: 0.17132237553596497, Test Loss: 0.22059190273284912\n",
      "Epoch 2254, Train Loss: 0.17094895243644714, Test Loss: 0.22023195028305054\n",
      "Epoch 2255, Train Loss: 0.17057661712169647, Test Loss: 0.21983756124973297\n",
      "Epoch 2256, Train Loss: 0.1702043116092682, Test Loss: 0.21945050358772278\n",
      "Epoch 2257, Train Loss: 0.16983242332935333, Test Loss: 0.21909455955028534\n",
      "Epoch 2258, Train Loss: 0.169460728764534, Test Loss: 0.21870075166225433\n",
      "Epoch 2259, Train Loss: 0.16909144818782806, Test Loss: 0.218299999833107\n",
      "Epoch 2260, Train Loss: 0.16871784627437592, Test Loss: 0.2179456651210785\n",
      "Epoch 2261, Train Loss: 0.16834650933742523, Test Loss: 0.2175920158624649\n",
      "Epoch 2262, Train Loss: 0.16797639429569244, Test Loss: 0.21720771491527557\n",
      "Epoch 2263, Train Loss: 0.16760654747486115, Test Loss: 0.21677903831005096\n",
      "Epoch 2264, Train Loss: 0.16723798215389252, Test Loss: 0.21641330420970917\n",
      "Epoch 2265, Train Loss: 0.1668689250946045, Test Loss: 0.21599671244621277\n",
      "Epoch 2266, Train Loss: 0.16650083661079407, Test Loss: 0.21563591063022614\n",
      "Epoch 2267, Train Loss: 0.16613300144672394, Test Loss: 0.21529261767864227\n",
      "Epoch 2268, Train Loss: 0.16576628386974335, Test Loss: 0.21490749716758728\n",
      "Epoch 2269, Train Loss: 0.16540057957172394, Test Loss: 0.21451686322689056\n",
      "Epoch 2270, Train Loss: 0.1650354415178299, Test Loss: 0.21413929760456085\n",
      "Epoch 2271, Train Loss: 0.16467170417308807, Test Loss: 0.21375098824501038\n",
      "Epoch 2272, Train Loss: 0.1643069088459015, Test Loss: 0.21338973939418793\n",
      "Epoch 2273, Train Loss: 0.1639433056116104, Test Loss: 0.21302005648612976\n",
      "Epoch 2274, Train Loss: 0.16357992589473724, Test Loss: 0.21265417337417603\n",
      "Epoch 2275, Train Loss: 0.16321757435798645, Test Loss: 0.2122831493616104\n",
      "Epoch 2276, Train Loss: 0.16285507380962372, Test Loss: 0.2118784338235855\n",
      "Epoch 2277, Train Loss: 0.1624944508075714, Test Loss: 0.211525559425354\n",
      "Epoch 2278, Train Loss: 0.16213443875312805, Test Loss: 0.21114760637283325\n",
      "Epoch 2279, Train Loss: 0.16177457571029663, Test Loss: 0.21077990531921387\n",
      "Epoch 2280, Train Loss: 0.16141581535339355, Test Loss: 0.2104155421257019\n",
      "Epoch 2281, Train Loss: 0.16105809807777405, Test Loss: 0.21004612743854523\n",
      "Epoch 2282, Train Loss: 0.1607004702091217, Test Loss: 0.20964892208576202\n",
      "Epoch 2283, Train Loss: 0.16034424304962158, Test Loss: 0.20930036902427673\n",
      "Epoch 2284, Train Loss: 0.159987673163414, Test Loss: 0.2089182734489441\n",
      "Epoch 2285, Train Loss: 0.15963196754455566, Test Loss: 0.20853734016418457\n",
      "Epoch 2286, Train Loss: 0.1592770218849182, Test Loss: 0.20816940069198608\n",
      "Epoch 2287, Train Loss: 0.15892280638217926, Test Loss: 0.20778363943099976\n",
      "Epoch 2288, Train Loss: 0.15856865048408508, Test Loss: 0.20742841064929962\n",
      "Epoch 2289, Train Loss: 0.1582149863243103, Test Loss: 0.207040935754776\n",
      "Epoch 2290, Train Loss: 0.15786191821098328, Test Loss: 0.2066570222377777\n",
      "Epoch 2291, Train Loss: 0.15751023590564728, Test Loss: 0.20627255737781525\n",
      "Epoch 2292, Train Loss: 0.15715783834457397, Test Loss: 0.20593371987342834\n",
      "Epoch 2293, Train Loss: 0.1568087637424469, Test Loss: 0.20554238557815552\n",
      "Epoch 2294, Train Loss: 0.15645618736743927, Test Loss: 0.20519967377185822\n",
      "Epoch 2295, Train Loss: 0.15610633790493011, Test Loss: 0.20484033226966858\n",
      "Epoch 2296, Train Loss: 0.15575797855854034, Test Loss: 0.2044733315706253\n",
      "Epoch 2297, Train Loss: 0.15541037917137146, Test Loss: 0.20408540964126587\n",
      "Epoch 2298, Train Loss: 0.1550615429878235, Test Loss: 0.20376837253570557\n",
      "Epoch 2299, Train Loss: 0.1547146588563919, Test Loss: 0.20337586104869843\n",
      "Epoch 2300, Train Loss: 0.15436798334121704, Test Loss: 0.20302417874336243\n",
      "Epoch 2301, Train Loss: 0.15402273833751678, Test Loss: 0.20264166593551636\n",
      "Epoch 2302, Train Loss: 0.1536777764558792, Test Loss: 0.20229385793209076\n",
      "Epoch 2303, Train Loss: 0.15333381295204163, Test Loss: 0.20193719863891602\n",
      "Epoch 2304, Train Loss: 0.15299049019813538, Test Loss: 0.20158271491527557\n",
      "Epoch 2305, Train Loss: 0.15264780819416046, Test Loss: 0.2012253701686859\n",
      "Epoch 2306, Train Loss: 0.15230624377727509, Test Loss: 0.2008705884218216\n",
      "Epoch 2307, Train Loss: 0.15196488797664642, Test Loss: 0.20052386820316315\n",
      "Epoch 2308, Train Loss: 0.15162424743175507, Test Loss: 0.2001553773880005\n",
      "Epoch 2309, Train Loss: 0.15128475427627563, Test Loss: 0.19976706802845\n",
      "Epoch 2310, Train Loss: 0.1509457230567932, Test Loss: 0.19942744076251984\n",
      "Epoch 2311, Train Loss: 0.15060673654079437, Test Loss: 0.19909074902534485\n",
      "Epoch 2312, Train Loss: 0.1502687782049179, Test Loss: 0.1987295299768448\n",
      "Epoch 2313, Train Loss: 0.149931862950325, Test Loss: 0.19837720692157745\n",
      "Epoch 2314, Train Loss: 0.1495954692363739, Test Loss: 0.19803085923194885\n",
      "Epoch 2315, Train Loss: 0.1492605358362198, Test Loss: 0.19767314195632935\n",
      "Epoch 2316, Train Loss: 0.14892564713954926, Test Loss: 0.19731901586055756\n",
      "Epoch 2317, Train Loss: 0.14859214425086975, Test Loss: 0.196987584233284\n",
      "Epoch 2318, Train Loss: 0.14826050400733948, Test Loss: 0.1966381072998047\n",
      "Epoch 2319, Train Loss: 0.14792591333389282, Test Loss: 0.19625580310821533\n",
      "Epoch 2320, Train Loss: 0.14759403467178345, Test Loss: 0.19590574502944946\n",
      "Epoch 2321, Train Loss: 0.14726315438747406, Test Loss: 0.19556677341461182\n",
      "Epoch 2322, Train Loss: 0.1469331979751587, Test Loss: 0.19522283971309662\n",
      "Epoch 2323, Train Loss: 0.14660488069057465, Test Loss: 0.19491255283355713\n",
      "Epoch 2324, Train Loss: 0.1462763398885727, Test Loss: 0.19458089768886566\n",
      "Epoch 2325, Train Loss: 0.1459476351737976, Test Loss: 0.19421705603599548\n",
      "Epoch 2326, Train Loss: 0.145620197057724, Test Loss: 0.1938457190990448\n",
      "Epoch 2327, Train Loss: 0.14529360830783844, Test Loss: 0.19350336492061615\n",
      "Epoch 2328, Train Loss: 0.14496773481369019, Test Loss: 0.19314813613891602\n",
      "Epoch 2329, Train Loss: 0.14464285969734192, Test Loss: 0.19279688596725464\n",
      "Epoch 2330, Train Loss: 0.14431916177272797, Test Loss: 0.1924484223127365\n",
      "Epoch 2331, Train Loss: 0.14399591088294983, Test Loss: 0.19212724268436432\n",
      "Epoch 2332, Train Loss: 0.143673837184906, Test Loss: 0.19176936149597168\n",
      "Epoch 2333, Train Loss: 0.1433531492948532, Test Loss: 0.19141066074371338\n",
      "Epoch 2334, Train Loss: 0.14303229749202728, Test Loss: 0.1910679042339325\n",
      "Epoch 2335, Train Loss: 0.14271274209022522, Test Loss: 0.1907154619693756\n",
      "Epoch 2336, Train Loss: 0.14239418506622314, Test Loss: 0.19040057063102722\n",
      "Epoch 2337, Train Loss: 0.1420767605304718, Test Loss: 0.19004465639591217\n",
      "Epoch 2338, Train Loss: 0.14175967872142792, Test Loss: 0.18971054255962372\n",
      "Epoch 2339, Train Loss: 0.14144432544708252, Test Loss: 0.18943265080451965\n",
      "Epoch 2340, Train Loss: 0.1411287486553192, Test Loss: 0.1890784353017807\n",
      "Epoch 2341, Train Loss: 0.1408146321773529, Test Loss: 0.18869611620903015\n",
      "Epoch 2342, Train Loss: 0.14050082862377167, Test Loss: 0.1883796900510788\n",
      "Epoch 2343, Train Loss: 0.14018797874450684, Test Loss: 0.18807470798492432\n",
      "Epoch 2344, Train Loss: 0.1398756206035614, Test Loss: 0.1877434104681015\n",
      "Epoch 2345, Train Loss: 0.13956421613693237, Test Loss: 0.18740160763263702\n",
      "Epoch 2346, Train Loss: 0.13925352692604065, Test Loss: 0.18707044422626495\n",
      "Epoch 2347, Train Loss: 0.1389446258544922, Test Loss: 0.18675680458545685\n",
      "Epoch 2348, Train Loss: 0.13863542675971985, Test Loss: 0.18643499910831451\n",
      "Epoch 2349, Train Loss: 0.13832761347293854, Test Loss: 0.18609113991260529\n",
      "Epoch 2350, Train Loss: 0.13802048563957214, Test Loss: 0.1857566237449646\n",
      "Epoch 2351, Train Loss: 0.1377142369747162, Test Loss: 0.18543201684951782\n",
      "Epoch 2352, Train Loss: 0.1374102234840393, Test Loss: 0.18507292866706848\n",
      "Epoch 2353, Train Loss: 0.13710419833660126, Test Loss: 0.18476985394954681\n",
      "Epoch 2354, Train Loss: 0.1368013620376587, Test Loss: 0.18445463478565216\n",
      "Epoch 2355, Train Loss: 0.13649752736091614, Test Loss: 0.18414169549942017\n",
      "Epoch 2356, Train Loss: 0.1361953765153885, Test Loss: 0.1838143765926361\n",
      "Epoch 2357, Train Loss: 0.13589520752429962, Test Loss: 0.18350794911384583\n",
      "Epoch 2358, Train Loss: 0.1355956643819809, Test Loss: 0.1831861138343811\n",
      "Epoch 2359, Train Loss: 0.13529518246650696, Test Loss: 0.18284186720848083\n",
      "Epoch 2360, Train Loss: 0.13499553501605988, Test Loss: 0.18254423141479492\n",
      "Epoch 2361, Train Loss: 0.13469725847244263, Test Loss: 0.1822301149368286\n",
      "Epoch 2362, Train Loss: 0.13439978659152985, Test Loss: 0.1818993240594864\n",
      "Epoch 2363, Train Loss: 0.13410349190235138, Test Loss: 0.18157628178596497\n",
      "Epoch 2364, Train Loss: 0.13380840420722961, Test Loss: 0.18125931918621063\n",
      "Epoch 2365, Train Loss: 0.13351264595985413, Test Loss: 0.18095575273036957\n",
      "Epoch 2366, Train Loss: 0.13321839272975922, Test Loss: 0.18064339458942413\n",
      "Epoch 2367, Train Loss: 0.1329251229763031, Test Loss: 0.18035663664340973\n",
      "Epoch 2368, Train Loss: 0.13263274729251862, Test Loss: 0.18005485832691193\n",
      "Epoch 2369, Train Loss: 0.13234148919582367, Test Loss: 0.17974179983139038\n",
      "Epoch 2370, Train Loss: 0.13205023109912872, Test Loss: 0.17940492928028107\n",
      "Epoch 2371, Train Loss: 0.13176023960113525, Test Loss: 0.17908087372779846\n",
      "Epoch 2372, Train Loss: 0.13147114217281342, Test Loss: 0.17875584959983826\n",
      "Epoch 2373, Train Loss: 0.1311834156513214, Test Loss: 0.17845219373703003\n",
      "Epoch 2374, Train Loss: 0.1308954507112503, Test Loss: 0.1781669706106186\n",
      "Epoch 2375, Train Loss: 0.1306089460849762, Test Loss: 0.17784589529037476\n",
      "Epoch 2376, Train Loss: 0.13032300770282745, Test Loss: 0.17752774059772491\n",
      "Epoch 2377, Train Loss: 0.1300380527973175, Test Loss: 0.1772250086069107\n",
      "Epoch 2378, Train Loss: 0.1297542154788971, Test Loss: 0.17690205574035645\n",
      "Epoch 2379, Train Loss: 0.1294715404510498, Test Loss: 0.17659862339496613\n",
      "Epoch 2380, Train Loss: 0.12918880581855774, Test Loss: 0.17631268501281738\n",
      "Epoch 2381, Train Loss: 0.1289077252149582, Test Loss: 0.17603254318237305\n",
      "Epoch 2382, Train Loss: 0.12862727046012878, Test Loss: 0.17571894824504852\n",
      "Epoch 2383, Train Loss: 0.12834784388542175, Test Loss: 0.17542745172977448\n",
      "Epoch 2384, Train Loss: 0.1280689388513565, Test Loss: 0.1751316636800766\n",
      "Epoch 2385, Train Loss: 0.12779177725315094, Test Loss: 0.17482736706733704\n",
      "Epoch 2386, Train Loss: 0.12751543521881104, Test Loss: 0.17451782524585724\n",
      "Epoch 2387, Train Loss: 0.12723854184150696, Test Loss: 0.17421819269657135\n",
      "Epoch 2388, Train Loss: 0.12696297466754913, Test Loss: 0.17392374575138092\n",
      "Epoch 2389, Train Loss: 0.12668995559215546, Test Loss: 0.17365644872188568\n",
      "Epoch 2390, Train Loss: 0.12641488015651703, Test Loss: 0.17334690690040588\n",
      "Epoch 2391, Train Loss: 0.1261424720287323, Test Loss: 0.17306435108184814\n",
      "Epoch 2392, Train Loss: 0.12586943805217743, Test Loss: 0.1727544367313385\n",
      "Epoch 2393, Train Loss: 0.12559816241264343, Test Loss: 0.17244583368301392\n",
      "Epoch 2394, Train Loss: 0.12532766163349152, Test Loss: 0.17218071222305298\n",
      "Epoch 2395, Train Loss: 0.12505772709846497, Test Loss: 0.17188632488250732\n",
      "Epoch 2396, Train Loss: 0.12478896975517273, Test Loss: 0.17156586050987244\n",
      "Epoch 2397, Train Loss: 0.12452049553394318, Test Loss: 0.17128555476665497\n",
      "Epoch 2398, Train Loss: 0.12425298243761063, Test Loss: 0.17099054157733917\n",
      "Epoch 2399, Train Loss: 0.12398650497198105, Test Loss: 0.17070871591567993\n",
      "Epoch 2400, Train Loss: 0.12372102588415146, Test Loss: 0.17039744555950165\n",
      "Epoch 2401, Train Loss: 0.12345557659864426, Test Loss: 0.17012427747249603\n",
      "Epoch 2402, Train Loss: 0.12319101393222809, Test Loss: 0.16984255611896515\n",
      "Epoch 2403, Train Loss: 0.12292718142271042, Test Loss: 0.16953477263450623\n",
      "Epoch 2404, Train Loss: 0.12266560643911362, Test Loss: 0.1692456752061844\n",
      "Epoch 2405, Train Loss: 0.12240227311849594, Test Loss: 0.16898822784423828\n",
      "Epoch 2406, Train Loss: 0.12214139103889465, Test Loss: 0.16870184242725372\n",
      "Epoch 2407, Train Loss: 0.12188085913658142, Test Loss: 0.1684078425168991\n",
      "Epoch 2408, Train Loss: 0.12162124365568161, Test Loss: 0.16814109683036804\n",
      "Epoch 2409, Train Loss: 0.12136272341012955, Test Loss: 0.16784627735614777\n",
      "Epoch 2410, Train Loss: 0.12110396474599838, Test Loss: 0.16756875813007355\n",
      "Epoch 2411, Train Loss: 0.12084643542766571, Test Loss: 0.16730226576328278\n",
      "Epoch 2412, Train Loss: 0.12058989703655243, Test Loss: 0.16702966392040253\n",
      "Epoch 2413, Train Loss: 0.12033429741859436, Test Loss: 0.16673843562602997\n",
      "Epoch 2414, Train Loss: 0.12007932364940643, Test Loss: 0.16646094620227814\n",
      "Epoch 2415, Train Loss: 0.11982599645853043, Test Loss: 0.16617277264595032\n",
      "Epoch 2416, Train Loss: 0.11957278847694397, Test Loss: 0.1659022867679596\n",
      "Epoch 2417, Train Loss: 0.11931918561458588, Test Loss: 0.1656428873538971\n",
      "Epoch 2418, Train Loss: 0.11906768381595612, Test Loss: 0.1653594672679901\n",
      "Epoch 2419, Train Loss: 0.11881668865680695, Test Loss: 0.1650885045528412\n",
      "Epoch 2420, Train Loss: 0.11856573820114136, Test Loss: 0.16483402252197266\n",
      "Epoch 2421, Train Loss: 0.11831601709127426, Test Loss: 0.16454213857650757\n",
      "Epoch 2422, Train Loss: 0.11806672066450119, Test Loss: 0.16427327692508698\n",
      "Epoch 2423, Train Loss: 0.11782018095254898, Test Loss: 0.16399915516376495\n",
      "Epoch 2424, Train Loss: 0.11757085472345352, Test Loss: 0.16372841596603394\n",
      "Epoch 2425, Train Loss: 0.11732442677021027, Test Loss: 0.16344837844371796\n",
      "Epoch 2426, Train Loss: 0.11707790940999985, Test Loss: 0.1631818562746048\n",
      "Epoch 2427, Train Loss: 0.11683221906423569, Test Loss: 0.1629418283700943\n",
      "Epoch 2428, Train Loss: 0.11658763140439987, Test Loss: 0.16265183687210083\n",
      "Epoch 2429, Train Loss: 0.1163439154624939, Test Loss: 0.16238152980804443\n",
      "Epoch 2430, Train Loss: 0.11610159277915955, Test Loss: 0.16211628913879395\n",
      "Epoch 2431, Train Loss: 0.11585918068885803, Test Loss: 0.16184522211551666\n",
      "Epoch 2432, Train Loss: 0.11561655253171921, Test Loss: 0.16159382462501526\n",
      "Epoch 2433, Train Loss: 0.1153763011097908, Test Loss: 0.16136780381202698\n",
      "Epoch 2434, Train Loss: 0.11513523012399673, Test Loss: 0.16110123693943024\n",
      "Epoch 2435, Train Loss: 0.114895761013031, Test Loss: 0.1608515828847885\n",
      "Epoch 2436, Train Loss: 0.1146579384803772, Test Loss: 0.16057682037353516\n",
      "Epoch 2437, Train Loss: 0.11441963911056519, Test Loss: 0.16032308340072632\n",
      "Epoch 2438, Train Loss: 0.11418277770280838, Test Loss: 0.16004891693592072\n",
      "Epoch 2439, Train Loss: 0.11394628882408142, Test Loss: 0.15979743003845215\n",
      "Epoch 2440, Train Loss: 0.11371112614870071, Test Loss: 0.159537672996521\n",
      "Epoch 2441, Train Loss: 0.11347784101963043, Test Loss: 0.15932585299015045\n",
      "Epoch 2442, Train Loss: 0.11324355006217957, Test Loss: 0.15907208621501923\n",
      "Epoch 2443, Train Loss: 0.11301056295633316, Test Loss: 0.15882828831672668\n",
      "Epoch 2444, Train Loss: 0.11277704685926437, Test Loss: 0.15856540203094482\n",
      "Epoch 2445, Train Loss: 0.1125451996922493, Test Loss: 0.15828940272331238\n",
      "Epoch 2446, Train Loss: 0.11231471598148346, Test Loss: 0.15804918110370636\n",
      "Epoch 2447, Train Loss: 0.11208589375019073, Test Loss: 0.15780729055404663\n",
      "Epoch 2448, Train Loss: 0.1118541955947876, Test Loss: 0.15754510462284088\n",
      "Epoch 2449, Train Loss: 0.11162489652633667, Test Loss: 0.1572989821434021\n",
      "Epoch 2450, Train Loss: 0.1113966703414917, Test Loss: 0.15703833103179932\n",
      "Epoch 2451, Train Loss: 0.11116843670606613, Test Loss: 0.156804159283638\n",
      "Epoch 2452, Train Loss: 0.11094105243682861, Test Loss: 0.15652182698249817\n",
      "Epoch 2453, Train Loss: 0.1107148826122284, Test Loss: 0.15626926720142365\n",
      "Epoch 2454, Train Loss: 0.11048895865678787, Test Loss: 0.15603090822696686\n",
      "Epoch 2455, Train Loss: 0.11026407033205032, Test Loss: 0.15578684210777283\n",
      "Epoch 2456, Train Loss: 0.11003971844911575, Test Loss: 0.15553058683872223\n",
      "Epoch 2457, Train Loss: 0.10981563478708267, Test Loss: 0.15528374910354614\n",
      "Epoch 2458, Train Loss: 0.10959284007549286, Test Loss: 0.15505284070968628\n",
      "Epoch 2459, Train Loss: 0.1093706265091896, Test Loss: 0.1548021286725998\n",
      "Epoch 2460, Train Loss: 0.10914965718984604, Test Loss: 0.1545591503381729\n",
      "Epoch 2461, Train Loss: 0.10892932862043381, Test Loss: 0.15429887175559998\n",
      "Epoch 2462, Train Loss: 0.10870829224586487, Test Loss: 0.15408669412136078\n",
      "Epoch 2463, Train Loss: 0.10848856717348099, Test Loss: 0.1538354903459549\n",
      "Epoch 2464, Train Loss: 0.10826980322599411, Test Loss: 0.15359918773174286\n",
      "Epoch 2465, Train Loss: 0.10805194824934006, Test Loss: 0.15338197350502014\n",
      "Epoch 2466, Train Loss: 0.10783518850803375, Test Loss: 0.15314346551895142\n",
      "Epoch 2467, Train Loss: 0.10761844366788864, Test Loss: 0.1529100090265274\n",
      "Epoch 2468, Train Loss: 0.10740199685096741, Test Loss: 0.15266554057598114\n",
      "Epoch 2469, Train Loss: 0.10718733072280884, Test Loss: 0.15240684151649475\n",
      "Epoch 2470, Train Loss: 0.1069728434085846, Test Loss: 0.15215592086315155\n",
      "Epoch 2471, Train Loss: 0.10675892233848572, Test Loss: 0.15194135904312134\n",
      "Epoch 2472, Train Loss: 0.10654570162296295, Test Loss: 0.15171155333518982\n",
      "Epoch 2473, Train Loss: 0.1063331812620163, Test Loss: 0.15149320662021637\n",
      "Epoch 2474, Train Loss: 0.10612191259860992, Test Loss: 0.15126444399356842\n",
      "Epoch 2475, Train Loss: 0.10591002553701401, Test Loss: 0.1510295271873474\n",
      "Epoch 2476, Train Loss: 0.10569945722818375, Test Loss: 0.15078623592853546\n",
      "Epoch 2477, Train Loss: 0.10548989474773407, Test Loss: 0.15056422352790833\n",
      "Epoch 2478, Train Loss: 0.10528061538934708, Test Loss: 0.15031172335147858\n",
      "Epoch 2479, Train Loss: 0.10507228225469589, Test Loss: 0.15007832646369934\n",
      "Epoch 2480, Train Loss: 0.10486433655023575, Test Loss: 0.14985248446464539\n",
      "Epoch 2481, Train Loss: 0.10465704649686813, Test Loss: 0.1496306210756302\n",
      "Epoch 2482, Train Loss: 0.10445058345794678, Test Loss: 0.14940135180950165\n",
      "Epoch 2483, Train Loss: 0.10424476861953735, Test Loss: 0.14918319880962372\n",
      "Epoch 2484, Train Loss: 0.10403985530138016, Test Loss: 0.14896470308303833\n",
      "Epoch 2485, Train Loss: 0.10383563488721848, Test Loss: 0.14873820543289185\n",
      "Epoch 2486, Train Loss: 0.10363262891769409, Test Loss: 0.14852014183998108\n",
      "Epoch 2487, Train Loss: 0.1034298911690712, Test Loss: 0.14830197393894196\n",
      "Epoch 2488, Train Loss: 0.10322719812393188, Test Loss: 0.14807739853858948\n",
      "Epoch 2489, Train Loss: 0.103024922311306, Test Loss: 0.14784552156925201\n",
      "Epoch 2490, Train Loss: 0.10282411426305771, Test Loss: 0.1476176232099533\n",
      "Epoch 2491, Train Loss: 0.10262395441532135, Test Loss: 0.14739499986171722\n",
      "Epoch 2492, Train Loss: 0.10242421180009842, Test Loss: 0.14717473089694977\n",
      "Epoch 2493, Train Loss: 0.10222511738538742, Test Loss: 0.146959125995636\n",
      "Epoch 2494, Train Loss: 0.10202686488628387, Test Loss: 0.14676420390605927\n",
      "Epoch 2495, Train Loss: 0.10182900726795197, Test Loss: 0.14653995633125305\n",
      "Epoch 2496, Train Loss: 0.10163309425115585, Test Loss: 0.14633247256278992\n",
      "Epoch 2497, Train Loss: 0.10143575072288513, Test Loss: 0.14610394835472107\n",
      "Epoch 2498, Train Loss: 0.10124064981937408, Test Loss: 0.14590142667293549\n",
      "Epoch 2499, Train Loss: 0.10104518383741379, Test Loss: 0.14567957818508148\n",
      "Epoch 2500, Train Loss: 0.10085076093673706, Test Loss: 0.14546272158622742\n",
      "Epoch 2501, Train Loss: 0.1006569117307663, Test Loss: 0.14524133503437042\n",
      "Epoch 2502, Train Loss: 0.10046469420194626, Test Loss: 0.1450343132019043\n",
      "Epoch 2503, Train Loss: 0.10027144849300385, Test Loss: 0.14482572674751282\n",
      "Epoch 2504, Train Loss: 0.10007931292057037, Test Loss: 0.14461319148540497\n",
      "Epoch 2505, Train Loss: 0.09988805651664734, Test Loss: 0.14440813660621643\n",
      "Epoch 2506, Train Loss: 0.0996973067522049, Test Loss: 0.14420339465141296\n",
      "Epoch 2507, Train Loss: 0.0995074063539505, Test Loss: 0.1440003663301468\n",
      "Epoch 2508, Train Loss: 0.09931784868240356, Test Loss: 0.1438024938106537\n",
      "Epoch 2509, Train Loss: 0.0991290956735611, Test Loss: 0.14362038671970367\n",
      "Epoch 2510, Train Loss: 0.09894081950187683, Test Loss: 0.14339995384216309\n",
      "Epoch 2511, Train Loss: 0.09875292330980301, Test Loss: 0.14317642152309418\n",
      "Epoch 2512, Train Loss: 0.0985657125711441, Test Loss: 0.1429705172777176\n",
      "Epoch 2513, Train Loss: 0.09837918728590012, Test Loss: 0.1427706927061081\n",
      "Epoch 2514, Train Loss: 0.09819403290748596, Test Loss: 0.14258405566215515\n",
      "Epoch 2515, Train Loss: 0.09800850600004196, Test Loss: 0.14237934350967407\n",
      "Epoch 2516, Train Loss: 0.09782376140356064, Test Loss: 0.14216479659080505\n",
      "Epoch 2517, Train Loss: 0.09763986617326736, Test Loss: 0.141950324177742\n",
      "Epoch 2518, Train Loss: 0.09745647758245468, Test Loss: 0.1417493224143982\n",
      "Epoch 2519, Train Loss: 0.09727403521537781, Test Loss: 0.14152204990386963\n",
      "Epoch 2520, Train Loss: 0.09709255397319794, Test Loss: 0.14131495356559753\n",
      "Epoch 2521, Train Loss: 0.09691048413515091, Test Loss: 0.14113137125968933\n",
      "Epoch 2522, Train Loss: 0.09673043340444565, Test Loss: 0.14091341197490692\n",
      "Epoch 2523, Train Loss: 0.09654948115348816, Test Loss: 0.14072130620479584\n",
      "Epoch 2524, Train Loss: 0.09636937826871872, Test Loss: 0.14054034650325775\n",
      "Epoch 2525, Train Loss: 0.09619038552045822, Test Loss: 0.14036811888217926\n",
      "Epoch 2526, Train Loss: 0.09601160883903503, Test Loss: 0.1401614099740982\n",
      "Epoch 2527, Train Loss: 0.09583375602960587, Test Loss: 0.13996359705924988\n",
      "Epoch 2528, Train Loss: 0.09565634280443192, Test Loss: 0.13977420330047607\n",
      "Epoch 2529, Train Loss: 0.09547969698905945, Test Loss: 0.13957886397838593\n",
      "Epoch 2530, Train Loss: 0.09530359506607056, Test Loss: 0.1393687129020691\n",
      "Epoch 2531, Train Loss: 0.095127634704113, Test Loss: 0.13917966187000275\n",
      "Epoch 2532, Train Loss: 0.09495261311531067, Test Loss: 0.1389857530593872\n",
      "Epoch 2533, Train Loss: 0.0947783961892128, Test Loss: 0.13880610466003418\n",
      "Epoch 2534, Train Loss: 0.09460432082414627, Test Loss: 0.13860328495502472\n",
      "Epoch 2535, Train Loss: 0.09443052113056183, Test Loss: 0.1384231001138687\n",
      "Epoch 2536, Train Loss: 0.09425806254148483, Test Loss: 0.13824211061000824\n",
      "Epoch 2537, Train Loss: 0.09408582001924515, Test Loss: 0.13805218040943146\n",
      "Epoch 2538, Train Loss: 0.09391476958990097, Test Loss: 0.13785260915756226\n",
      "Epoch 2539, Train Loss: 0.09374433755874634, Test Loss: 0.13767224550247192\n",
      "Epoch 2540, Train Loss: 0.09357407689094543, Test Loss: 0.13748037815093994\n",
      "Epoch 2541, Train Loss: 0.09340409934520721, Test Loss: 0.1373089700937271\n",
      "Epoch 2542, Train Loss: 0.09323621541261673, Test Loss: 0.13713449239730835\n",
      "Epoch 2543, Train Loss: 0.09306617826223373, Test Loss: 0.1369333416223526\n",
      "Epoch 2544, Train Loss: 0.09289849549531937, Test Loss: 0.1367436647415161\n",
      "Epoch 2545, Train Loss: 0.09273108094930649, Test Loss: 0.13656583428382874\n",
      "Epoch 2546, Train Loss: 0.09256383031606674, Test Loss: 0.13637177646160126\n",
      "Epoch 2547, Train Loss: 0.0923975259065628, Test Loss: 0.13618972897529602\n",
      "Epoch 2548, Train Loss: 0.0922316312789917, Test Loss: 0.13601529598236084\n",
      "Epoch 2549, Train Loss: 0.09206704795360565, Test Loss: 0.13584937155246735\n",
      "Epoch 2550, Train Loss: 0.09190249443054199, Test Loss: 0.135655015707016\n",
      "Epoch 2551, Train Loss: 0.09173828363418579, Test Loss: 0.1354714184999466\n",
      "Epoch 2552, Train Loss: 0.09157471358776093, Test Loss: 0.13529451191425323\n",
      "Epoch 2553, Train Loss: 0.09141173213720322, Test Loss: 0.13512256741523743\n",
      "Epoch 2554, Train Loss: 0.09125029295682907, Test Loss: 0.1349434107542038\n",
      "Epoch 2555, Train Loss: 0.09108847379684448, Test Loss: 0.1347668617963791\n",
      "Epoch 2556, Train Loss: 0.09092653542757034, Test Loss: 0.13459669053554535\n",
      "Epoch 2557, Train Loss: 0.09076603502035141, Test Loss: 0.13441474735736847\n",
      "Epoch 2558, Train Loss: 0.09060598909854889, Test Loss: 0.13425281643867493\n",
      "Epoch 2559, Train Loss: 0.09044647961854935, Test Loss: 0.13405151665210724\n",
      "Epoch 2560, Train Loss: 0.09028764814138412, Test Loss: 0.13386917114257812\n",
      "Epoch 2561, Train Loss: 0.09012959897518158, Test Loss: 0.1336982548236847\n",
      "Epoch 2562, Train Loss: 0.0899711400270462, Test Loss: 0.13352617621421814\n",
      "Epoch 2563, Train Loss: 0.08981404453516006, Test Loss: 0.133366659283638\n",
      "Epoch 2564, Train Loss: 0.08965730667114258, Test Loss: 0.13321207463741302\n",
      "Epoch 2565, Train Loss: 0.08950139582157135, Test Loss: 0.13303034007549286\n",
      "Epoch 2566, Train Loss: 0.0893455371260643, Test Loss: 0.1328587383031845\n",
      "Epoch 2567, Train Loss: 0.08919023722410202, Test Loss: 0.1326897144317627\n",
      "Epoch 2568, Train Loss: 0.08903557807207108, Test Loss: 0.13251081109046936\n",
      "Epoch 2569, Train Loss: 0.08888152241706848, Test Loss: 0.13233759999275208\n",
      "Epoch 2570, Train Loss: 0.08872778713703156, Test Loss: 0.13215763866901398\n",
      "Epoch 2571, Train Loss: 0.08857524394989014, Test Loss: 0.13198041915893555\n",
      "Epoch 2572, Train Loss: 0.08842262625694275, Test Loss: 0.131828173995018\n",
      "Epoch 2573, Train Loss: 0.08827203512191772, Test Loss: 0.13166217505931854\n",
      "Epoch 2574, Train Loss: 0.08811915665864944, Test Loss: 0.13149365782737732\n",
      "Epoch 2575, Train Loss: 0.0879688635468483, Test Loss: 0.13133323192596436\n",
      "Epoch 2576, Train Loss: 0.08781818300485611, Test Loss: 0.13115328550338745\n",
      "Epoch 2577, Train Loss: 0.08766929060220718, Test Loss: 0.1309994012117386\n",
      "Epoch 2578, Train Loss: 0.08751928061246872, Test Loss: 0.1308397501707077\n",
      "Epoch 2579, Train Loss: 0.08737081289291382, Test Loss: 0.13066577911376953\n",
      "Epoch 2580, Train Loss: 0.08722299337387085, Test Loss: 0.1305113285779953\n",
      "Epoch 2581, Train Loss: 0.08707630634307861, Test Loss: 0.13034328818321228\n",
      "Epoch 2582, Train Loss: 0.08692784607410431, Test Loss: 0.13018208742141724\n",
      "Epoch 2583, Train Loss: 0.08678135275840759, Test Loss: 0.13001015782356262\n",
      "Epoch 2584, Train Loss: 0.08663500100374222, Test Loss: 0.12985529005527496\n",
      "Epoch 2585, Train Loss: 0.08648944646120071, Test Loss: 0.1296885758638382\n",
      "Epoch 2586, Train Loss: 0.08634450286626816, Test Loss: 0.12953296303749084\n",
      "Epoch 2587, Train Loss: 0.0862000584602356, Test Loss: 0.12938576936721802\n",
      "Epoch 2588, Train Loss: 0.08605621010065079, Test Loss: 0.1292269378900528\n",
      "Epoch 2589, Train Loss: 0.08591298013925552, Test Loss: 0.1290532350540161\n",
      "Epoch 2590, Train Loss: 0.0857696533203125, Test Loss: 0.12889860570430756\n",
      "Epoch 2591, Train Loss: 0.08562728762626648, Test Loss: 0.12874197959899902\n",
      "Epoch 2592, Train Loss: 0.08548561483621597, Test Loss: 0.1285681575536728\n",
      "Epoch 2593, Train Loss: 0.08534422516822815, Test Loss: 0.12841758131980896\n",
      "Epoch 2594, Train Loss: 0.08520328253507614, Test Loss: 0.12824171781539917\n",
      "Epoch 2595, Train Loss: 0.08506276458501816, Test Loss: 0.12810711562633514\n",
      "Epoch 2596, Train Loss: 0.08492320775985718, Test Loss: 0.12794366478919983\n",
      "Epoch 2597, Train Loss: 0.0847838819026947, Test Loss: 0.12779873609542847\n",
      "Epoch 2598, Train Loss: 0.08464504033327103, Test Loss: 0.12762150168418884\n",
      "Epoch 2599, Train Loss: 0.084506556391716, Test Loss: 0.12746024131774902\n",
      "Epoch 2600, Train Loss: 0.08436908572912216, Test Loss: 0.1273200958967209\n",
      "Epoch 2601, Train Loss: 0.08423087000846863, Test Loss: 0.12718306481838226\n",
      "Epoch 2602, Train Loss: 0.08409498631954193, Test Loss: 0.1270441859960556\n",
      "Epoch 2603, Train Loss: 0.0839574858546257, Test Loss: 0.12690094113349915\n",
      "Epoch 2604, Train Loss: 0.0838216170668602, Test Loss: 0.1267494410276413\n",
      "Epoch 2605, Train Loss: 0.0836857333779335, Test Loss: 0.12659230828285217\n",
      "Epoch 2606, Train Loss: 0.08355072140693665, Test Loss: 0.12644881010055542\n",
      "Epoch 2607, Train Loss: 0.08341627568006516, Test Loss: 0.12628330290317535\n",
      "Epoch 2608, Train Loss: 0.08328218758106232, Test Loss: 0.12613484263420105\n",
      "Epoch 2609, Train Loss: 0.08314938843250275, Test Loss: 0.12597031891345978\n",
      "Epoch 2610, Train Loss: 0.08301671594381332, Test Loss: 0.12582740187644958\n",
      "Epoch 2611, Train Loss: 0.08288297057151794, Test Loss: 0.12567000091075897\n",
      "Epoch 2612, Train Loss: 0.08275085687637329, Test Loss: 0.12553788721561432\n",
      "Epoch 2613, Train Loss: 0.08261856436729431, Test Loss: 0.12539790570735931\n",
      "Epoch 2614, Train Loss: 0.0824868381023407, Test Loss: 0.12525878846645355\n",
      "Epoch 2615, Train Loss: 0.08235607296228409, Test Loss: 0.12510617077350616\n",
      "Epoch 2616, Train Loss: 0.08222568035125732, Test Loss: 0.12496377527713776\n",
      "Epoch 2617, Train Loss: 0.08209565281867981, Test Loss: 0.12482307851314545\n",
      "Epoch 2618, Train Loss: 0.08196597546339035, Test Loss: 0.12466330826282501\n",
      "Epoch 2619, Train Loss: 0.08183671534061432, Test Loss: 0.12451174110174179\n",
      "Epoch 2620, Train Loss: 0.08170747011899948, Test Loss: 0.12437150627374649\n",
      "Epoch 2621, Train Loss: 0.08157941699028015, Test Loss: 0.12422362715005875\n",
      "Epoch 2622, Train Loss: 0.08145228028297424, Test Loss: 0.12407733500003815\n",
      "Epoch 2623, Train Loss: 0.08132397383451462, Test Loss: 0.12394529581069946\n",
      "Epoch 2624, Train Loss: 0.08119714260101318, Test Loss: 0.12380953133106232\n",
      "Epoch 2625, Train Loss: 0.08107257634401321, Test Loss: 0.12365186214447021\n",
      "Epoch 2626, Train Loss: 0.0809476226568222, Test Loss: 0.12351623922586441\n",
      "Epoch 2627, Train Loss: 0.08081909269094467, Test Loss: 0.12337804585695267\n",
      "Epoch 2628, Train Loss: 0.08069384843111038, Test Loss: 0.12324695289134979\n",
      "Epoch 2629, Train Loss: 0.08056944608688354, Test Loss: 0.12311945855617523\n",
      "Epoch 2630, Train Loss: 0.08044444769620895, Test Loss: 0.12299537658691406\n",
      "Epoch 2631, Train Loss: 0.08032098412513733, Test Loss: 0.1228426992893219\n",
      "Epoch 2632, Train Loss: 0.08019734919071198, Test Loss: 0.12271001189947128\n",
      "Epoch 2633, Train Loss: 0.08007419109344482, Test Loss: 0.12255758047103882\n",
      "Epoch 2634, Train Loss: 0.07995105534791946, Test Loss: 0.12243366241455078\n",
      "Epoch 2635, Train Loss: 0.07982906699180603, Test Loss: 0.12230182439088821\n",
      "Epoch 2636, Train Loss: 0.07970765978097916, Test Loss: 0.12217652797698975\n",
      "Epoch 2637, Train Loss: 0.07958623021841049, Test Loss: 0.12204970419406891\n",
      "Epoch 2638, Train Loss: 0.07946516573429108, Test Loss: 0.12191431224346161\n",
      "Epoch 2639, Train Loss: 0.07934452593326569, Test Loss: 0.12176506966352463\n",
      "Epoch 2640, Train Loss: 0.07922559231519699, Test Loss: 0.12161139398813248\n",
      "Epoch 2641, Train Loss: 0.07910517603158951, Test Loss: 0.12149038165807724\n",
      "Epoch 2642, Train Loss: 0.07898600399494171, Test Loss: 0.121352918446064\n",
      "Epoch 2643, Train Loss: 0.07886721938848495, Test Loss: 0.12121769785881042\n",
      "Epoch 2644, Train Loss: 0.07874953746795654, Test Loss: 0.12107916921377182\n",
      "Epoch 2645, Train Loss: 0.07863139361143112, Test Loss: 0.1209520623087883\n",
      "Epoch 2646, Train Loss: 0.07851353287696838, Test Loss: 0.12082237750291824\n",
      "Epoch 2647, Train Loss: 0.0783965066075325, Test Loss: 0.12070205062627792\n",
      "Epoch 2648, Train Loss: 0.07828005403280258, Test Loss: 0.12056419253349304\n",
      "Epoch 2649, Train Loss: 0.07816365361213684, Test Loss: 0.12043855339288712\n",
      "Epoch 2650, Train Loss: 0.07804837077856064, Test Loss: 0.12031244486570358\n",
      "Epoch 2651, Train Loss: 0.07793261855840683, Test Loss: 0.1201765239238739\n",
      "Epoch 2652, Train Loss: 0.07781753689050674, Test Loss: 0.12004319578409195\n",
      "Epoch 2653, Train Loss: 0.07770322263240814, Test Loss: 0.11992549896240234\n",
      "Epoch 2654, Train Loss: 0.07758969813585281, Test Loss: 0.11980370432138443\n",
      "Epoch 2655, Train Loss: 0.07747533172369003, Test Loss: 0.11968386173248291\n",
      "Epoch 2656, Train Loss: 0.0773618221282959, Test Loss: 0.11954771727323532\n",
      "Epoch 2657, Train Loss: 0.07724875211715698, Test Loss: 0.11942661553621292\n",
      "Epoch 2658, Train Loss: 0.07713630795478821, Test Loss: 0.11928828805685043\n",
      "Epoch 2659, Train Loss: 0.07702390104532242, Test Loss: 0.11917084455490112\n",
      "Epoch 2660, Train Loss: 0.07691243290901184, Test Loss: 0.11902869492769241\n",
      "Epoch 2661, Train Loss: 0.07680078595876694, Test Loss: 0.11892831325531006\n",
      "Epoch 2662, Train Loss: 0.07668977230787277, Test Loss: 0.11880996078252792\n",
      "Epoch 2663, Train Loss: 0.07657990604639053, Test Loss: 0.11869417130947113\n",
      "Epoch 2664, Train Loss: 0.07646912336349487, Test Loss: 0.11857684701681137\n",
      "Epoch 2665, Train Loss: 0.07635927200317383, Test Loss: 0.11844943463802338\n",
      "Epoch 2666, Train Loss: 0.07624948024749756, Test Loss: 0.11833017319440842\n",
      "Epoch 2667, Train Loss: 0.0761403888463974, Test Loss: 0.11821094155311584\n",
      "Epoch 2668, Train Loss: 0.07603184133768082, Test Loss: 0.11809622496366501\n",
      "Epoch 2669, Train Loss: 0.07592448592185974, Test Loss: 0.11799268424510956\n",
      "Epoch 2670, Train Loss: 0.07581547647714615, Test Loss: 0.11785927414894104\n",
      "Epoch 2671, Train Loss: 0.07570827752351761, Test Loss: 0.11773842573165894\n",
      "Epoch 2672, Train Loss: 0.0755997970700264, Test Loss: 0.11759776622056961\n",
      "Epoch 2673, Train Loss: 0.07549276202917099, Test Loss: 0.11747699230909348\n",
      "Epoch 2674, Train Loss: 0.07538628578186035, Test Loss: 0.1173483207821846\n",
      "Epoch 2675, Train Loss: 0.07528044283390045, Test Loss: 0.11722522228956223\n",
      "Epoch 2676, Train Loss: 0.07517439872026443, Test Loss: 0.11711099743843079\n",
      "Epoch 2677, Train Loss: 0.07506965845823288, Test Loss: 0.1170029416680336\n",
      "Epoch 2678, Train Loss: 0.07496347278356552, Test Loss: 0.1169014647603035\n",
      "Epoch 2679, Train Loss: 0.07485876977443695, Test Loss: 0.11678355932235718\n",
      "Epoch 2680, Train Loss: 0.07475448399782181, Test Loss: 0.11666819453239441\n",
      "Epoch 2681, Train Loss: 0.07464976608753204, Test Loss: 0.11653950810432434\n",
      "Epoch 2682, Train Loss: 0.07454633712768555, Test Loss: 0.11643471568822861\n",
      "Epoch 2683, Train Loss: 0.07444276660680771, Test Loss: 0.11632899194955826\n",
      "Epoch 2684, Train Loss: 0.07433941960334778, Test Loss: 0.11619607359170914\n",
      "Epoch 2685, Train Loss: 0.07423710078001022, Test Loss: 0.11607616394758224\n",
      "Epoch 2686, Train Loss: 0.07413474470376968, Test Loss: 0.11594779789447784\n",
      "Epoch 2687, Train Loss: 0.07403333485126495, Test Loss: 0.11582881957292557\n",
      "Epoch 2688, Train Loss: 0.07393144816160202, Test Loss: 0.11572367697954178\n",
      "Epoch 2689, Train Loss: 0.07382972538471222, Test Loss: 0.11561696231365204\n",
      "Epoch 2690, Train Loss: 0.07372858375310898, Test Loss: 0.11549782752990723\n",
      "Epoch 2691, Train Loss: 0.07362794131040573, Test Loss: 0.11538466066122055\n",
      "Epoch 2692, Train Loss: 0.0735277608036995, Test Loss: 0.11526621133089066\n",
      "Epoch 2693, Train Loss: 0.07342782616615295, Test Loss: 0.11516173928976059\n",
      "Epoch 2694, Train Loss: 0.07332862913608551, Test Loss: 0.11505986005067825\n",
      "Epoch 2695, Train Loss: 0.07322882860898972, Test Loss: 0.11494652181863785\n",
      "Epoch 2696, Train Loss: 0.07312975078821182, Test Loss: 0.11483209580183029\n",
      "Epoch 2697, Train Loss: 0.07303105294704437, Test Loss: 0.11473365873098373\n",
      "Epoch 2698, Train Loss: 0.07293294370174408, Test Loss: 0.11463014781475067\n",
      "Epoch 2699, Train Loss: 0.07283506542444229, Test Loss: 0.11451788991689682\n",
      "Epoch 2700, Train Loss: 0.07273757457733154, Test Loss: 0.11441300064325333\n",
      "Epoch 2701, Train Loss: 0.07264046370983124, Test Loss: 0.11430782079696655\n",
      "Epoch 2702, Train Loss: 0.07254374772310257, Test Loss: 0.11418849974870682\n",
      "Epoch 2703, Train Loss: 0.07244691997766495, Test Loss: 0.11409475654363632\n",
      "Epoch 2704, Train Loss: 0.07235082238912582, Test Loss: 0.11398101598024368\n",
      "Epoch 2705, Train Loss: 0.0722547173500061, Test Loss: 0.11386960744857788\n",
      "Epoch 2706, Train Loss: 0.07215923070907593, Test Loss: 0.11376839131116867\n",
      "Epoch 2707, Train Loss: 0.07206428050994873, Test Loss: 0.11367984116077423\n",
      "Epoch 2708, Train Loss: 0.07196953892707825, Test Loss: 0.113568976521492\n",
      "Epoch 2709, Train Loss: 0.07187503576278687, Test Loss: 0.1134655550122261\n",
      "Epoch 2710, Train Loss: 0.07178056985139847, Test Loss: 0.11335471272468567\n",
      "Epoch 2711, Train Loss: 0.07168713957071304, Test Loss: 0.11325404047966003\n",
      "Epoch 2712, Train Loss: 0.0715930387377739, Test Loss: 0.11314208805561066\n",
      "Epoch 2713, Train Loss: 0.07149990648031235, Test Loss: 0.11303842812776566\n",
      "Epoch 2714, Train Loss: 0.07140649855136871, Test Loss: 0.11293650418519974\n",
      "Epoch 2715, Train Loss: 0.07131379842758179, Test Loss: 0.11283788084983826\n",
      "Epoch 2716, Train Loss: 0.0712214857339859, Test Loss: 0.11272203177213669\n",
      "Epoch 2717, Train Loss: 0.07112976163625717, Test Loss: 0.11262233555316925\n",
      "Epoch 2718, Train Loss: 0.07103773206472397, Test Loss: 0.11253403127193451\n",
      "Epoch 2719, Train Loss: 0.07094623148441315, Test Loss: 0.11243198812007904\n",
      "Epoch 2720, Train Loss: 0.07085487991571426, Test Loss: 0.11233426630496979\n",
      "Epoch 2721, Train Loss: 0.07076463848352432, Test Loss: 0.11225220561027527\n",
      "Epoch 2722, Train Loss: 0.07067348062992096, Test Loss: 0.11214341968297958\n",
      "Epoch 2723, Train Loss: 0.07058323919773102, Test Loss: 0.11204952001571655\n",
      "Epoch 2724, Train Loss: 0.07049315422773361, Test Loss: 0.11193334311246872\n",
      "Epoch 2725, Train Loss: 0.07040353864431381, Test Loss: 0.11181827634572983\n",
      "Epoch 2726, Train Loss: 0.07031369209289551, Test Loss: 0.1117258071899414\n",
      "Epoch 2727, Train Loss: 0.07022438943386078, Test Loss: 0.11162661761045456\n",
      "Epoch 2728, Train Loss: 0.07013571262359619, Test Loss: 0.11151795089244843\n",
      "Epoch 2729, Train Loss: 0.0700472816824913, Test Loss: 0.1114175021648407\n",
      "Epoch 2730, Train Loss: 0.06995976716279984, Test Loss: 0.11133316159248352\n",
      "Epoch 2731, Train Loss: 0.06987105309963226, Test Loss: 0.11125655472278595\n",
      "Epoch 2732, Train Loss: 0.06978335231542587, Test Loss: 0.1111423671245575\n",
      "Epoch 2733, Train Loss: 0.0696958526968956, Test Loss: 0.11104961484670639\n",
      "Epoch 2734, Train Loss: 0.06960871815681458, Test Loss: 0.11095675826072693\n",
      "Epoch 2735, Train Loss: 0.06952203065156937, Test Loss: 0.11086376011371613\n",
      "Epoch 2736, Train Loss: 0.06943538784980774, Test Loss: 0.11077596247196198\n",
      "Epoch 2737, Train Loss: 0.06935002654790878, Test Loss: 0.11067981272935867\n",
      "Epoch 2738, Train Loss: 0.06926381587982178, Test Loss: 0.11058008670806885\n",
      "Epoch 2739, Train Loss: 0.06917846947908401, Test Loss: 0.11046569049358368\n",
      "Epoch 2740, Train Loss: 0.06909316033124924, Test Loss: 0.11037508398294449\n",
      "Epoch 2741, Train Loss: 0.0690082237124443, Test Loss: 0.11028866469860077\n",
      "Epoch 2742, Train Loss: 0.06892465054988861, Test Loss: 0.11019179224967957\n",
      "Epoch 2743, Train Loss: 0.06884009391069412, Test Loss: 0.11009369790554047\n",
      "Epoch 2744, Train Loss: 0.06875522434711456, Test Loss: 0.11000297963619232\n",
      "Epoch 2745, Train Loss: 0.06867195665836334, Test Loss: 0.10990497469902039\n",
      "Epoch 2746, Train Loss: 0.06858833134174347, Test Loss: 0.10981104522943497\n",
      "Epoch 2747, Train Loss: 0.06850456446409225, Test Loss: 0.10973894596099854\n",
      "Epoch 2748, Train Loss: 0.0684221163392067, Test Loss: 0.10963356494903564\n",
      "Epoch 2749, Train Loss: 0.06833899021148682, Test Loss: 0.10955049842596054\n",
      "Epoch 2750, Train Loss: 0.0682564452290535, Test Loss: 0.10947273671627045\n",
      "Epoch 2751, Train Loss: 0.06817439198493958, Test Loss: 0.10939168184995651\n",
      "Epoch 2752, Train Loss: 0.06809255480766296, Test Loss: 0.10929947346448898\n",
      "Epoch 2753, Train Loss: 0.0680110976099968, Test Loss: 0.10920998454093933\n",
      "Epoch 2754, Train Loss: 0.06793014705181122, Test Loss: 0.10913463681936264\n",
      "Epoch 2755, Train Loss: 0.06784900277853012, Test Loss: 0.10903605818748474\n",
      "Epoch 2756, Train Loss: 0.06776861846446991, Test Loss: 0.10895424336194992\n",
      "Epoch 2757, Train Loss: 0.0676879733800888, Test Loss: 0.1088615357875824\n",
      "Epoch 2758, Train Loss: 0.06760784238576889, Test Loss: 0.10877037793397903\n",
      "Epoch 2759, Train Loss: 0.06752815842628479, Test Loss: 0.10868772119283676\n",
      "Epoch 2760, Train Loss: 0.06744879484176636, Test Loss: 0.1085776835680008\n",
      "Epoch 2761, Train Loss: 0.06736890226602554, Test Loss: 0.1084989458322525\n",
      "Epoch 2762, Train Loss: 0.06729026138782501, Test Loss: 0.10841740667819977\n",
      "Epoch 2763, Train Loss: 0.06721095740795135, Test Loss: 0.1083301305770874\n",
      "Epoch 2764, Train Loss: 0.06713290512561798, Test Loss: 0.10824026167392731\n",
      "Epoch 2765, Train Loss: 0.06705425679683685, Test Loss: 0.10815385729074478\n",
      "Epoch 2766, Train Loss: 0.06697651743888855, Test Loss: 0.10805974155664444\n",
      "Epoch 2767, Train Loss: 0.06689853221178055, Test Loss: 0.10798875242471695\n",
      "Epoch 2768, Train Loss: 0.06682082265615463, Test Loss: 0.10791312158107758\n",
      "Epoch 2769, Train Loss: 0.06674407422542572, Test Loss: 0.10783824324607849\n",
      "Epoch 2770, Train Loss: 0.06666657328605652, Test Loss: 0.10774492472410202\n",
      "Epoch 2771, Train Loss: 0.06658980250358582, Test Loss: 0.10765581578016281\n",
      "Epoch 2772, Train Loss: 0.0665137767791748, Test Loss: 0.10757720470428467\n",
      "Epoch 2773, Train Loss: 0.06643746793270111, Test Loss: 0.10749431699514389\n",
      "Epoch 2774, Train Loss: 0.06636153906583786, Test Loss: 0.10741113871335983\n",
      "Epoch 2775, Train Loss: 0.06628651916980743, Test Loss: 0.10733909904956818\n",
      "Epoch 2776, Train Loss: 0.06621041893959045, Test Loss: 0.10724777728319168\n",
      "Epoch 2777, Train Loss: 0.06613524258136749, Test Loss: 0.10715078562498093\n",
      "Epoch 2778, Train Loss: 0.06606066226959229, Test Loss: 0.10708325356245041\n",
      "Epoch 2779, Train Loss: 0.06598687916994095, Test Loss: 0.10700079053640366\n",
      "Epoch 2780, Train Loss: 0.06591186672449112, Test Loss: 0.10690800845623016\n",
      "Epoch 2781, Train Loss: 0.06583826243877411, Test Loss: 0.10681505501270294\n",
      "Epoch 2782, Train Loss: 0.06576395034790039, Test Loss: 0.10676112025976181\n",
      "Epoch 2783, Train Loss: 0.06569024920463562, Test Loss: 0.10668928921222687\n",
      "Epoch 2784, Train Loss: 0.06561711430549622, Test Loss: 0.10659775137901306\n",
      "Epoch 2785, Train Loss: 0.06554414331912994, Test Loss: 0.10652140527963638\n",
      "Epoch 2786, Train Loss: 0.06547154486179352, Test Loss: 0.1064448133111\n",
      "Epoch 2787, Train Loss: 0.06539914011955261, Test Loss: 0.10637591779232025\n",
      "Epoch 2788, Train Loss: 0.0653267577290535, Test Loss: 0.10628465563058853\n",
      "Epoch 2789, Train Loss: 0.06525486707687378, Test Loss: 0.10621173679828644\n",
      "Epoch 2790, Train Loss: 0.0651828944683075, Test Loss: 0.10613157600164413\n",
      "Epoch 2791, Train Loss: 0.06511159241199493, Test Loss: 0.10604102164506912\n",
      "Epoch 2792, Train Loss: 0.06504007428884506, Test Loss: 0.10596922785043716\n",
      "Epoch 2793, Train Loss: 0.06496944278478622, Test Loss: 0.1058875173330307\n",
      "Epoch 2794, Train Loss: 0.0648985356092453, Test Loss: 0.10581646859645844\n",
      "Epoch 2795, Train Loss: 0.06482725590467453, Test Loss: 0.10575215518474579\n",
      "Epoch 2796, Train Loss: 0.06475695967674255, Test Loss: 0.10567392408847809\n",
      "Epoch 2797, Train Loss: 0.06468711793422699, Test Loss: 0.1055961474776268\n",
      "Epoch 2798, Train Loss: 0.06461745500564575, Test Loss: 0.10551220923662186\n",
      "Epoch 2799, Train Loss: 0.06454730778932571, Test Loss: 0.10544577986001968\n",
      "Epoch 2800, Train Loss: 0.06447789072990417, Test Loss: 0.10538770258426666\n",
      "Epoch 2801, Train Loss: 0.06440853327512741, Test Loss: 0.10531492531299591\n",
      "Epoch 2802, Train Loss: 0.06433961540460587, Test Loss: 0.10524063557386398\n",
      "Epoch 2803, Train Loss: 0.0642707422375679, Test Loss: 0.10515845566987991\n",
      "Epoch 2804, Train Loss: 0.06420335173606873, Test Loss: 0.10507480055093765\n",
      "Epoch 2805, Train Loss: 0.06413456052541733, Test Loss: 0.10499392449855804\n",
      "Epoch 2806, Train Loss: 0.06406620889902115, Test Loss: 0.1049104630947113\n",
      "Epoch 2807, Train Loss: 0.06399782747030258, Test Loss: 0.10484320670366287\n",
      "Epoch 2808, Train Loss: 0.06393059343099594, Test Loss: 0.10478217154741287\n",
      "Epoch 2809, Train Loss: 0.06386326998472214, Test Loss: 0.10470976680517197\n",
      "Epoch 2810, Train Loss: 0.06379548460245132, Test Loss: 0.10463303327560425\n",
      "Epoch 2811, Train Loss: 0.06372841447591782, Test Loss: 0.10456765443086624\n",
      "Epoch 2812, Train Loss: 0.06366180628538132, Test Loss: 0.10449351370334625\n",
      "Epoch 2813, Train Loss: 0.06359550356864929, Test Loss: 0.10441586375236511\n",
      "Epoch 2814, Train Loss: 0.06352879106998444, Test Loss: 0.10436064749956131\n",
      "Epoch 2815, Train Loss: 0.06346278637647629, Test Loss: 0.10428860038518906\n",
      "Epoch 2816, Train Loss: 0.06339672207832336, Test Loss: 0.10421087592840195\n",
      "Epoch 2817, Train Loss: 0.06333094090223312, Test Loss: 0.104146808385849\n",
      "Epoch 2818, Train Loss: 0.06326554715633392, Test Loss: 0.10408469289541245\n",
      "Epoch 2819, Train Loss: 0.0632002130150795, Test Loss: 0.1040055975317955\n",
      "Epoch 2820, Train Loss: 0.06313499808311462, Test Loss: 0.10393662750720978\n",
      "Epoch 2821, Train Loss: 0.06307024508714676, Test Loss: 0.10386843234300613\n",
      "Epoch 2822, Train Loss: 0.06300553679466248, Test Loss: 0.10380274057388306\n",
      "Epoch 2823, Train Loss: 0.06294114142656326, Test Loss: 0.10372088849544525\n",
      "Epoch 2824, Train Loss: 0.06287667900323868, Test Loss: 0.10365597903728485\n",
      "Epoch 2825, Train Loss: 0.06281286478042603, Test Loss: 0.10359697043895721\n",
      "Epoch 2826, Train Loss: 0.06274867802858353, Test Loss: 0.10351623594760895\n",
      "Epoch 2827, Train Loss: 0.0626850500702858, Test Loss: 0.10345103591680527\n",
      "Epoch 2828, Train Loss: 0.06262246519327164, Test Loss: 0.10337340086698532\n",
      "Epoch 2829, Train Loss: 0.0625585988163948, Test Loss: 0.10331448912620544\n",
      "Epoch 2830, Train Loss: 0.062495630234479904, Test Loss: 0.10323931276798248\n",
      "Epoch 2831, Train Loss: 0.062433794140815735, Test Loss: 0.10316230356693268\n",
      "Epoch 2832, Train Loss: 0.062370214611291885, Test Loss: 0.103109210729599\n",
      "Epoch 2833, Train Loss: 0.062307871878147125, Test Loss: 0.10303996503353119\n",
      "Epoch 2834, Train Loss: 0.06224562227725983, Test Loss: 0.1029621809720993\n",
      "Epoch 2835, Train Loss: 0.06218363344669342, Test Loss: 0.10290190577507019\n",
      "Epoch 2836, Train Loss: 0.06212186813354492, Test Loss: 0.1028391495347023\n",
      "Epoch 2837, Train Loss: 0.06206042692065239, Test Loss: 0.10278420895338058\n",
      "Epoch 2838, Train Loss: 0.06199925020337105, Test Loss: 0.10271796584129333\n",
      "Epoch 2839, Train Loss: 0.06193847954273224, Test Loss: 0.10265572369098663\n",
      "Epoch 2840, Train Loss: 0.06187754124403, Test Loss: 0.10258899629116058\n",
      "Epoch 2841, Train Loss: 0.06181592866778374, Test Loss: 0.10252247005701065\n",
      "Epoch 2842, Train Loss: 0.0617554672062397, Test Loss: 0.10245383530855179\n",
      "Epoch 2843, Train Loss: 0.06169511377811432, Test Loss: 0.10239552706480026\n",
      "Epoch 2844, Train Loss: 0.061635229736566544, Test Loss: 0.10232871025800705\n",
      "Epoch 2845, Train Loss: 0.061574969440698624, Test Loss: 0.1022692620754242\n",
      "Epoch 2846, Train Loss: 0.061514947563409805, Test Loss: 0.10221130400896072\n",
      "Epoch 2847, Train Loss: 0.06145596131682396, Test Loss: 0.10214542597532272\n",
      "Epoch 2848, Train Loss: 0.061396945267915726, Test Loss: 0.10208560526371002\n",
      "Epoch 2849, Train Loss: 0.061336833983659744, Test Loss: 0.10201171785593033\n",
      "Epoch 2850, Train Loss: 0.061278048902750015, Test Loss: 0.10195367783308029\n",
      "Epoch 2851, Train Loss: 0.0612201914191246, Test Loss: 0.10189209878444672\n",
      "Epoch 2852, Train Loss: 0.06116044148802757, Test Loss: 0.10184028744697571\n",
      "Epoch 2853, Train Loss: 0.06110253557562828, Test Loss: 0.10176936537027359\n",
      "Epoch 2854, Train Loss: 0.06104345992207527, Test Loss: 0.10171239823102951\n",
      "Epoch 2855, Train Loss: 0.06098552048206329, Test Loss: 0.10163547098636627\n",
      "Epoch 2856, Train Loss: 0.06092788279056549, Test Loss: 0.10158274322748184\n",
      "Epoch 2857, Train Loss: 0.06087002903223038, Test Loss: 0.10152334719896317\n",
      "Epoch 2858, Train Loss: 0.060812417417764664, Test Loss: 0.10145517438650131\n",
      "Epoch 2859, Train Loss: 0.060755230486392975, Test Loss: 0.10139532387256622\n",
      "Epoch 2860, Train Loss: 0.06069837510585785, Test Loss: 0.10133499652147293\n",
      "Epoch 2861, Train Loss: 0.06064198911190033, Test Loss: 0.10127809643745422\n",
      "Epoch 2862, Train Loss: 0.06058406084775925, Test Loss: 0.10121004283428192\n",
      "Epoch 2863, Train Loss: 0.0605274923145771, Test Loss: 0.10114677995443344\n",
      "Epoch 2864, Train Loss: 0.06047128885984421, Test Loss: 0.10108139365911484\n",
      "Epoch 2865, Train Loss: 0.060415200889110565, Test Loss: 0.101018525660038\n",
      "Epoch 2866, Train Loss: 0.060359369963407516, Test Loss: 0.10094978660345078\n",
      "Epoch 2867, Train Loss: 0.06030283868312836, Test Loss: 0.10090747475624084\n",
      "Epoch 2868, Train Loss: 0.06024763360619545, Test Loss: 0.10083712637424469\n",
      "Epoch 2869, Train Loss: 0.060192082077264786, Test Loss: 0.10077861696481705\n",
      "Epoch 2870, Train Loss: 0.06013647839426994, Test Loss: 0.10073297470808029\n",
      "Epoch 2871, Train Loss: 0.060081444680690765, Test Loss: 0.10067576915025711\n",
      "Epoch 2872, Train Loss: 0.06002620980143547, Test Loss: 0.10061881691217422\n",
      "Epoch 2873, Train Loss: 0.05997171625494957, Test Loss: 0.10055411607027054\n",
      "Epoch 2874, Train Loss: 0.05991673842072487, Test Loss: 0.10050758719444275\n",
      "Epoch 2875, Train Loss: 0.05986236408352852, Test Loss: 0.1004452109336853\n",
      "Epoch 2876, Train Loss: 0.059807952493429184, Test Loss: 0.1003890410065651\n",
      "Epoch 2877, Train Loss: 0.059754058718681335, Test Loss: 0.10032337158918381\n",
      "Epoch 2878, Train Loss: 0.05969979986548424, Test Loss: 0.10027128458023071\n",
      "Epoch 2879, Train Loss: 0.05964665487408638, Test Loss: 0.10021457821130753\n",
      "Epoch 2880, Train Loss: 0.059592485427856445, Test Loss: 0.1001603901386261\n",
      "Epoch 2881, Train Loss: 0.05953923240303993, Test Loss: 0.10010796785354614\n",
      "Epoch 2882, Train Loss: 0.05948560684919357, Test Loss: 0.10006172955036163\n",
      "Epoch 2883, Train Loss: 0.05943256616592407, Test Loss: 0.10000964254140854\n",
      "Epoch 2884, Train Loss: 0.05937986820936203, Test Loss: 0.09995778650045395\n",
      "Epoch 2885, Train Loss: 0.05932680144906044, Test Loss: 0.09989207983016968\n",
      "Epoch 2886, Train Loss: 0.059274543076753616, Test Loss: 0.09981738775968552\n",
      "Epoch 2887, Train Loss: 0.059221673756837845, Test Loss: 0.09975569695234299\n",
      "Epoch 2888, Train Loss: 0.05916881188750267, Test Loss: 0.09970656782388687\n",
      "Epoch 2889, Train Loss: 0.059117481112480164, Test Loss: 0.0996595248579979\n",
      "Epoch 2890, Train Loss: 0.05906439572572708, Test Loss: 0.09959983080625534\n",
      "Epoch 2891, Train Loss: 0.05901239812374115, Test Loss: 0.0995456725358963\n",
      "Epoch 2892, Train Loss: 0.058961622416973114, Test Loss: 0.09948744624853134\n",
      "Epoch 2893, Train Loss: 0.0589093379676342, Test Loss: 0.09944011270999908\n",
      "Epoch 2894, Train Loss: 0.05885881930589676, Test Loss: 0.0993897095322609\n",
      "Epoch 2895, Train Loss: 0.058806248009204865, Test Loss: 0.09934964776039124\n",
      "Epoch 2896, Train Loss: 0.05875546857714653, Test Loss: 0.09928886592388153\n",
      "Epoch 2897, Train Loss: 0.05870424956083298, Test Loss: 0.09924176335334778\n",
      "Epoch 2898, Train Loss: 0.05865320935845375, Test Loss: 0.09919045120477676\n",
      "Epoch 2899, Train Loss: 0.058602891862392426, Test Loss: 0.09914901852607727\n",
      "Epoch 2900, Train Loss: 0.05855175480246544, Test Loss: 0.09907390177249908\n",
      "Epoch 2901, Train Loss: 0.058501314371824265, Test Loss: 0.09901944547891617\n",
      "Epoch 2902, Train Loss: 0.058451149612665176, Test Loss: 0.09896552562713623\n",
      "Epoch 2903, Train Loss: 0.05840105563402176, Test Loss: 0.09891153126955032\n",
      "Epoch 2904, Train Loss: 0.05835110321640968, Test Loss: 0.09886440634727478\n",
      "Epoch 2905, Train Loss: 0.058301638811826706, Test Loss: 0.09879275411367416\n",
      "Epoch 2906, Train Loss: 0.058252111077308655, Test Loss: 0.0987386554479599\n",
      "Epoch 2907, Train Loss: 0.05820251628756523, Test Loss: 0.09868144989013672\n",
      "Epoch 2908, Train Loss: 0.05815254524350166, Test Loss: 0.09863698482513428\n",
      "Epoch 2909, Train Loss: 0.058103397488594055, Test Loss: 0.09858203679323196\n",
      "Epoch 2910, Train Loss: 0.05805479735136032, Test Loss: 0.09852337837219238\n",
      "Epoch 2911, Train Loss: 0.058005351573228836, Test Loss: 0.09848573803901672\n",
      "Epoch 2912, Train Loss: 0.05795639380812645, Test Loss: 0.09845218062400818\n",
      "Epoch 2913, Train Loss: 0.0579075962305069, Test Loss: 0.09839919954538345\n",
      "Epoch 2914, Train Loss: 0.05785961076617241, Test Loss: 0.09833518415689468\n",
      "Epoch 2915, Train Loss: 0.05781068280339241, Test Loss: 0.09830071032047272\n",
      "Epoch 2916, Train Loss: 0.057762302458286285, Test Loss: 0.09824705868959427\n",
      "Epoch 2917, Train Loss: 0.057714350521564484, Test Loss: 0.0982021689414978\n",
      "Epoch 2918, Train Loss: 0.057666659355163574, Test Loss: 0.0981343686580658\n",
      "Epoch 2919, Train Loss: 0.05761850252747536, Test Loss: 0.09808147698640823\n",
      "Epoch 2920, Train Loss: 0.0575706921517849, Test Loss: 0.09803719073534012\n",
      "Epoch 2921, Train Loss: 0.05752277746796608, Test Loss: 0.09800280630588531\n",
      "Epoch 2922, Train Loss: 0.05747554451227188, Test Loss: 0.09794273227453232\n",
      "Epoch 2923, Train Loss: 0.057428065687417984, Test Loss: 0.09790049493312836\n",
      "Epoch 2924, Train Loss: 0.05738110467791557, Test Loss: 0.09783940017223358\n",
      "Epoch 2925, Train Loss: 0.057333581149578094, Test Loss: 0.0978030189871788\n",
      "Epoch 2926, Train Loss: 0.05728674679994583, Test Loss: 0.09775276482105255\n",
      "Epoch 2927, Train Loss: 0.05724033713340759, Test Loss: 0.09770563244819641\n",
      "Epoch 2928, Train Loss: 0.05719403550028801, Test Loss: 0.09764894843101501\n",
      "Epoch 2929, Train Loss: 0.05714689940214157, Test Loss: 0.09761211276054382\n",
      "Epoch 2930, Train Loss: 0.05710035189986229, Test Loss: 0.09756527841091156\n",
      "Epoch 2931, Train Loss: 0.05705423280596733, Test Loss: 0.09751081466674805\n",
      "Epoch 2932, Train Loss: 0.05700909346342087, Test Loss: 0.09747504442930222\n",
      "Epoch 2933, Train Loss: 0.05696214735507965, Test Loss: 0.09741488844156265\n",
      "Epoch 2934, Train Loss: 0.056916188448667526, Test Loss: 0.09737476706504822\n",
      "Epoch 2935, Train Loss: 0.0568702258169651, Test Loss: 0.0973282903432846\n",
      "Epoch 2936, Train Loss: 0.05682467296719551, Test Loss: 0.09728085994720459\n",
      "Epoch 2937, Train Loss: 0.056779179722070694, Test Loss: 0.09723442047834396\n",
      "Epoch 2938, Train Loss: 0.056733839213848114, Test Loss: 0.0971958190202713\n",
      "Epoch 2939, Train Loss: 0.05668886750936508, Test Loss: 0.09714820235967636\n",
      "Epoch 2940, Train Loss: 0.056643739342689514, Test Loss: 0.09710351377725601\n",
      "Epoch 2941, Train Loss: 0.05659860745072365, Test Loss: 0.09705205261707306\n",
      "Epoch 2942, Train Loss: 0.05655382201075554, Test Loss: 0.0970090851187706\n",
      "Epoch 2943, Train Loss: 0.0565093457698822, Test Loss: 0.09696453809738159\n",
      "Epoch 2944, Train Loss: 0.05646473169326782, Test Loss: 0.0969182625412941\n",
      "Epoch 2945, Train Loss: 0.05642025172710419, Test Loss: 0.0968625396490097\n",
      "Epoch 2946, Train Loss: 0.05637602508068085, Test Loss: 0.09682057052850723\n",
      "Epoch 2947, Train Loss: 0.056332193315029144, Test Loss: 0.09677036106586456\n",
      "Epoch 2948, Train Loss: 0.05628763511776924, Test Loss: 0.09672589600086212\n",
      "Epoch 2949, Train Loss: 0.05624380335211754, Test Loss: 0.09668086469173431\n",
      "Epoch 2950, Train Loss: 0.05620026960968971, Test Loss: 0.09663097560405731\n",
      "Epoch 2951, Train Loss: 0.056156057864427567, Test Loss: 0.0965975821018219\n",
      "Epoch 2952, Train Loss: 0.05611272528767586, Test Loss: 0.09656289964914322\n",
      "Epoch 2953, Train Loss: 0.05606929957866669, Test Loss: 0.09650501608848572\n",
      "Epoch 2954, Train Loss: 0.05602588132023811, Test Loss: 0.09646353870630264\n",
      "Epoch 2955, Train Loss: 0.05598268657922745, Test Loss: 0.09642075002193451\n",
      "Epoch 2956, Train Loss: 0.05593985319137573, Test Loss: 0.09636391699314117\n",
      "Epoch 2957, Train Loss: 0.055896591395139694, Test Loss: 0.0963292121887207\n",
      "Epoch 2958, Train Loss: 0.055853746831417084, Test Loss: 0.0962940976023674\n",
      "Epoch 2959, Train Loss: 0.05581093207001686, Test Loss: 0.09625086933374405\n",
      "Epoch 2960, Train Loss: 0.05576847493648529, Test Loss: 0.0962052196264267\n",
      "Epoch 2961, Train Loss: 0.0557258203625679, Test Loss: 0.09616400301456451\n",
      "Epoch 2962, Train Loss: 0.05568372458219528, Test Loss: 0.09612064808607101\n",
      "Epoch 2963, Train Loss: 0.05564168095588684, Test Loss: 0.09607337415218353\n",
      "Epoch 2964, Train Loss: 0.05559968203306198, Test Loss: 0.09603229910135269\n",
      "Epoch 2965, Train Loss: 0.05555729195475578, Test Loss: 0.09598321467638016\n",
      "Epoch 2966, Train Loss: 0.05551551282405853, Test Loss: 0.09594348818063736\n",
      "Epoch 2967, Train Loss: 0.05547338351607323, Test Loss: 0.0959109216928482\n",
      "Epoch 2968, Train Loss: 0.055431924760341644, Test Loss: 0.09587166458368301\n",
      "Epoch 2969, Train Loss: 0.05539066717028618, Test Loss: 0.09582572430372238\n",
      "Epoch 2970, Train Loss: 0.05534916743636131, Test Loss: 0.09577109664678574\n",
      "Epoch 2971, Train Loss: 0.055307645350694656, Test Loss: 0.09573279321193695\n",
      "Epoch 2972, Train Loss: 0.05526629835367203, Test Loss: 0.09568925201892853\n",
      "Epoch 2973, Train Loss: 0.05522525683045387, Test Loss: 0.09564805775880814\n",
      "Epoch 2974, Train Loss: 0.055185019969940186, Test Loss: 0.09558573365211487\n",
      "Epoch 2975, Train Loss: 0.055143505334854126, Test Loss: 0.09556029736995697\n",
      "Epoch 2976, Train Loss: 0.05510278791189194, Test Loss: 0.09552055597305298\n",
      "Epoch 2977, Train Loss: 0.05506233870983124, Test Loss: 0.09546582400798798\n",
      "Epoch 2978, Train Loss: 0.055022094398736954, Test Loss: 0.09541863948106766\n",
      "Epoch 2979, Train Loss: 0.054981403052806854, Test Loss: 0.09539302438497543\n",
      "Epoch 2980, Train Loss: 0.054941415786743164, Test Loss: 0.09534381330013275\n",
      "Epoch 2981, Train Loss: 0.054900895804166794, Test Loss: 0.09531481564044952\n",
      "Epoch 2982, Train Loss: 0.05486130714416504, Test Loss: 0.09526992589235306\n",
      "Epoch 2983, Train Loss: 0.05482049658894539, Test Loss: 0.09524120390415192\n",
      "Epoch 2984, Train Loss: 0.0547809861600399, Test Loss: 0.09519070386886597\n",
      "Epoch 2985, Train Loss: 0.05474117398262024, Test Loss: 0.09515390545129776\n",
      "Epoch 2986, Train Loss: 0.054701268672943115, Test Loss: 0.0951242744922638\n",
      "Epoch 2987, Train Loss: 0.054661594331264496, Test Loss: 0.09507870674133301\n",
      "Epoch 2988, Train Loss: 0.054622311145067215, Test Loss: 0.0950469821691513\n",
      "Epoch 2989, Train Loss: 0.05458316206932068, Test Loss: 0.09499909728765488\n",
      "Epoch 2990, Train Loss: 0.054543636739254, Test Loss: 0.09497400373220444\n",
      "Epoch 2991, Train Loss: 0.054504893720149994, Test Loss: 0.09493454545736313\n",
      "Epoch 2992, Train Loss: 0.054465603083372116, Test Loss: 0.09489485621452332\n",
      "Epoch 2993, Train Loss: 0.05442660674452782, Test Loss: 0.09487059712409973\n",
      "Epoch 2994, Train Loss: 0.05438760668039322, Test Loss: 0.09483013302087784\n",
      "Epoch 2995, Train Loss: 0.05434894561767578, Test Loss: 0.09479150921106339\n",
      "Epoch 2996, Train Loss: 0.054310392588377, Test Loss: 0.09474893659353256\n",
      "Epoch 2997, Train Loss: 0.0542721226811409, Test Loss: 0.09469978511333466\n",
      "Epoch 2998, Train Loss: 0.05423402041196823, Test Loss: 0.09466561675071716\n",
      "Epoch 2999, Train Loss: 0.05419611558318138, Test Loss: 0.09461825340986252\n",
      "Epoch 3000, Train Loss: 0.05415725335478783, Test Loss: 0.09457799047231674\n",
      "Epoch 3001, Train Loss: 0.0541190505027771, Test Loss: 0.09453443437814713\n",
      "Epoch 3002, Train Loss: 0.05408108979463577, Test Loss: 0.09450006484985352\n",
      "Epoch 3003, Train Loss: 0.054043248295784, Test Loss: 0.0944674164056778\n",
      "Epoch 3004, Train Loss: 0.05400529131293297, Test Loss: 0.09442158788442612\n",
      "Epoch 3005, Train Loss: 0.05396752804517746, Test Loss: 0.09438271820545197\n",
      "Epoch 3006, Train Loss: 0.05392972752451897, Test Loss: 0.09435325860977173\n",
      "Epoch 3007, Train Loss: 0.05389248579740524, Test Loss: 0.0943019688129425\n",
      "Epoch 3008, Train Loss: 0.05385468155145645, Test Loss: 0.0942830890417099\n",
      "Epoch 3009, Train Loss: 0.05381755530834198, Test Loss: 0.09424200654029846\n",
      "Epoch 3010, Train Loss: 0.05378042906522751, Test Loss: 0.09420863538980484\n",
      "Epoch 3011, Train Loss: 0.053743284195661545, Test Loss: 0.09415942430496216\n",
      "Epoch 3012, Train Loss: 0.05370640009641647, Test Loss: 0.09412401169538498\n",
      "Epoch 3013, Train Loss: 0.05366950482130051, Test Loss: 0.09409445524215698\n",
      "Epoch 3014, Train Loss: 0.05363255739212036, Test Loss: 0.09405835717916489\n",
      "Epoch 3015, Train Loss: 0.05359598621726036, Test Loss: 0.09401556849479675\n",
      "Epoch 3016, Train Loss: 0.0535590834915638, Test Loss: 0.0939786434173584\n",
      "Epoch 3017, Train Loss: 0.05352264642715454, Test Loss: 0.09394259750843048\n",
      "Epoch 3018, Train Loss: 0.05348651856184006, Test Loss: 0.09391836822032928\n",
      "Epoch 3019, Train Loss: 0.05344988405704498, Test Loss: 0.09388277679681778\n",
      "Epoch 3020, Train Loss: 0.05341332405805588, Test Loss: 0.0938459038734436\n",
      "Epoch 3021, Train Loss: 0.05337700992822647, Test Loss: 0.093816839158535\n",
      "Epoch 3022, Train Loss: 0.053340863436460495, Test Loss: 0.09377924352884293\n",
      "Epoch 3023, Train Loss: 0.0533050037920475, Test Loss: 0.0937381163239479\n",
      "Epoch 3024, Train Loss: 0.05326930060982704, Test Loss: 0.09369908273220062\n",
      "Epoch 3025, Train Loss: 0.05323314666748047, Test Loss: 0.09367450326681137\n",
      "Epoch 3026, Train Loss: 0.05319749191403389, Test Loss: 0.09362153708934784\n",
      "Epoch 3027, Train Loss: 0.05316188186407089, Test Loss: 0.0936017632484436\n",
      "Epoch 3028, Train Loss: 0.053126491606235504, Test Loss: 0.09355389326810837\n",
      "Epoch 3029, Train Loss: 0.053090885281562805, Test Loss: 0.09352190047502518\n",
      "Epoch 3030, Train Loss: 0.053055599331855774, Test Loss: 0.09347369521856308\n",
      "Epoch 3031, Train Loss: 0.053020037710666656, Test Loss: 0.09344770759344101\n",
      "Epoch 3032, Train Loss: 0.05298474058508873, Test Loss: 0.09340883046388626\n",
      "Epoch 3033, Train Loss: 0.05294978991150856, Test Loss: 0.09336795657873154\n",
      "Epoch 3034, Train Loss: 0.05291442945599556, Test Loss: 0.09335031360387802\n",
      "Epoch 3035, Train Loss: 0.052879657596349716, Test Loss: 0.09330783039331436\n",
      "Epoch 3036, Train Loss: 0.05284483730792999, Test Loss: 0.0932801142334938\n",
      "Epoch 3037, Train Loss: 0.052809976041316986, Test Loss: 0.09324628114700317\n",
      "Epoch 3038, Train Loss: 0.05277532711625099, Test Loss: 0.09321132302284241\n",
      "Epoch 3039, Train Loss: 0.0527411587536335, Test Loss: 0.09317170828580856\n",
      "Epoch 3040, Train Loss: 0.052706822752952576, Test Loss: 0.0931316390633583\n",
      "Epoch 3041, Train Loss: 0.052671514451503754, Test Loss: 0.09309586137533188\n",
      "Epoch 3042, Train Loss: 0.05263729393482208, Test Loss: 0.09307266026735306\n",
      "Epoch 3043, Train Loss: 0.05260275676846504, Test Loss: 0.09305531531572342\n",
      "Epoch 3044, Train Loss: 0.05256859213113785, Test Loss: 0.09301037341356277\n",
      "Epoch 3045, Train Loss: 0.05253448709845543, Test Loss: 0.0929834395647049\n",
      "Epoch 3046, Train Loss: 0.05250032991170883, Test Loss: 0.09294436872005463\n",
      "Epoch 3047, Train Loss: 0.05246661603450775, Test Loss: 0.09290878474712372\n",
      "Epoch 3048, Train Loss: 0.05243254452943802, Test Loss: 0.09286347031593323\n",
      "Epoch 3049, Train Loss: 0.05239897221326828, Test Loss: 0.09283557534217834\n",
      "Epoch 3050, Train Loss: 0.052365053445100784, Test Loss: 0.09281094372272491\n",
      "Epoch 3051, Train Loss: 0.05233224853873253, Test Loss: 0.09277042001485825\n",
      "Epoch 3052, Train Loss: 0.05229844152927399, Test Loss: 0.09272665530443192\n",
      "Epoch 3053, Train Loss: 0.05226444825530052, Test Loss: 0.09269438683986664\n",
      "Epoch 3054, Train Loss: 0.052230630069971085, Test Loss: 0.09267625957727432\n",
      "Epoch 3055, Train Loss: 0.0521974042057991, Test Loss: 0.09264831990003586\n",
      "Epoch 3056, Train Loss: 0.0521647147834301, Test Loss: 0.09261749684810638\n",
      "Epoch 3057, Train Loss: 0.05213114246726036, Test Loss: 0.09257931262254715\n",
      "Epoch 3058, Train Loss: 0.052097730338573456, Test Loss: 0.0925382748246193\n",
      "Epoch 3059, Train Loss: 0.0520646758377552, Test Loss: 0.09251013398170471\n",
      "Epoch 3060, Train Loss: 0.05203137546777725, Test Loss: 0.09249561280012131\n",
      "Epoch 3061, Train Loss: 0.05199870467185974, Test Loss: 0.09246870130300522\n",
      "Epoch 3062, Train Loss: 0.05196550861001015, Test Loss: 0.09243151545524597\n",
      "Epoch 3063, Train Loss: 0.05193290114402771, Test Loss: 0.09239645302295685\n",
      "Epoch 3064, Train Loss: 0.0519004687666893, Test Loss: 0.0923665389418602\n",
      "Epoch 3065, Train Loss: 0.051867734640836716, Test Loss: 0.0923338308930397\n",
      "Epoch 3066, Train Loss: 0.051835477352142334, Test Loss: 0.09230764955282211\n",
      "Epoch 3067, Train Loss: 0.05180275812745094, Test Loss: 0.09226521104574203\n",
      "Epoch 3068, Train Loss: 0.051770374178886414, Test Loss: 0.09223359823226929\n",
      "Epoch 3069, Train Loss: 0.05173822119832039, Test Loss: 0.0922023355960846\n",
      "Epoch 3070, Train Loss: 0.051705893129110336, Test Loss: 0.09216398745775223\n",
      "Epoch 3071, Train Loss: 0.05167371779680252, Test Loss: 0.09214833378791809\n",
      "Epoch 3072, Train Loss: 0.0516418032348156, Test Loss: 0.09210506826639175\n",
      "Epoch 3073, Train Loss: 0.05160989984869957, Test Loss: 0.09207785874605179\n",
      "Epoch 3074, Train Loss: 0.051577936857938766, Test Loss: 0.09205613285303116\n",
      "Epoch 3075, Train Loss: 0.051545530557632446, Test Loss: 0.09201519936323166\n",
      "Epoch 3076, Train Loss: 0.051514070481061935, Test Loss: 0.09197226166725159\n",
      "Epoch 3077, Train Loss: 0.051482412964105606, Test Loss: 0.09194477647542953\n",
      "Epoch 3078, Train Loss: 0.05145030841231346, Test Loss: 0.0919283851981163\n",
      "Epoch 3079, Train Loss: 0.05141954869031906, Test Loss: 0.09189089387655258\n",
      "Epoch 3080, Train Loss: 0.051387377083301544, Test Loss: 0.09185805171728134\n",
      "Epoch 3081, Train Loss: 0.051356036216020584, Test Loss: 0.09182709455490112\n",
      "Epoch 3082, Train Loss: 0.05132463946938515, Test Loss: 0.09180144965648651\n",
      "Epoch 3083, Train Loss: 0.051294244825839996, Test Loss: 0.09177517145872116\n",
      "Epoch 3084, Train Loss: 0.05126198008656502, Test Loss: 0.0917457640171051\n",
      "Epoch 3085, Train Loss: 0.05123114585876465, Test Loss: 0.09170571714639664\n",
      "Epoch 3086, Train Loss: 0.05120009928941727, Test Loss: 0.09167612344026566\n",
      "Epoch 3087, Train Loss: 0.051169369369745255, Test Loss: 0.09166489541530609\n",
      "Epoch 3088, Train Loss: 0.051137879490852356, Test Loss: 0.09163249284029007\n",
      "Epoch 3089, Train Loss: 0.051106952130794525, Test Loss: 0.09160913527011871\n",
      "Epoch 3090, Train Loss: 0.051076337695121765, Test Loss: 0.09158001840114594\n",
      "Epoch 3091, Train Loss: 0.051046013832092285, Test Loss: 0.09153042733669281\n",
      "Epoch 3092, Train Loss: 0.05101488158106804, Test Loss: 0.0915074348449707\n",
      "Epoch 3093, Train Loss: 0.05098433792591095, Test Loss: 0.09147705882787704\n",
      "Epoch 3094, Train Loss: 0.05095426365733147, Test Loss: 0.09143750369548798\n",
      "Epoch 3095, Train Loss: 0.050923947244882584, Test Loss: 0.09140729159116745\n",
      "Epoch 3096, Train Loss: 0.050893329083919525, Test Loss: 0.09138806909322739\n",
      "Epoch 3097, Train Loss: 0.05086277052760124, Test Loss: 0.09136959910392761\n",
      "Epoch 3098, Train Loss: 0.05083271861076355, Test Loss: 0.09133424609899521\n",
      "Epoch 3099, Train Loss: 0.050802867859601974, Test Loss: 0.09129257500171661\n",
      "Epoch 3100, Train Loss: 0.05077249929308891, Test Loss: 0.09126918017864227\n",
      "Epoch 3101, Train Loss: 0.05074251815676689, Test Loss: 0.09124449640512466\n",
      "Epoch 3102, Train Loss: 0.050713106989860535, Test Loss: 0.09121309220790863\n",
      "Epoch 3103, Train Loss: 0.05068317800760269, Test Loss: 0.09119446575641632\n",
      "Epoch 3104, Train Loss: 0.05065285414457321, Test Loss: 0.09116113930940628\n",
      "Epoch 3105, Train Loss: 0.050623003393411636, Test Loss: 0.09111925214529037\n",
      "Epoch 3106, Train Loss: 0.050593726336956024, Test Loss: 0.0910913422703743\n",
      "Epoch 3107, Train Loss: 0.05056389793753624, Test Loss: 0.09106987714767456\n",
      "Epoch 3108, Train Loss: 0.05053449049592018, Test Loss: 0.09104267507791519\n",
      "Epoch 3109, Train Loss: 0.05050504580140114, Test Loss: 0.09102713316679001\n",
      "Epoch 3110, Train Loss: 0.05047600343823433, Test Loss: 0.09098664671182632\n",
      "Epoch 3111, Train Loss: 0.05044599995017052, Test Loss: 0.09096013009548187\n",
      "Epoch 3112, Train Loss: 0.05041643604636192, Test Loss: 0.0909443274140358\n",
      "Epoch 3113, Train Loss: 0.0503874309360981, Test Loss: 0.09091588109731674\n",
      "Epoch 3114, Train Loss: 0.050358183681964874, Test Loss: 0.09088771790266037\n",
      "Epoch 3115, Train Loss: 0.050329018384218216, Test Loss: 0.09086061269044876\n",
      "Epoch 3116, Train Loss: 0.05029980093240738, Test Loss: 0.09084167331457138\n",
      "Epoch 3117, Train Loss: 0.05027094483375549, Test Loss: 0.09080862253904343\n",
      "Epoch 3118, Train Loss: 0.05024177208542824, Test Loss: 0.09078424423933029\n",
      "Epoch 3119, Train Loss: 0.05021290108561516, Test Loss: 0.09074892848730087\n",
      "Epoch 3120, Train Loss: 0.05018455162644386, Test Loss: 0.09071296453475952\n",
      "Epoch 3121, Train Loss: 0.050155628472566605, Test Loss: 0.0906996801495552\n",
      "Epoch 3122, Train Loss: 0.05012688785791397, Test Loss: 0.09067665785551071\n",
      "Epoch 3123, Train Loss: 0.05009830743074417, Test Loss: 0.09065758436918259\n",
      "Epoch 3124, Train Loss: 0.05006960406899452, Test Loss: 0.09063120186328888\n",
      "Epoch 3125, Train Loss: 0.05004109814763069, Test Loss: 0.09060205519199371\n",
      "Epoch 3126, Train Loss: 0.05001281946897507, Test Loss: 0.09056852757930756\n",
      "Epoch 3127, Train Loss: 0.04998452961444855, Test Loss: 0.09055174887180328\n",
      "Epoch 3128, Train Loss: 0.04995615780353546, Test Loss: 0.09050891548395157\n",
      "Epoch 3129, Train Loss: 0.04992791637778282, Test Loss: 0.09047303348779678\n",
      "Epoch 3130, Train Loss: 0.04990000277757645, Test Loss: 0.09045542031526566\n",
      "Epoch 3131, Train Loss: 0.04987207427620888, Test Loss: 0.09042131155729294\n",
      "Epoch 3132, Train Loss: 0.04984388127923012, Test Loss: 0.09040111303329468\n",
      "Epoch 3133, Train Loss: 0.04981543868780136, Test Loss: 0.09038105607032776\n",
      "Epoch 3134, Train Loss: 0.04978760704398155, Test Loss: 0.0903501957654953\n",
      "Epoch 3135, Train Loss: 0.049760520458221436, Test Loss: 0.09033473581075668\n",
      "Epoch 3136, Train Loss: 0.049731746315956116, Test Loss: 0.09031648188829422\n",
      "Epoch 3137, Train Loss: 0.049704428762197495, Test Loss: 0.09029115736484528\n",
      "Epoch 3138, Train Loss: 0.04967617988586426, Test Loss: 0.09026151895523071\n",
      "Epoch 3139, Train Loss: 0.04964851588010788, Test Loss: 0.09023512899875641\n",
      "Epoch 3140, Train Loss: 0.04962131008505821, Test Loss: 0.09020576626062393\n",
      "Epoch 3141, Train Loss: 0.04959367588162422, Test Loss: 0.09017575532197952\n",
      "Epoch 3142, Train Loss: 0.049566131085157394, Test Loss: 0.09016025066375732\n",
      "Epoch 3143, Train Loss: 0.04953872784972191, Test Loss: 0.09013713151216507\n",
      "Epoch 3144, Train Loss: 0.049511365592479706, Test Loss: 0.09011291712522507\n",
      "Epoch 3145, Train Loss: 0.04948457330465317, Test Loss: 0.09007993340492249\n",
      "Epoch 3146, Train Loss: 0.0494578592479229, Test Loss: 0.09005583822727203\n",
      "Epoch 3147, Train Loss: 0.049429699778556824, Test Loss: 0.09003772586584091\n",
      "Epoch 3148, Train Loss: 0.049402881413698196, Test Loss: 0.09001477807760239\n",
      "Epoch 3149, Train Loss: 0.04937520995736122, Test Loss: 0.08997934311628342\n",
      "Epoch 3150, Train Loss: 0.049348484724760056, Test Loss: 0.08994710445404053\n",
      "Epoch 3151, Train Loss: 0.04932212829589844, Test Loss: 0.08993532508611679\n",
      "Epoch 3152, Train Loss: 0.04929477348923683, Test Loss: 0.08990181982517242\n",
      "Epoch 3153, Train Loss: 0.049267642199993134, Test Loss: 0.08987656235694885\n",
      "Epoch 3154, Train Loss: 0.04924061894416809, Test Loss: 0.08983725309371948\n",
      "Epoch 3155, Train Loss: 0.049213822931051254, Test Loss: 0.08981847763061523\n",
      "Epoch 3156, Train Loss: 0.049187082797288895, Test Loss: 0.08980555832386017\n",
      "Epoch 3157, Train Loss: 0.049160465598106384, Test Loss: 0.08978327363729477\n",
      "Epoch 3158, Train Loss: 0.0491337776184082, Test Loss: 0.0897558182477951\n",
      "Epoch 3159, Train Loss: 0.04910758510231972, Test Loss: 0.08973010629415512\n",
      "Epoch 3160, Train Loss: 0.04908081889152527, Test Loss: 0.08971957862377167\n",
      "Epoch 3161, Train Loss: 0.0490543395280838, Test Loss: 0.0896829292178154\n",
      "Epoch 3162, Train Loss: 0.04902810603380203, Test Loss: 0.08965402841567993\n",
      "Epoch 3163, Train Loss: 0.04900159314274788, Test Loss: 0.08963628858327866\n",
      "Epoch 3164, Train Loss: 0.04897559434175491, Test Loss: 0.08961594849824905\n",
      "Epoch 3165, Train Loss: 0.04894888401031494, Test Loss: 0.089592345058918\n",
      "Epoch 3166, Train Loss: 0.04892292618751526, Test Loss: 0.08957184106111526\n",
      "Epoch 3167, Train Loss: 0.048897020518779755, Test Loss: 0.0895395502448082\n",
      "Epoch 3168, Train Loss: 0.04887126013636589, Test Loss: 0.08951351791620255\n",
      "Epoch 3169, Train Loss: 0.0488455593585968, Test Loss: 0.08950253576040268\n",
      "Epoch 3170, Train Loss: 0.048820529133081436, Test Loss: 0.08947315812110901\n",
      "Epoch 3171, Train Loss: 0.04879353195428848, Test Loss: 0.08945143967866898\n",
      "Epoch 3172, Train Loss: 0.0487670861184597, Test Loss: 0.08943583816289902\n",
      "Epoch 3173, Train Loss: 0.04874149709939957, Test Loss: 0.08941077440977097\n",
      "Epoch 3174, Train Loss: 0.0487157441675663, Test Loss: 0.089382603764534\n",
      "Epoch 3175, Train Loss: 0.04868987947702408, Test Loss: 0.08936700969934464\n",
      "Epoch 3176, Train Loss: 0.04866398870944977, Test Loss: 0.08934230357408524\n",
      "Epoch 3177, Train Loss: 0.04863839969038963, Test Loss: 0.08931291848421097\n",
      "Epoch 3178, Train Loss: 0.04861307889223099, Test Loss: 0.08928056061267853\n",
      "Epoch 3179, Train Loss: 0.048587750643491745, Test Loss: 0.08925104886293411\n",
      "Epoch 3180, Train Loss: 0.04856191575527191, Test Loss: 0.08923909068107605\n",
      "Epoch 3181, Train Loss: 0.04853656515479088, Test Loss: 0.08921505510807037\n",
      "Epoch 3182, Train Loss: 0.04851146042346954, Test Loss: 0.0891924798488617\n",
      "Epoch 3183, Train Loss: 0.0484863817691803, Test Loss: 0.08916725218296051\n",
      "Epoch 3184, Train Loss: 0.04846104606986046, Test Loss: 0.0891520157456398\n",
      "Epoch 3185, Train Loss: 0.04843538999557495, Test Loss: 0.08912903815507889\n",
      "Epoch 3186, Train Loss: 0.04841041937470436, Test Loss: 0.0891159251332283\n",
      "Epoch 3187, Train Loss: 0.048385173082351685, Test Loss: 0.0890999510884285\n",
      "Epoch 3188, Train Loss: 0.04836057499051094, Test Loss: 0.08906828612089157\n",
      "Epoch 3189, Train Loss: 0.04833479970693588, Test Loss: 0.08904649317264557\n",
      "Epoch 3190, Train Loss: 0.048310358077287674, Test Loss: 0.08902668952941895\n",
      "Epoch 3191, Train Loss: 0.0482853502035141, Test Loss: 0.08900023251771927\n",
      "Epoch 3192, Train Loss: 0.0482608862221241, Test Loss: 0.0889701396226883\n",
      "Epoch 3193, Train Loss: 0.04823504388332367, Test Loss: 0.08895894885063171\n",
      "Epoch 3194, Train Loss: 0.04821080341935158, Test Loss: 0.08893361687660217\n",
      "Epoch 3195, Train Loss: 0.04818544536828995, Test Loss: 0.08892031013965607\n",
      "Epoch 3196, Train Loss: 0.04816059395670891, Test Loss: 0.08889634162187576\n",
      "Epoch 3197, Train Loss: 0.04813629761338234, Test Loss: 0.08887989073991776\n",
      "Epoch 3198, Train Loss: 0.0481119342148304, Test Loss: 0.08885953575372696\n",
      "Epoch 3199, Train Loss: 0.04808701574802399, Test Loss: 0.0888340175151825\n",
      "Epoch 3200, Train Loss: 0.048062585294246674, Test Loss: 0.08880864828824997\n",
      "Epoch 3201, Train Loss: 0.048038139939308167, Test Loss: 0.08878854662179947\n",
      "Epoch 3202, Train Loss: 0.04801374301314354, Test Loss: 0.08877354860305786\n",
      "Epoch 3203, Train Loss: 0.04799012094736099, Test Loss: 0.08875920623540878\n",
      "Epoch 3204, Train Loss: 0.047965098172426224, Test Loss: 0.08873672783374786\n",
      "Epoch 3205, Train Loss: 0.047941744327545166, Test Loss: 0.08870818465948105\n",
      "Epoch 3206, Train Loss: 0.04791712760925293, Test Loss: 0.08867819607257843\n",
      "Epoch 3207, Train Loss: 0.04789238050580025, Test Loss: 0.08865387737751007\n",
      "Epoch 3208, Train Loss: 0.04786839336156845, Test Loss: 0.08863060176372528\n",
      "Epoch 3209, Train Loss: 0.047844599932432175, Test Loss: 0.08861420303583145\n",
      "Epoch 3210, Train Loss: 0.047820959240198135, Test Loss: 0.08858061581850052\n",
      "Epoch 3211, Train Loss: 0.04779646173119545, Test Loss: 0.0885712057352066\n",
      "Epoch 3212, Train Loss: 0.04777226597070694, Test Loss: 0.08854347467422485\n",
      "Epoch 3213, Train Loss: 0.047749970108270645, Test Loss: 0.08851849287748337\n",
      "Epoch 3214, Train Loss: 0.047725047916173935, Test Loss: 0.08849899470806122\n",
      "Epoch 3215, Train Loss: 0.04770172759890556, Test Loss: 0.08847705274820328\n",
      "Epoch 3216, Train Loss: 0.047677699476480484, Test Loss: 0.08846403658390045\n",
      "Epoch 3217, Train Loss: 0.04765378311276436, Test Loss: 0.08845135569572449\n",
      "Epoch 3218, Train Loss: 0.04763022065162659, Test Loss: 0.08842286467552185\n",
      "Epoch 3219, Train Loss: 0.04760642722249031, Test Loss: 0.08840727806091309\n",
      "Epoch 3220, Train Loss: 0.047582950443029404, Test Loss: 0.08837785571813583\n",
      "Epoch 3221, Train Loss: 0.047559741884469986, Test Loss: 0.08836148679256439\n",
      "Epoch 3222, Train Loss: 0.04753663390874863, Test Loss: 0.0883297473192215\n",
      "Epoch 3223, Train Loss: 0.047513753175735474, Test Loss: 0.08830022066831589\n",
      "Epoch 3224, Train Loss: 0.047490090131759644, Test Loss: 0.08829352259635925\n",
      "Epoch 3225, Train Loss: 0.04746643826365471, Test Loss: 0.08827023208141327\n",
      "Epoch 3226, Train Loss: 0.04744311049580574, Test Loss: 0.08825013786554337\n",
      "Epoch 3227, Train Loss: 0.047420088201761246, Test Loss: 0.08824611455202103\n",
      "Epoch 3228, Train Loss: 0.04739706590771675, Test Loss: 0.08821208775043488\n",
      "Epoch 3229, Train Loss: 0.0473739318549633, Test Loss: 0.08820328116416931\n",
      "Epoch 3230, Train Loss: 0.047351039946079254, Test Loss: 0.08818623423576355\n",
      "Epoch 3231, Train Loss: 0.04732784256339073, Test Loss: 0.08816102892160416\n",
      "Epoch 3232, Train Loss: 0.047305963933467865, Test Loss: 0.08812640607357025\n",
      "Epoch 3233, Train Loss: 0.047281816601753235, Test Loss: 0.08811414241790771\n",
      "Epoch 3234, Train Loss: 0.04725879430770874, Test Loss: 0.08810039609670639\n",
      "Epoch 3235, Train Loss: 0.04723602533340454, Test Loss: 0.0880846157670021\n",
      "Epoch 3236, Train Loss: 0.04721354320645332, Test Loss: 0.08805767446756363\n",
      "Epoch 3237, Train Loss: 0.047191135585308075, Test Loss: 0.0880400612950325\n",
      "Epoch 3238, Train Loss: 0.047167979180812836, Test Loss: 0.08802870661020279\n",
      "Epoch 3239, Train Loss: 0.04714654013514519, Test Loss: 0.08799584209918976\n",
      "Epoch 3240, Train Loss: 0.04712409898638725, Test Loss: 0.08797497302293777\n",
      "Epoch 3241, Train Loss: 0.04710090160369873, Test Loss: 0.08796394616365433\n",
      "Epoch 3242, Train Loss: 0.04707765206694603, Test Loss: 0.08793525397777557\n",
      "Epoch 3243, Train Loss: 0.04705531895160675, Test Loss: 0.08790865540504456\n",
      "Epoch 3244, Train Loss: 0.04703295975923538, Test Loss: 0.08788978308439255\n",
      "Epoch 3245, Train Loss: 0.04701019078493118, Test Loss: 0.08789130300283432\n",
      "Epoch 3246, Train Loss: 0.046987805515527725, Test Loss: 0.08787572383880615\n",
      "Epoch 3247, Train Loss: 0.046965327113866806, Test Loss: 0.08785169571638107\n",
      "Epoch 3248, Train Loss: 0.046943094581365585, Test Loss: 0.08782967180013657\n",
      "Epoch 3249, Train Loss: 0.04692120850086212, Test Loss: 0.08782507479190826\n",
      "Epoch 3250, Train Loss: 0.046898726373910904, Test Loss: 0.08780177682638168\n",
      "Epoch 3251, Train Loss: 0.04687639698386192, Test Loss: 0.08778591454029083\n",
      "Epoch 3252, Train Loss: 0.04685423523187637, Test Loss: 0.08775942772626877\n",
      "Epoch 3253, Train Loss: 0.04683222249150276, Test Loss: 0.0877445712685585\n",
      "Epoch 3254, Train Loss: 0.046810053288936615, Test Loss: 0.087722547352314\n",
      "Epoch 3255, Train Loss: 0.046788476407527924, Test Loss: 0.08769985288381577\n",
      "Epoch 3256, Train Loss: 0.046765998005867004, Test Loss: 0.08767668157815933\n",
      "Epoch 3257, Train Loss: 0.046744272112846375, Test Loss: 0.08766565471887589\n",
      "Epoch 3258, Train Loss: 0.04672294855117798, Test Loss: 0.08762935549020767\n",
      "Epoch 3259, Train Loss: 0.04670139029622078, Test Loss: 0.08760285377502441\n",
      "Epoch 3260, Train Loss: 0.04667910560965538, Test Loss: 0.08758717775344849\n",
      "Epoch 3261, Train Loss: 0.046657007187604904, Test Loss: 0.08756887912750244\n",
      "Epoch 3262, Train Loss: 0.04663531854748726, Test Loss: 0.08755157887935638\n",
      "Epoch 3263, Train Loss: 0.046613678336143494, Test Loss: 0.08752920478582382\n",
      "Epoch 3264, Train Loss: 0.04659232869744301, Test Loss: 0.08751700073480606\n",
      "Epoch 3265, Train Loss: 0.04656993970274925, Test Loss: 0.08751381188631058\n",
      "Epoch 3266, Train Loss: 0.04654917120933533, Test Loss: 0.08749708533287048\n",
      "Epoch 3267, Train Loss: 0.04652697965502739, Test Loss: 0.08747673779726028\n",
      "Epoch 3268, Train Loss: 0.046505335718393326, Test Loss: 0.08746153116226196\n",
      "Epoch 3269, Train Loss: 0.04648420214653015, Test Loss: 0.08743645995855331\n",
      "Epoch 3270, Train Loss: 0.046463239938020706, Test Loss: 0.08740708976984024\n",
      "Epoch 3271, Train Loss: 0.04644118621945381, Test Loss: 0.0874035432934761\n",
      "Epoch 3272, Train Loss: 0.04641973227262497, Test Loss: 0.08739326894283295\n",
      "Epoch 3273, Train Loss: 0.046398360282182693, Test Loss: 0.0873732641339302\n",
      "Epoch 3274, Train Loss: 0.0463770367205143, Test Loss: 0.08734749257564545\n",
      "Epoch 3275, Train Loss: 0.04635588452219963, Test Loss: 0.08732329308986664\n",
      "Epoch 3276, Train Loss: 0.046334877610206604, Test Loss: 0.08729948848485947\n",
      "Epoch 3277, Train Loss: 0.04631347209215164, Test Loss: 0.08729737997055054\n",
      "Epoch 3278, Train Loss: 0.04629376903176308, Test Loss: 0.0872841402888298\n",
      "Epoch 3279, Train Loss: 0.046271465718746185, Test Loss: 0.08724430203437805\n",
      "Epoch 3280, Train Loss: 0.04625041410326958, Test Loss: 0.0872296392917633\n",
      "Epoch 3281, Train Loss: 0.04622924327850342, Test Loss: 0.0872221291065216\n",
      "Epoch 3282, Train Loss: 0.04620804265141487, Test Loss: 0.08720305562019348\n",
      "Epoch 3283, Train Loss: 0.04618752375245094, Test Loss: 0.08717277646064758\n",
      "Epoch 3284, Train Loss: 0.04616625979542732, Test Loss: 0.08716841787099838\n",
      "Epoch 3285, Train Loss: 0.04614538699388504, Test Loss: 0.08716102689504623\n",
      "Epoch 3286, Train Loss: 0.04612495005130768, Test Loss: 0.08714064210653305\n",
      "Epoch 3287, Train Loss: 0.04610363394021988, Test Loss: 0.08712571114301682\n",
      "Epoch 3288, Train Loss: 0.04608304798603058, Test Loss: 0.08711191266775131\n",
      "Epoch 3289, Train Loss: 0.046062350273132324, Test Loss: 0.08709999918937683\n",
      "Epoch 3290, Train Loss: 0.04604174196720123, Test Loss: 0.08707425743341446\n",
      "Epoch 3291, Train Loss: 0.046021562069654465, Test Loss: 0.0870451107621193\n",
      "Epoch 3292, Train Loss: 0.04600071907043457, Test Loss: 0.08703107386827469\n",
      "Epoch 3293, Train Loss: 0.045980311930179596, Test Loss: 0.08702269196510315\n",
      "Epoch 3294, Train Loss: 0.04595910757780075, Test Loss: 0.08700727671384811\n",
      "Epoch 3295, Train Loss: 0.045938700437545776, Test Loss: 0.08698561787605286\n",
      "Epoch 3296, Train Loss: 0.045918263494968414, Test Loss: 0.0869617834687233\n",
      "Epoch 3297, Train Loss: 0.04589839279651642, Test Loss: 0.08695200830698013\n",
      "Epoch 3298, Train Loss: 0.04587746784090996, Test Loss: 0.08691797405481339\n",
      "Epoch 3299, Train Loss: 0.04585651680827141, Test Loss: 0.08692079782485962\n",
      "Epoch 3300, Train Loss: 0.045836176723241806, Test Loss: 0.08690883219242096\n",
      "Epoch 3301, Train Loss: 0.04581613466143608, Test Loss: 0.08688879758119583\n",
      "Epoch 3302, Train Loss: 0.04579571262001991, Test Loss: 0.0868627279996872\n",
      "Epoch 3303, Train Loss: 0.045775171369314194, Test Loss: 0.08684440702199936\n",
      "Epoch 3304, Train Loss: 0.04575522989034653, Test Loss: 0.08682022243738174\n",
      "Epoch 3305, Train Loss: 0.045735184103250504, Test Loss: 0.08680698275566101\n",
      "Epoch 3306, Train Loss: 0.04571495205163956, Test Loss: 0.08680392801761627\n",
      "Epoch 3307, Train Loss: 0.04569505900144577, Test Loss: 0.08677075803279877\n",
      "Epoch 3308, Train Loss: 0.04567496478557587, Test Loss: 0.08675560355186462\n",
      "Epoch 3309, Train Loss: 0.04565448313951492, Test Loss: 0.08675186336040497\n",
      "Epoch 3310, Train Loss: 0.04563481733202934, Test Loss: 0.08674008399248123\n",
      "Epoch 3311, Train Loss: 0.045614439994096756, Test Loss: 0.08673151582479477\n",
      "Epoch 3312, Train Loss: 0.04559476301074028, Test Loss: 0.08670643717050552\n",
      "Epoch 3313, Train Loss: 0.0455748550593853, Test Loss: 0.08668404817581177\n",
      "Epoch 3314, Train Loss: 0.04555509239435196, Test Loss: 0.0866728350520134\n",
      "Epoch 3315, Train Loss: 0.0455356165766716, Test Loss: 0.0866502970457077\n",
      "Epoch 3316, Train Loss: 0.04551507905125618, Test Loss: 0.08663533627986908\n",
      "Epoch 3317, Train Loss: 0.045495983213186264, Test Loss: 0.08661870658397675\n",
      "Epoch 3318, Train Loss: 0.04547559469938278, Test Loss: 0.08661030232906342\n",
      "Epoch 3319, Train Loss: 0.045456450432538986, Test Loss: 0.08660001307725906\n",
      "Epoch 3320, Train Loss: 0.045436792075634, Test Loss: 0.08658912032842636\n",
      "Epoch 3321, Train Loss: 0.04541716352105141, Test Loss: 0.0865616500377655\n",
      "Epoch 3322, Train Loss: 0.045397866517305374, Test Loss: 0.0865524560213089\n",
      "Epoch 3323, Train Loss: 0.04537774622440338, Test Loss: 0.08654173463582993\n",
      "Epoch 3324, Train Loss: 0.04535827040672302, Test Loss: 0.08651085942983627\n",
      "Epoch 3325, Train Loss: 0.045338671654462814, Test Loss: 0.08650104701519012\n",
      "Epoch 3326, Train Loss: 0.04531896114349365, Test Loss: 0.08647660166025162\n",
      "Epoch 3327, Train Loss: 0.0452994666993618, Test Loss: 0.08645765483379364\n",
      "Epoch 3328, Train Loss: 0.04528012126684189, Test Loss: 0.08644163608551025\n",
      "Epoch 3329, Train Loss: 0.04526044800877571, Test Loss: 0.08642985671758652\n",
      "Epoch 3330, Train Loss: 0.0452408492565155, Test Loss: 0.0864221379160881\n",
      "Epoch 3331, Train Loss: 0.04522204026579857, Test Loss: 0.08641519397497177\n",
      "Epoch 3332, Train Loss: 0.04520237073302269, Test Loss: 0.08639330416917801\n",
      "Epoch 3333, Train Loss: 0.04518312215805054, Test Loss: 0.08637835830450058\n",
      "Epoch 3334, Train Loss: 0.04516347870230675, Test Loss: 0.08635517954826355\n",
      "Epoch 3335, Train Loss: 0.045144256204366684, Test Loss: 0.08633952587842941\n",
      "Epoch 3336, Train Loss: 0.04512534290552139, Test Loss: 0.08631228655576706\n",
      "Epoch 3337, Train Loss: 0.04510599002242088, Test Loss: 0.08629816025495529\n",
      "Epoch 3338, Train Loss: 0.04508696496486664, Test Loss: 0.08629775792360306\n",
      "Epoch 3339, Train Loss: 0.04506753757596016, Test Loss: 0.08628744632005692\n",
      "Epoch 3340, Train Loss: 0.045048512518405914, Test Loss: 0.08626414090394974\n",
      "Epoch 3341, Train Loss: 0.04502958431839943, Test Loss: 0.08625852316617966\n",
      "Epoch 3342, Train Loss: 0.04501072317361832, Test Loss: 0.08623845130205154\n",
      "Epoch 3343, Train Loss: 0.0449916310608387, Test Loss: 0.0862281545996666\n",
      "Epoch 3344, Train Loss: 0.044972632080316544, Test Loss: 0.08621692657470703\n",
      "Epoch 3345, Train Loss: 0.0449536107480526, Test Loss: 0.08618268370628357\n",
      "Epoch 3346, Train Loss: 0.044935014098882675, Test Loss: 0.08617224544286728\n",
      "Epoch 3347, Train Loss: 0.04491578787565231, Test Loss: 0.08616113662719727\n",
      "Epoch 3348, Train Loss: 0.0448966920375824, Test Loss: 0.08614415675401688\n",
      "Epoch 3349, Train Loss: 0.0448782816529274, Test Loss: 0.0861380398273468\n",
      "Epoch 3350, Train Loss: 0.044859327375888824, Test Loss: 0.08611360937356949\n",
      "Epoch 3351, Train Loss: 0.04484013840556145, Test Loss: 0.08610419929027557\n",
      "Epoch 3352, Train Loss: 0.044821597635746, Test Loss: 0.08609367161989212\n",
      "Epoch 3353, Train Loss: 0.04480280354619026, Test Loss: 0.08607426285743713\n",
      "Epoch 3354, Train Loss: 0.044784288853406906, Test Loss: 0.08606140315532684\n",
      "Epoch 3355, Train Loss: 0.04476555809378624, Test Loss: 0.08603595197200775\n",
      "Epoch 3356, Train Loss: 0.04474708065390587, Test Loss: 0.08601676672697067\n",
      "Epoch 3357, Train Loss: 0.044728364795446396, Test Loss: 0.08599793165922165\n",
      "Epoch 3358, Train Loss: 0.04470963031053543, Test Loss: 0.08598536252975464\n",
      "Epoch 3359, Train Loss: 0.04469131678342819, Test Loss: 0.08596543222665787\n",
      "Epoch 3360, Train Loss: 0.044672876596450806, Test Loss: 0.08595573157072067\n",
      "Epoch 3361, Train Loss: 0.04465421289205551, Test Loss: 0.08594033122062683\n",
      "Epoch 3362, Train Loss: 0.04463604837656021, Test Loss: 0.08592548221349716\n",
      "Epoch 3363, Train Loss: 0.04461723193526268, Test Loss: 0.08591421693563461\n",
      "Epoch 3364, Train Loss: 0.04459929093718529, Test Loss: 0.08589667081832886\n",
      "Epoch 3365, Train Loss: 0.044580813497304916, Test Loss: 0.08588527143001556\n",
      "Epoch 3366, Train Loss: 0.04456228017807007, Test Loss: 0.08588554710149765\n",
      "Epoch 3367, Train Loss: 0.04454494267702103, Test Loss: 0.0858708843588829\n",
      "Epoch 3368, Train Loss: 0.04452568292617798, Test Loss: 0.08585947006940842\n",
      "Epoch 3369, Train Loss: 0.04450729116797447, Test Loss: 0.08583909273147583\n",
      "Epoch 3370, Train Loss: 0.04448927566409111, Test Loss: 0.08581984788179398\n",
      "Epoch 3371, Train Loss: 0.044472116976976395, Test Loss: 0.08580724149942398\n",
      "Epoch 3372, Train Loss: 0.04445308446884155, Test Loss: 0.08580708503723145\n",
      "Epoch 3373, Train Loss: 0.04443500563502312, Test Loss: 0.08578343689441681\n",
      "Epoch 3374, Train Loss: 0.04441647604107857, Test Loss: 0.08576427400112152\n",
      "Epoch 3375, Train Loss: 0.044398464262485504, Test Loss: 0.08574727177619934\n",
      "Epoch 3376, Train Loss: 0.044380396604537964, Test Loss: 0.08574151992797852\n",
      "Epoch 3377, Train Loss: 0.04436244070529938, Test Loss: 0.08572015911340714\n",
      "Epoch 3378, Train Loss: 0.0443447045981884, Test Loss: 0.0857047364115715\n",
      "Epoch 3379, Train Loss: 0.04432711377739906, Test Loss: 0.08568762242794037\n",
      "Epoch 3380, Train Loss: 0.04430857673287392, Test Loss: 0.08567917346954346\n",
      "Epoch 3381, Train Loss: 0.04429071396589279, Test Loss: 0.08565618842840195\n",
      "Epoch 3382, Train Loss: 0.044273026287555695, Test Loss: 0.08563585579395294\n",
      "Epoch 3383, Train Loss: 0.0442548543214798, Test Loss: 0.08563005179166794\n",
      "Epoch 3384, Train Loss: 0.04423690214753151, Test Loss: 0.08561382442712784\n",
      "Epoch 3385, Train Loss: 0.044218916445970535, Test Loss: 0.08561483770608902\n",
      "Epoch 3386, Train Loss: 0.04420110583305359, Test Loss: 0.0855969712138176\n",
      "Epoch 3387, Train Loss: 0.04418379068374634, Test Loss: 0.08558334410190582\n",
      "Epoch 3388, Train Loss: 0.04416557773947716, Test Loss: 0.0855756551027298\n",
      "Epoch 3389, Train Loss: 0.044147927314043045, Test Loss: 0.08556006103754044\n",
      "Epoch 3390, Train Loss: 0.04413013532757759, Test Loss: 0.08554880321025848\n",
      "Epoch 3391, Train Loss: 0.04411245882511139, Test Loss: 0.08553225547075272\n",
      "Epoch 3392, Train Loss: 0.04409543052315712, Test Loss: 0.08550078421831131\n",
      "Epoch 3393, Train Loss: 0.04407723620533943, Test Loss: 0.08548211306333542\n",
      "Epoch 3394, Train Loss: 0.04405991733074188, Test Loss: 0.08546184748411179\n",
      "Epoch 3395, Train Loss: 0.04404190182685852, Test Loss: 0.08546116948127747\n",
      "Epoch 3396, Train Loss: 0.04402419179677963, Test Loss: 0.08545692265033722\n",
      "Epoch 3397, Train Loss: 0.04400673136115074, Test Loss: 0.08543334156274796\n",
      "Epoch 3398, Train Loss: 0.04398958757519722, Test Loss: 0.0854228138923645\n",
      "Epoch 3399, Train Loss: 0.04397176206111908, Test Loss: 0.08540760725736618\n",
      "Epoch 3400, Train Loss: 0.04395454749464989, Test Loss: 0.08539649099111557\n",
      "Epoch 3401, Train Loss: 0.0439373180270195, Test Loss: 0.08539941906929016\n",
      "Epoch 3402, Train Loss: 0.043919362127780914, Test Loss: 0.08538389205932617\n",
      "Epoch 3403, Train Loss: 0.04390229657292366, Test Loss: 0.08537456393241882\n",
      "Epoch 3404, Train Loss: 0.04388505220413208, Test Loss: 0.0853542760014534\n",
      "Epoch 3405, Train Loss: 0.04386778548359871, Test Loss: 0.08533761650323868\n",
      "Epoch 3406, Train Loss: 0.043850526213645935, Test Loss: 0.08531831949949265\n",
      "Epoch 3407, Train Loss: 0.04383299499750137, Test Loss: 0.08531813323497772\n",
      "Epoch 3408, Train Loss: 0.04381561279296875, Test Loss: 0.0853114202618599\n",
      "Epoch 3409, Train Loss: 0.04379844665527344, Test Loss: 0.08529365807771683\n",
      "Epoch 3410, Train Loss: 0.04378117620944977, Test Loss: 0.08528698980808258\n",
      "Epoch 3411, Train Loss: 0.04376409202814102, Test Loss: 0.08527536690235138\n",
      "Epoch 3412, Train Loss: 0.04374701529741287, Test Loss: 0.08524345606565475\n",
      "Epoch 3413, Train Loss: 0.04372992366552353, Test Loss: 0.0852183923125267\n",
      "Epoch 3414, Train Loss: 0.043712615966796875, Test Loss: 0.08522402495145798\n",
      "Epoch 3415, Train Loss: 0.04369574040174484, Test Loss: 0.08521269261837006\n",
      "Epoch 3416, Train Loss: 0.04367942363023758, Test Loss: 0.08518854528665543\n",
      "Epoch 3417, Train Loss: 0.043661609292030334, Test Loss: 0.08518713712692261\n",
      "Epoch 3418, Train Loss: 0.04364458844065666, Test Loss: 0.08517559617757797\n",
      "Epoch 3419, Train Loss: 0.04362792521715164, Test Loss: 0.0851612389087677\n",
      "Epoch 3420, Train Loss: 0.04361070320010185, Test Loss: 0.08515127003192902\n",
      "Epoch 3421, Train Loss: 0.04359385371208191, Test Loss: 0.08514430373907089\n",
      "Epoch 3422, Train Loss: 0.04357733950018883, Test Loss: 0.08511549234390259\n",
      "Epoch 3423, Train Loss: 0.04356052353978157, Test Loss: 0.08511658012866974\n",
      "Epoch 3424, Train Loss: 0.04354342818260193, Test Loss: 0.08509629219770432\n",
      "Epoch 3425, Train Loss: 0.043527379631996155, Test Loss: 0.08507533371448517\n",
      "Epoch 3426, Train Loss: 0.04351036995649338, Test Loss: 0.08506181836128235\n",
      "Epoch 3427, Train Loss: 0.04349324107170105, Test Loss: 0.08505723625421524\n",
      "Epoch 3428, Train Loss: 0.043476466089487076, Test Loss: 0.0850394070148468\n",
      "Epoch 3429, Train Loss: 0.043459534645080566, Test Loss: 0.08502008765935898\n",
      "Epoch 3430, Train Loss: 0.04344300925731659, Test Loss: 0.08500262349843979\n",
      "Epoch 3431, Train Loss: 0.043426379561424255, Test Loss: 0.08498695492744446\n",
      "Epoch 3432, Train Loss: 0.04340989887714386, Test Loss: 0.08497735857963562\n",
      "Epoch 3433, Train Loss: 0.0433930978178978, Test Loss: 0.08496913313865662\n",
      "Epoch 3434, Train Loss: 0.04337657243013382, Test Loss: 0.08495751023292542\n",
      "Epoch 3435, Train Loss: 0.04335985705256462, Test Loss: 0.08495273441076279\n",
      "Epoch 3436, Train Loss: 0.04334373027086258, Test Loss: 0.08493547141551971\n",
      "Epoch 3437, Train Loss: 0.04332711920142174, Test Loss: 0.08492297679185867\n",
      "Epoch 3438, Train Loss: 0.04331039637327194, Test Loss: 0.08491253852844238\n",
      "Epoch 3439, Train Loss: 0.04329361766576767, Test Loss: 0.08489751815795898\n",
      "Epoch 3440, Train Loss: 0.04327746108174324, Test Loss: 0.08488452434539795\n",
      "Epoch 3441, Train Loss: 0.04326079040765762, Test Loss: 0.08488442003726959\n",
      "Epoch 3442, Train Loss: 0.04324452579021454, Test Loss: 0.08487488329410553\n",
      "Epoch 3443, Train Loss: 0.04322773590683937, Test Loss: 0.08486395329236984\n",
      "Epoch 3444, Train Loss: 0.04321152716875076, Test Loss: 0.08484715223312378\n",
      "Epoch 3445, Train Loss: 0.04319596290588379, Test Loss: 0.08482450991868973\n",
      "Epoch 3446, Train Loss: 0.04317871853709221, Test Loss: 0.08482492715120316\n",
      "Epoch 3447, Train Loss: 0.04316258803009987, Test Loss: 0.08479976654052734\n",
      "Epoch 3448, Train Loss: 0.043145958334207535, Test Loss: 0.08480124175548553\n",
      "Epoch 3449, Train Loss: 0.04313018545508385, Test Loss: 0.08478988707065582\n",
      "Epoch 3450, Train Loss: 0.04311360791325569, Test Loss: 0.08479882776737213\n",
      "Epoch 3451, Train Loss: 0.043097276240587234, Test Loss: 0.08478372544050217\n",
      "Epoch 3452, Train Loss: 0.04308168962597847, Test Loss: 0.08475742489099503\n",
      "Epoch 3453, Train Loss: 0.04306543245911598, Test Loss: 0.08473947644233704\n",
      "Epoch 3454, Train Loss: 0.04304865747690201, Test Loss: 0.08472178876399994\n",
      "Epoch 3455, Train Loss: 0.04303285479545593, Test Loss: 0.08470645546913147\n",
      "Epoch 3456, Train Loss: 0.04301660880446434, Test Loss: 0.08469179272651672\n",
      "Epoch 3457, Train Loss: 0.043000686913728714, Test Loss: 0.08468330651521683\n",
      "Epoch 3458, Train Loss: 0.04298458248376846, Test Loss: 0.08468455821275711\n",
      "Epoch 3459, Train Loss: 0.04296951740980148, Test Loss: 0.08466339856386185\n",
      "Epoch 3460, Train Loss: 0.0429525300860405, Test Loss: 0.08465936779975891\n",
      "Epoch 3461, Train Loss: 0.04293634742498398, Test Loss: 0.0846436396241188\n",
      "Epoch 3462, Train Loss: 0.04292021319270134, Test Loss: 0.08463151752948761\n",
      "Epoch 3463, Train Loss: 0.042904697358608246, Test Loss: 0.08461654931306839\n",
      "Epoch 3464, Train Loss: 0.042889244854450226, Test Loss: 0.08460255712270737\n",
      "Epoch 3465, Train Loss: 0.04287301003932953, Test Loss: 0.08458729088306427\n",
      "Epoch 3466, Train Loss: 0.0428573414683342, Test Loss: 0.08458021283149719\n",
      "Epoch 3467, Train Loss: 0.04284114018082619, Test Loss: 0.08457300066947937\n",
      "Epoch 3468, Train Loss: 0.04282507672905922, Test Loss: 0.08457094430923462\n",
      "Epoch 3469, Train Loss: 0.04280919209122658, Test Loss: 0.08454383164644241\n",
      "Epoch 3470, Train Loss: 0.04279365390539169, Test Loss: 0.08454374969005585\n",
      "Epoch 3471, Train Loss: 0.04277773201465607, Test Loss: 0.08452215045690536\n",
      "Epoch 3472, Train Loss: 0.04276232421398163, Test Loss: 0.08450642973184586\n",
      "Epoch 3473, Train Loss: 0.042746651917696, Test Loss: 0.08449303358793259\n",
      "Epoch 3474, Train Loss: 0.04273080825805664, Test Loss: 0.08447905629873276\n",
      "Epoch 3475, Train Loss: 0.04271511361002922, Test Loss: 0.08446945250034332\n",
      "Epoch 3476, Train Loss: 0.042700350284576416, Test Loss: 0.08444701135158539\n",
      "Epoch 3477, Train Loss: 0.04268383979797363, Test Loss: 0.08444299548864365\n",
      "Epoch 3478, Train Loss: 0.04266855865716934, Test Loss: 0.08443046361207962\n",
      "Epoch 3479, Train Loss: 0.04265237972140312, Test Loss: 0.08442696183919907\n",
      "Epoch 3480, Train Loss: 0.04263658449053764, Test Loss: 0.0844246968626976\n",
      "Epoch 3481, Train Loss: 0.04262172058224678, Test Loss: 0.08441267162561417\n",
      "Epoch 3482, Train Loss: 0.042606156319379807, Test Loss: 0.08439844101667404\n",
      "Epoch 3483, Train Loss: 0.04259049519896507, Test Loss: 0.08438841998577118\n",
      "Epoch 3484, Train Loss: 0.04257496818900108, Test Loss: 0.08437319099903107\n",
      "Epoch 3485, Train Loss: 0.04255944490432739, Test Loss: 0.08435865491628647\n",
      "Epoch 3486, Train Loss: 0.04254404827952385, Test Loss: 0.08434639126062393\n",
      "Epoch 3487, Train Loss: 0.04252873361110687, Test Loss: 0.08434180170297623\n",
      "Epoch 3488, Train Loss: 0.04251297935843468, Test Loss: 0.08434109389781952\n",
      "Epoch 3489, Train Loss: 0.042497456073760986, Test Loss: 0.0843241885304451\n",
      "Epoch 3490, Train Loss: 0.042482201009988785, Test Loss: 0.08430900424718857\n",
      "Epoch 3491, Train Loss: 0.042466819286346436, Test Loss: 0.08429257571697235\n",
      "Epoch 3492, Train Loss: 0.04245143383741379, Test Loss: 0.08428298681974411\n",
      "Epoch 3493, Train Loss: 0.04243563115596771, Test Loss: 0.08428177237510681\n",
      "Epoch 3494, Train Loss: 0.04242068529129028, Test Loss: 0.08428201824426651\n",
      "Epoch 3495, Train Loss: 0.0424051433801651, Test Loss: 0.08426667004823685\n",
      "Epoch 3496, Train Loss: 0.042389895766973495, Test Loss: 0.0842491015791893\n",
      "Epoch 3497, Train Loss: 0.04237488657236099, Test Loss: 0.0842258632183075\n",
      "Epoch 3498, Train Loss: 0.04235977679491043, Test Loss: 0.08421896398067474\n",
      "Epoch 3499, Train Loss: 0.04234464466571808, Test Loss: 0.08420627564191818\n",
      "Epoch 3500, Train Loss: 0.04232913255691528, Test Loss: 0.08420228958129883\n",
      "Epoch 3501, Train Loss: 0.04231377691030502, Test Loss: 0.08419539779424667\n",
      "Epoch 3502, Train Loss: 0.04229844734072685, Test Loss: 0.08418799191713333\n",
      "Epoch 3503, Train Loss: 0.04228345304727554, Test Loss: 0.08418465405702591\n",
      "Epoch 3504, Train Loss: 0.04226873070001602, Test Loss: 0.08415057510137558\n",
      "Epoch 3505, Train Loss: 0.042253151535987854, Test Loss: 0.08414873480796814\n",
      "Epoch 3506, Train Loss: 0.04223859682679176, Test Loss: 0.08415316045284271\n",
      "Epoch 3507, Train Loss: 0.04222296178340912, Test Loss: 0.08413766324520111\n",
      "Epoch 3508, Train Loss: 0.042208146303892136, Test Loss: 0.08413369953632355\n",
      "Epoch 3509, Train Loss: 0.04219275712966919, Test Loss: 0.08411858230829239\n",
      "Epoch 3510, Train Loss: 0.042178113013505936, Test Loss: 0.0841156467795372\n",
      "Epoch 3511, Train Loss: 0.04216316342353821, Test Loss: 0.0840897485613823\n",
      "Epoch 3512, Train Loss: 0.04214782267808914, Test Loss: 0.08408638834953308\n",
      "Epoch 3513, Train Loss: 0.04213513433933258, Test Loss: 0.08406662195920944\n",
      "Epoch 3514, Train Loss: 0.042117733508348465, Test Loss: 0.08404707908630371\n",
      "Epoch 3515, Train Loss: 0.04210268706083298, Test Loss: 0.08403990417718887\n",
      "Epoch 3516, Train Loss: 0.04208743944764137, Test Loss: 0.08403850346803665\n",
      "Epoch 3517, Train Loss: 0.04207242280244827, Test Loss: 0.08403179794549942\n",
      "Epoch 3518, Train Loss: 0.042058221995830536, Test Loss: 0.08400215208530426\n",
      "Epoch 3519, Train Loss: 0.04204285517334938, Test Loss: 0.08399271219968796\n",
      "Epoch 3520, Train Loss: 0.04202760383486748, Test Loss: 0.08399241417646408\n",
      "Epoch 3521, Train Loss: 0.04201310873031616, Test Loss: 0.08397307246923447\n",
      "Epoch 3522, Train Loss: 0.041997652500867844, Test Loss: 0.08396745473146439\n",
      "Epoch 3523, Train Loss: 0.04198310896754265, Test Loss: 0.0839587152004242\n",
      "Epoch 3524, Train Loss: 0.04196838662028313, Test Loss: 0.08394420146942139\n",
      "Epoch 3525, Train Loss: 0.04195386916399002, Test Loss: 0.08391661196947098\n",
      "Epoch 3526, Train Loss: 0.04193852096796036, Test Loss: 0.08391562104225159\n",
      "Epoch 3527, Train Loss: 0.041923653334379196, Test Loss: 0.08392372727394104\n",
      "Epoch 3528, Train Loss: 0.041908953338861465, Test Loss: 0.08391214907169342\n",
      "Epoch 3529, Train Loss: 0.041894376277923584, Test Loss: 0.08388982713222504\n",
      "Epoch 3530, Train Loss: 0.04188030585646629, Test Loss: 0.08388952165842056\n",
      "Epoch 3531, Train Loss: 0.04186496138572693, Test Loss: 0.08387356996536255\n",
      "Epoch 3532, Train Loss: 0.04185177758336067, Test Loss: 0.08384445309638977\n",
      "Epoch 3533, Train Loss: 0.041835684329271317, Test Loss: 0.08383473753929138\n",
      "Epoch 3534, Train Loss: 0.04182088002562523, Test Loss: 0.08383964747190475\n",
      "Epoch 3535, Train Loss: 0.0418064147233963, Test Loss: 0.0838213562965393\n",
      "Epoch 3536, Train Loss: 0.041791848838329315, Test Loss: 0.08380965143442154\n",
      "Epoch 3537, Train Loss: 0.04177749156951904, Test Loss: 0.0837976485490799\n",
      "Epoch 3538, Train Loss: 0.041762176901102066, Test Loss: 0.0838010385632515\n",
      "Epoch 3539, Train Loss: 0.04174792766571045, Test Loss: 0.08378994464874268\n",
      "Epoch 3540, Train Loss: 0.041733790189027786, Test Loss: 0.08376328647136688\n",
      "Epoch 3541, Train Loss: 0.04171837493777275, Test Loss: 0.08376545459032059\n",
      "Epoch 3542, Train Loss: 0.041704241186380386, Test Loss: 0.08374861627817154\n",
      "Epoch 3543, Train Loss: 0.04168923944234848, Test Loss: 0.0837419182062149\n",
      "Epoch 3544, Train Loss: 0.04167528450489044, Test Loss: 0.08373231440782547\n",
      "Epoch 3545, Train Loss: 0.04166049882769585, Test Loss: 0.08373056352138519\n",
      "Epoch 3546, Train Loss: 0.041646212339401245, Test Loss: 0.08370839804410934\n",
      "Epoch 3547, Train Loss: 0.04163118451833725, Test Loss: 0.08371055126190186\n",
      "Epoch 3548, Train Loss: 0.0416170097887516, Test Loss: 0.08371269702911377\n",
      "Epoch 3549, Train Loss: 0.04160251095890999, Test Loss: 0.0837014690041542\n",
      "Epoch 3550, Train Loss: 0.04158817604184151, Test Loss: 0.08368854969739914\n",
      "Epoch 3551, Train Loss: 0.04157437011599541, Test Loss: 0.08368279784917831\n",
      "Epoch 3552, Train Loss: 0.041559942066669464, Test Loss: 0.08367078751325607\n",
      "Epoch 3553, Train Loss: 0.041545044630765915, Test Loss: 0.08365968614816666\n",
      "Epoch 3554, Train Loss: 0.0415305532515049, Test Loss: 0.08364686369895935\n",
      "Epoch 3555, Train Loss: 0.0415165089070797, Test Loss: 0.08363504707813263\n",
      "Epoch 3556, Train Loss: 0.04150249809026718, Test Loss: 0.08361560106277466\n",
      "Epoch 3557, Train Loss: 0.04148787632584572, Test Loss: 0.08360040187835693\n",
      "Epoch 3558, Train Loss: 0.0414731465280056, Test Loss: 0.08359964191913605\n",
      "Epoch 3559, Train Loss: 0.04145939648151398, Test Loss: 0.0836099237203598\n",
      "Epoch 3560, Train Loss: 0.041444674134254456, Test Loss: 0.08358088880777359\n",
      "Epoch 3561, Train Loss: 0.041430648416280746, Test Loss: 0.08357373625040054\n",
      "Epoch 3562, Train Loss: 0.04141673818230629, Test Loss: 0.08355344831943512\n",
      "Epoch 3563, Train Loss: 0.041401833295822144, Test Loss: 0.08354837447404861\n",
      "Epoch 3564, Train Loss: 0.04138806089758873, Test Loss: 0.083541139960289\n",
      "Epoch 3565, Train Loss: 0.04137371852993965, Test Loss: 0.08352959156036377\n",
      "Epoch 3566, Train Loss: 0.04135958477854729, Test Loss: 0.0835212841629982\n",
      "Epoch 3567, Train Loss: 0.04134554788470268, Test Loss: 0.08350738883018494\n",
      "Epoch 3568, Train Loss: 0.04133129492402077, Test Loss: 0.08349551260471344\n",
      "Epoch 3569, Train Loss: 0.04131755977869034, Test Loss: 0.08348900079727173\n",
      "Epoch 3570, Train Loss: 0.041302938014268875, Test Loss: 0.08347982913255692\n",
      "Epoch 3571, Train Loss: 0.041288942098617554, Test Loss: 0.08346909284591675\n",
      "Epoch 3572, Train Loss: 0.04127543047070503, Test Loss: 0.08344566822052002\n",
      "Epoch 3573, Train Loss: 0.041261713951826096, Test Loss: 0.08344171941280365\n",
      "Epoch 3574, Train Loss: 0.041246939450502396, Test Loss: 0.08343323320150375\n",
      "Epoch 3575, Train Loss: 0.04123306646943092, Test Loss: 0.08341962099075317\n",
      "Epoch 3576, Train Loss: 0.041218966245651245, Test Loss: 0.08342026174068451\n",
      "Epoch 3577, Train Loss: 0.04120451584458351, Test Loss: 0.08340481668710709\n",
      "Epoch 3578, Train Loss: 0.041190844029188156, Test Loss: 0.08339837938547134\n",
      "Epoch 3579, Train Loss: 0.04117697477340698, Test Loss: 0.08337961137294769\n",
      "Epoch 3580, Train Loss: 0.04116304591298103, Test Loss: 0.08336248993873596\n",
      "Epoch 3581, Train Loss: 0.041149839758872986, Test Loss: 0.08334613591432571\n",
      "Epoch 3582, Train Loss: 0.041135676205158234, Test Loss: 0.08334261178970337\n",
      "Epoch 3583, Train Loss: 0.04112160578370094, Test Loss: 0.08333241939544678\n",
      "Epoch 3584, Train Loss: 0.04110751673579216, Test Loss: 0.08332736790180206\n",
      "Epoch 3585, Train Loss: 0.04109330102801323, Test Loss: 0.08333099633455276\n",
      "Epoch 3586, Train Loss: 0.041079409420490265, Test Loss: 0.08331825584173203\n",
      "Epoch 3587, Train Loss: 0.04106542468070984, Test Loss: 0.08331958204507828\n",
      "Epoch 3588, Train Loss: 0.04105180501937866, Test Loss: 0.08330464363098145\n",
      "Epoch 3589, Train Loss: 0.041037868708372116, Test Loss: 0.0832986831665039\n",
      "Epoch 3590, Train Loss: 0.04102397337555885, Test Loss: 0.08328469842672348\n",
      "Epoch 3591, Train Loss: 0.04101022332906723, Test Loss: 0.08327729254961014\n",
      "Epoch 3592, Train Loss: 0.0409964993596077, Test Loss: 0.08326873928308487\n",
      "Epoch 3593, Train Loss: 0.040982894599437714, Test Loss: 0.08326025307178497\n",
      "Epoch 3594, Train Loss: 0.040969401597976685, Test Loss: 0.08325323462486267\n",
      "Epoch 3595, Train Loss: 0.04095581918954849, Test Loss: 0.08324536681175232\n",
      "Epoch 3596, Train Loss: 0.040941979736089706, Test Loss: 0.08322340250015259\n",
      "Epoch 3597, Train Loss: 0.0409281924366951, Test Loss: 0.08320476114749908\n",
      "Epoch 3598, Train Loss: 0.040914781391620636, Test Loss: 0.08318660408258438\n",
      "Epoch 3599, Train Loss: 0.040900830179452896, Test Loss: 0.08320139348506927\n",
      "Epoch 3600, Train Loss: 0.0408872626721859, Test Loss: 0.08317810297012329\n",
      "Epoch 3601, Train Loss: 0.04087381809949875, Test Loss: 0.08317401260137558\n",
      "Epoch 3602, Train Loss: 0.04085983335971832, Test Loss: 0.08317148685455322\n",
      "Epoch 3603, Train Loss: 0.040846969932317734, Test Loss: 0.08316359668970108\n",
      "Epoch 3604, Train Loss: 0.04083336889743805, Test Loss: 0.08315157145261765\n",
      "Epoch 3605, Train Loss: 0.04081987962126732, Test Loss: 0.08312517404556274\n",
      "Epoch 3606, Train Loss: 0.04080634564161301, Test Loss: 0.08311747014522552\n",
      "Epoch 3607, Train Loss: 0.04079233855009079, Test Loss: 0.08310570567846298\n",
      "Epoch 3608, Train Loss: 0.04077897220849991, Test Loss: 0.08308926969766617\n",
      "Epoch 3609, Train Loss: 0.040765486657619476, Test Loss: 0.08308382332324982\n",
      "Epoch 3610, Train Loss: 0.04075213149189949, Test Loss: 0.08308247476816177\n",
      "Epoch 3611, Train Loss: 0.0407392755150795, Test Loss: 0.08307798951864243\n",
      "Epoch 3612, Train Loss: 0.04072507470846176, Test Loss: 0.08306584507226944\n",
      "Epoch 3613, Train Loss: 0.040711384266614914, Test Loss: 0.08306822180747986\n",
      "Epoch 3614, Train Loss: 0.04069838300347328, Test Loss: 0.0830460861325264\n",
      "Epoch 3615, Train Loss: 0.04068462550640106, Test Loss: 0.08303192257881165\n",
      "Epoch 3616, Train Loss: 0.040671177208423615, Test Loss: 0.08303044736385345\n",
      "Epoch 3617, Train Loss: 0.040657930076122284, Test Loss: 0.08302110433578491\n",
      "Epoch 3618, Train Loss: 0.04064416140317917, Test Loss: 0.0830160602927208\n",
      "Epoch 3619, Train Loss: 0.0406307615339756, Test Loss: 0.08301704376935959\n",
      "Epoch 3620, Train Loss: 0.04061717912554741, Test Loss: 0.08301085978746414\n",
      "Epoch 3621, Train Loss: 0.04060396924614906, Test Loss: 0.08299757540225983\n",
      "Epoch 3622, Train Loss: 0.040590524673461914, Test Loss: 0.08298569172620773\n",
      "Epoch 3623, Train Loss: 0.04057731106877327, Test Loss: 0.08297673612833023\n",
      "Epoch 3624, Train Loss: 0.04056454822421074, Test Loss: 0.08296035975217819\n",
      "Epoch 3625, Train Loss: 0.0405510812997818, Test Loss: 0.08294554799795151\n",
      "Epoch 3626, Train Loss: 0.04053765907883644, Test Loss: 0.08293838798999786\n",
      "Epoch 3627, Train Loss: 0.040524501353502274, Test Loss: 0.08292757719755173\n",
      "Epoch 3628, Train Loss: 0.0405108816921711, Test Loss: 0.08293253183364868\n",
      "Epoch 3629, Train Loss: 0.04049736261367798, Test Loss: 0.08292923122644424\n",
      "Epoch 3630, Train Loss: 0.040484294295310974, Test Loss: 0.08292166888713837\n",
      "Epoch 3631, Train Loss: 0.04047062247991562, Test Loss: 0.08289837092161179\n",
      "Epoch 3632, Train Loss: 0.04045777767896652, Test Loss: 0.08289796113967896\n",
      "Epoch 3633, Train Loss: 0.04044483229517937, Test Loss: 0.08289442211389542\n",
      "Epoch 3634, Train Loss: 0.04043096676468849, Test Loss: 0.08288992941379547\n",
      "Epoch 3635, Train Loss: 0.04041785001754761, Test Loss: 0.08287321776151657\n",
      "Epoch 3636, Train Loss: 0.04040522500872612, Test Loss: 0.08285053074359894\n",
      "Epoch 3637, Train Loss: 0.04039132967591286, Test Loss: 0.08285222202539444\n",
      "Epoch 3638, Train Loss: 0.04037851840257645, Test Loss: 0.08283242583274841\n",
      "Epoch 3639, Train Loss: 0.04036533460021019, Test Loss: 0.08281976729631424\n",
      "Epoch 3640, Train Loss: 0.040352046489715576, Test Loss: 0.08281756937503815\n",
      "Epoch 3641, Train Loss: 0.04033968597650528, Test Loss: 0.08281486481428146\n",
      "Epoch 3642, Train Loss: 0.040326036512851715, Test Loss: 0.0828031674027443\n",
      "Epoch 3643, Train Loss: 0.040312737226486206, Test Loss: 0.08278453350067139\n",
      "Epoch 3644, Train Loss: 0.040300581604242325, Test Loss: 0.08276329189538956\n",
      "Epoch 3645, Train Loss: 0.040287084877491, Test Loss: 0.08276418596506119\n",
      "Epoch 3646, Train Loss: 0.040273599326610565, Test Loss: 0.08275491744279861\n",
      "Epoch 3647, Train Loss: 0.04026041552424431, Test Loss: 0.08275222778320312\n",
      "Epoch 3648, Train Loss: 0.040247637778520584, Test Loss: 0.08274799585342407\n",
      "Epoch 3649, Train Loss: 0.04023447260260582, Test Loss: 0.08273351192474365\n",
      "Epoch 3650, Train Loss: 0.04022189602255821, Test Loss: 0.0827275961637497\n",
      "Epoch 3651, Train Loss: 0.04020845890045166, Test Loss: 0.08273027092218399\n",
      "Epoch 3652, Train Loss: 0.040195319801568985, Test Loss: 0.08271915465593338\n",
      "Epoch 3653, Train Loss: 0.04018281400203705, Test Loss: 0.08270490169525146\n",
      "Epoch 3654, Train Loss: 0.04016963392496109, Test Loss: 0.08270775526762009\n",
      "Epoch 3655, Train Loss: 0.040156397968530655, Test Loss: 0.08270557224750519\n",
      "Epoch 3656, Train Loss: 0.04014335200190544, Test Loss: 0.08268643915653229\n",
      "Epoch 3657, Train Loss: 0.04013054072856903, Test Loss: 0.08267821371555328\n",
      "Epoch 3658, Train Loss: 0.04011774808168411, Test Loss: 0.08267176896333694\n",
      "Epoch 3659, Train Loss: 0.040105730295181274, Test Loss: 0.08264859765768051\n",
      "Epoch 3660, Train Loss: 0.04009219631552696, Test Loss: 0.0826486274600029\n",
      "Epoch 3661, Train Loss: 0.04008004441857338, Test Loss: 0.0826226994395256\n",
      "Epoch 3662, Train Loss: 0.04006638750433922, Test Loss: 0.08262674510478973\n",
      "Epoch 3663, Train Loss: 0.04005352035164833, Test Loss: 0.08262425661087036\n",
      "Epoch 3664, Train Loss: 0.040040481835603714, Test Loss: 0.08262275159358978\n",
      "Epoch 3665, Train Loss: 0.04002762958407402, Test Loss: 0.08260084688663483\n",
      "Epoch 3666, Train Loss: 0.040015239268541336, Test Loss: 0.08259648829698563\n",
      "Epoch 3667, Train Loss: 0.040002141147851944, Test Loss: 0.08257856965065002\n",
      "Epoch 3668, Train Loss: 0.039990030229091644, Test Loss: 0.0825619176030159\n",
      "Epoch 3669, Train Loss: 0.03997664898633957, Test Loss: 0.08256653696298599\n",
      "Epoch 3670, Train Loss: 0.03996497392654419, Test Loss: 0.08255766332149506\n",
      "Epoch 3671, Train Loss: 0.0399513877928257, Test Loss: 0.08254840224981308\n",
      "Epoch 3672, Train Loss: 0.039938293397426605, Test Loss: 0.08253699541091919\n",
      "Epoch 3673, Train Loss: 0.039925362914800644, Test Loss: 0.08253200352191925\n",
      "Epoch 3674, Train Loss: 0.039912719279527664, Test Loss: 0.08253205567598343\n",
      "Epoch 3675, Train Loss: 0.03990001976490021, Test Loss: 0.08253567665815353\n",
      "Epoch 3676, Train Loss: 0.03988729789853096, Test Loss: 0.0825124979019165\n",
      "Epoch 3677, Train Loss: 0.039874497801065445, Test Loss: 0.08251334726810455\n",
      "Epoch 3678, Train Loss: 0.03986217826604843, Test Loss: 0.08249487727880478\n",
      "Epoch 3679, Train Loss: 0.039849862456321716, Test Loss: 0.08249259740114212\n",
      "Epoch 3680, Train Loss: 0.039837226271629333, Test Loss: 0.08247578889131546\n",
      "Epoch 3681, Train Loss: 0.03982377424836159, Test Loss: 0.08246922492980957\n",
      "Epoch 3682, Train Loss: 0.03981117904186249, Test Loss: 0.08247114717960358\n",
      "Epoch 3683, Train Loss: 0.0397985465824604, Test Loss: 0.0824713334441185\n",
      "Epoch 3684, Train Loss: 0.03978583589196205, Test Loss: 0.08246156573295593\n",
      "Epoch 3685, Train Loss: 0.03977319598197937, Test Loss: 0.08244939148426056\n",
      "Epoch 3686, Train Loss: 0.03976091369986534, Test Loss: 0.08243875205516815\n",
      "Epoch 3687, Train Loss: 0.03974802419543266, Test Loss: 0.08244340866804123\n",
      "Epoch 3688, Train Loss: 0.039735276252031326, Test Loss: 0.08242650330066681\n",
      "Epoch 3689, Train Loss: 0.03972311317920685, Test Loss: 0.08241333812475204\n",
      "Epoch 3690, Train Loss: 0.03971060365438461, Test Loss: 0.08240743726491928\n",
      "Epoch 3691, Train Loss: 0.03969791531562805, Test Loss: 0.08239006996154785\n",
      "Epoch 3692, Train Loss: 0.039685580879449844, Test Loss: 0.08238237351179123\n",
      "Epoch 3693, Train Loss: 0.039672836661338806, Test Loss: 0.08237361907958984\n",
      "Epoch 3694, Train Loss: 0.03966023400425911, Test Loss: 0.08237235993146896\n",
      "Epoch 3695, Train Loss: 0.03964769095182419, Test Loss: 0.08235910534858704\n",
      "Epoch 3696, Train Loss: 0.039635058492422104, Test Loss: 0.08235315978527069\n",
      "Epoch 3697, Train Loss: 0.03962266817688942, Test Loss: 0.08234783262014389\n",
      "Epoch 3698, Train Loss: 0.039610330015420914, Test Loss: 0.08233064413070679\n",
      "Epoch 3699, Train Loss: 0.03959774225950241, Test Loss: 0.08232542872428894\n",
      "Epoch 3700, Train Loss: 0.039585549384355545, Test Loss: 0.0823163315653801\n",
      "Epoch 3701, Train Loss: 0.03957279399037361, Test Loss: 0.08231928199529648\n",
      "Epoch 3702, Train Loss: 0.03956097736954689, Test Loss: 0.08231896907091141\n",
      "Epoch 3703, Train Loss: 0.03954843431711197, Test Loss: 0.08229977637529373\n",
      "Epoch 3704, Train Loss: 0.0395355261862278, Test Loss: 0.0822916179895401\n",
      "Epoch 3705, Train Loss: 0.03952328860759735, Test Loss: 0.08227001130580902\n",
      "Epoch 3706, Train Loss: 0.039511144161224365, Test Loss: 0.0822615996003151\n",
      "Epoch 3707, Train Loss: 0.039498865604400635, Test Loss: 0.08225387334823608\n",
      "Epoch 3708, Train Loss: 0.039486076682806015, Test Loss: 0.08224993944168091\n",
      "Epoch 3709, Train Loss: 0.039474908262491226, Test Loss: 0.08223830163478851\n",
      "Epoch 3710, Train Loss: 0.03946197032928467, Test Loss: 0.08223523199558258\n",
      "Epoch 3711, Train Loss: 0.03944900259375572, Test Loss: 0.08223126083612442\n",
      "Epoch 3712, Train Loss: 0.03943802788853645, Test Loss: 0.08222609013319016\n",
      "Epoch 3713, Train Loss: 0.03942444548010826, Test Loss: 0.08221862465143204\n",
      "Epoch 3714, Train Loss: 0.039412643760442734, Test Loss: 0.08219973742961884\n",
      "Epoch 3715, Train Loss: 0.03939972072839737, Test Loss: 0.08219888061285019\n",
      "Epoch 3716, Train Loss: 0.039387527853250504, Test Loss: 0.0821928083896637\n",
      "Epoch 3717, Train Loss: 0.039375726133584976, Test Loss: 0.08218017220497131\n",
      "Epoch 3718, Train Loss: 0.03936335816979408, Test Loss: 0.08217597007751465\n",
      "Epoch 3719, Train Loss: 0.039351001381874084, Test Loss: 0.08216850459575653\n",
      "Epoch 3720, Train Loss: 0.039338577538728714, Test Loss: 0.0821649506688118\n",
      "Epoch 3721, Train Loss: 0.03932672366499901, Test Loss: 0.08215944468975067\n",
      "Epoch 3722, Train Loss: 0.039314307272434235, Test Loss: 0.08215388655662537\n",
      "Epoch 3723, Train Loss: 0.03930291533470154, Test Loss: 0.08212660998106003\n",
      "Epoch 3724, Train Loss: 0.03929024934768677, Test Loss: 0.08211696147918701\n",
      "Epoch 3725, Train Loss: 0.03927754983305931, Test Loss: 0.08211196959018707\n",
      "Epoch 3726, Train Loss: 0.03926556184887886, Test Loss: 0.0821228101849556\n",
      "Epoch 3727, Train Loss: 0.03925339877605438, Test Loss: 0.08210082352161407\n",
      "Epoch 3728, Train Loss: 0.03924156352877617, Test Loss: 0.0820993036031723\n",
      "Epoch 3729, Train Loss: 0.039229027926921844, Test Loss: 0.08208545297384262\n",
      "Epoch 3730, Train Loss: 0.03921767696738243, Test Loss: 0.08206295967102051\n",
      "Epoch 3731, Train Loss: 0.03920536860823631, Test Loss: 0.08207112550735474\n",
      "Epoch 3732, Train Loss: 0.039192862808704376, Test Loss: 0.08206264674663544\n",
      "Epoch 3733, Train Loss: 0.039180636405944824, Test Loss: 0.08205942064523697\n",
      "Epoch 3734, Train Loss: 0.039168909192085266, Test Loss: 0.08206138014793396\n",
      "Epoch 3735, Train Loss: 0.039156634360551834, Test Loss: 0.08205175399780273\n",
      "Epoch 3736, Train Loss: 0.03914453461766243, Test Loss: 0.08204704523086548\n",
      "Epoch 3737, Train Loss: 0.03913257643580437, Test Loss: 0.08203528076410294\n",
      "Epoch 3738, Train Loss: 0.03912074491381645, Test Loss: 0.0820263996720314\n",
      "Epoch 3739, Train Loss: 0.03910854458808899, Test Loss: 0.0820126160979271\n",
      "Epoch 3740, Train Loss: 0.03909648209810257, Test Loss: 0.08201562613248825\n",
      "Epoch 3741, Train Loss: 0.039084818214178085, Test Loss: 0.0819995105266571\n",
      "Epoch 3742, Train Loss: 0.03907385468482971, Test Loss: 0.08198445290327072\n",
      "Epoch 3743, Train Loss: 0.03906054049730301, Test Loss: 0.08197756856679916\n",
      "Epoch 3744, Train Loss: 0.03904877230525017, Test Loss: 0.0819697305560112\n",
      "Epoch 3745, Train Loss: 0.039037179201841354, Test Loss: 0.08195514231920242\n",
      "Epoch 3746, Train Loss: 0.03902485594153404, Test Loss: 0.08194687217473984\n",
      "Epoch 3747, Train Loss: 0.03901251032948494, Test Loss: 0.08194941282272339\n",
      "Epoch 3748, Train Loss: 0.03900105133652687, Test Loss: 0.08193256705999374\n",
      "Epoch 3749, Train Loss: 0.038989145308732986, Test Loss: 0.08192635327577591\n",
      "Epoch 3750, Train Loss: 0.03897674381732941, Test Loss: 0.0819355696439743\n",
      "Epoch 3751, Train Loss: 0.03896467387676239, Test Loss: 0.08193439245223999\n",
      "Epoch 3752, Train Loss: 0.03895287960767746, Test Loss: 0.08193227648735046\n",
      "Epoch 3753, Train Loss: 0.03894087299704552, Test Loss: 0.08192668855190277\n",
      "Epoch 3754, Train Loss: 0.038929421454668045, Test Loss: 0.08190273493528366\n",
      "Epoch 3755, Train Loss: 0.038917116820812225, Test Loss: 0.0819036141037941\n",
      "Epoch 3756, Train Loss: 0.03890550509095192, Test Loss: 0.08188937604427338\n",
      "Epoch 3757, Train Loss: 0.03889348730444908, Test Loss: 0.08189177513122559\n",
      "Epoch 3758, Train Loss: 0.03888169676065445, Test Loss: 0.08188217133283615\n",
      "Epoch 3759, Train Loss: 0.03887036442756653, Test Loss: 0.08186332136392593\n",
      "Epoch 3760, Train Loss: 0.038858238607645035, Test Loss: 0.08185788989067078\n",
      "Epoch 3761, Train Loss: 0.03884607180953026, Test Loss: 0.08185611665248871\n",
      "Epoch 3762, Train Loss: 0.038834359496831894, Test Loss: 0.08185551315546036\n",
      "Epoch 3763, Train Loss: 0.0388224795460701, Test Loss: 0.08185117691755295\n",
      "Epoch 3764, Train Loss: 0.03881106898188591, Test Loss: 0.0818355455994606\n",
      "Epoch 3765, Train Loss: 0.03879949077963829, Test Loss: 0.08181776106357574\n",
      "Epoch 3766, Train Loss: 0.038787852972745895, Test Loss: 0.08181916922330856\n",
      "Epoch 3767, Train Loss: 0.038775574415922165, Test Loss: 0.08180540800094604\n",
      "Epoch 3768, Train Loss: 0.03876372054219246, Test Loss: 0.08181066066026688\n",
      "Epoch 3769, Train Loss: 0.03875176981091499, Test Loss: 0.08180626481771469\n",
      "Epoch 3770, Train Loss: 0.03873992711305618, Test Loss: 0.08179550617933273\n",
      "Epoch 3771, Train Loss: 0.03872865438461304, Test Loss: 0.08177394419908524\n",
      "Epoch 3772, Train Loss: 0.03871706500649452, Test Loss: 0.08176187425851822\n",
      "Epoch 3773, Train Loss: 0.03870498761534691, Test Loss: 0.08175864070653915\n",
      "Epoch 3774, Train Loss: 0.03869355469942093, Test Loss: 0.08175349235534668\n",
      "Epoch 3775, Train Loss: 0.0386824831366539, Test Loss: 0.08174420893192291\n",
      "Epoch 3776, Train Loss: 0.03867003321647644, Test Loss: 0.08174698799848557\n",
      "Epoch 3777, Train Loss: 0.03865800052881241, Test Loss: 0.08173756301403046\n",
      "Epoch 3778, Train Loss: 0.038646355271339417, Test Loss: 0.08172827214002609\n",
      "Epoch 3779, Train Loss: 0.03863535448908806, Test Loss: 0.08171433955430984\n",
      "Epoch 3780, Train Loss: 0.03862316533923149, Test Loss: 0.08171133697032928\n",
      "Epoch 3781, Train Loss: 0.038611773401498795, Test Loss: 0.08170671761035919\n",
      "Epoch 3782, Train Loss: 0.038599979132413864, Test Loss: 0.08169303834438324\n",
      "Epoch 3783, Train Loss: 0.03858815133571625, Test Loss: 0.08168630301952362\n",
      "Epoch 3784, Train Loss: 0.038576677441596985, Test Loss: 0.08168841153383255\n",
      "Epoch 3785, Train Loss: 0.038564931601285934, Test Loss: 0.08167533576488495\n",
      "Epoch 3786, Train Loss: 0.03855327516794205, Test Loss: 0.08167658001184464\n",
      "Epoch 3787, Train Loss: 0.03854198753833771, Test Loss: 0.08167634159326553\n",
      "Epoch 3788, Train Loss: 0.0385303758084774, Test Loss: 0.08166997134685516\n",
      "Epoch 3789, Train Loss: 0.0385185182094574, Test Loss: 0.08165466785430908\n",
      "Epoch 3790, Train Loss: 0.03850685432553291, Test Loss: 0.08165572583675385\n",
      "Epoch 3791, Train Loss: 0.03849554806947708, Test Loss: 0.08165525645017624\n",
      "Epoch 3792, Train Loss: 0.03848399966955185, Test Loss: 0.08164364099502563\n",
      "Epoch 3793, Train Loss: 0.038472145795822144, Test Loss: 0.08164505660533905\n",
      "Epoch 3794, Train Loss: 0.03846081718802452, Test Loss: 0.08162786066532135\n",
      "Epoch 3795, Train Loss: 0.03844957798719406, Test Loss: 0.08161525428295135\n",
      "Epoch 3796, Train Loss: 0.038437798619270325, Test Loss: 0.08161336183547974\n",
      "Epoch 3797, Train Loss: 0.038426272571086884, Test Loss: 0.08160610496997833\n",
      "Epoch 3798, Train Loss: 0.038414690643548965, Test Loss: 0.08160024136304855\n",
      "Epoch 3799, Train Loss: 0.038403045386075974, Test Loss: 0.08158604800701141\n",
      "Epoch 3800, Train Loss: 0.038391679525375366, Test Loss: 0.08158084750175476\n",
      "Epoch 3801, Train Loss: 0.0383809357881546, Test Loss: 0.0815768837928772\n",
      "Epoch 3802, Train Loss: 0.03836899995803833, Test Loss: 0.08155800402164459\n",
      "Epoch 3803, Train Loss: 0.03835732489824295, Test Loss: 0.08155294507741928\n",
      "Epoch 3804, Train Loss: 0.03834562748670578, Test Loss: 0.08155647665262222\n",
      "Epoch 3805, Train Loss: 0.03833431005477905, Test Loss: 0.08155032247304916\n",
      "Epoch 3806, Train Loss: 0.038322947919368744, Test Loss: 0.08154098689556122\n",
      "Epoch 3807, Train Loss: 0.03831174597144127, Test Loss: 0.08154049515724182\n",
      "Epoch 3808, Train Loss: 0.038300298154354095, Test Loss: 0.08153848350048065\n",
      "Epoch 3809, Train Loss: 0.03828887268900871, Test Loss: 0.08151831477880478\n",
      "Epoch 3810, Train Loss: 0.038277413696050644, Test Loss: 0.08151120692491531\n",
      "Epoch 3811, Train Loss: 0.038266006857156754, Test Loss: 0.08151248842477798\n",
      "Epoch 3812, Train Loss: 0.03825581818819046, Test Loss: 0.08149231970310211\n",
      "Epoch 3813, Train Loss: 0.03824325650930405, Test Loss: 0.08149164170026779\n",
      "Epoch 3814, Train Loss: 0.03823214769363403, Test Loss: 0.081476591527462\n",
      "Epoch 3815, Train Loss: 0.03822052851319313, Test Loss: 0.08148407936096191\n",
      "Epoch 3816, Train Loss: 0.038209594786167145, Test Loss: 0.08147400617599487\n",
      "Epoch 3817, Train Loss: 0.03819770738482475, Test Loss: 0.08148312568664551\n",
      "Epoch 3818, Train Loss: 0.038186363875865936, Test Loss: 0.08145648241043091\n",
      "Epoch 3819, Train Loss: 0.03817509487271309, Test Loss: 0.08145654946565628\n",
      "Epoch 3820, Train Loss: 0.038163911551237106, Test Loss: 0.08144352585077286\n",
      "Epoch 3821, Train Loss: 0.03815286606550217, Test Loss: 0.08144105970859528\n",
      "Epoch 3822, Train Loss: 0.038142893463373184, Test Loss: 0.08143008500337601\n",
      "Epoch 3823, Train Loss: 0.038130056113004684, Test Loss: 0.08143245428800583\n",
      "Epoch 3824, Train Loss: 0.0381191112101078, Test Loss: 0.08141203969717026\n",
      "Epoch 3825, Train Loss: 0.03810776397585869, Test Loss: 0.08140788972377777\n",
      "Epoch 3826, Train Loss: 0.038097020238637924, Test Loss: 0.08140337467193604\n",
      "Epoch 3827, Train Loss: 0.038085028529167175, Test Loss: 0.08139418065547943\n",
      "Epoch 3828, Train Loss: 0.03807391971349716, Test Loss: 0.08139194548130035\n",
      "Epoch 3829, Train Loss: 0.03806237503886223, Test Loss: 0.0813809186220169\n",
      "Epoch 3830, Train Loss: 0.038051407784223557, Test Loss: 0.08136994391679764\n",
      "Epoch 3831, Train Loss: 0.03803979232907295, Test Loss: 0.08136465400457382\n",
      "Epoch 3832, Train Loss: 0.03802860155701637, Test Loss: 0.08136036992073059\n",
      "Epoch 3833, Train Loss: 0.038017261773347855, Test Loss: 0.08135678619146347\n",
      "Epoch 3834, Train Loss: 0.03800603747367859, Test Loss: 0.08135685324668884\n",
      "Epoch 3835, Train Loss: 0.037995029240846634, Test Loss: 0.08134260773658752\n",
      "Epoch 3836, Train Loss: 0.03798433393239975, Test Loss: 0.0813392698764801\n",
      "Epoch 3837, Train Loss: 0.03797281160950661, Test Loss: 0.08132684230804443\n",
      "Epoch 3838, Train Loss: 0.037961624562740326, Test Loss: 0.08133590966463089\n",
      "Epoch 3839, Train Loss: 0.03795012831687927, Test Loss: 0.08132436126470566\n",
      "Epoch 3840, Train Loss: 0.03793926537036896, Test Loss: 0.08130337297916412\n",
      "Epoch 3841, Train Loss: 0.03792894259095192, Test Loss: 0.08129137754440308\n",
      "Epoch 3842, Train Loss: 0.03791728988289833, Test Loss: 0.08128474652767181\n",
      "Epoch 3843, Train Loss: 0.03790567070245743, Test Loss: 0.08129178732633591\n",
      "Epoch 3844, Train Loss: 0.037894539535045624, Test Loss: 0.08128432184457779\n",
      "Epoch 3845, Train Loss: 0.03788399323821068, Test Loss: 0.0812695175409317\n",
      "Epoch 3846, Train Loss: 0.037872493267059326, Test Loss: 0.0812741219997406\n",
      "Epoch 3847, Train Loss: 0.0378619022667408, Test Loss: 0.08126416057348251\n",
      "Epoch 3848, Train Loss: 0.03785092011094093, Test Loss: 0.0812552347779274\n",
      "Epoch 3849, Train Loss: 0.037839386612176895, Test Loss: 0.0812678411602974\n",
      "Epoch 3850, Train Loss: 0.037829235196113586, Test Loss: 0.08125566691160202\n",
      "Epoch 3851, Train Loss: 0.03781764581799507, Test Loss: 0.08124148100614548\n",
      "Epoch 3852, Train Loss: 0.03780636191368103, Test Loss: 0.0812310203909874\n",
      "Epoch 3853, Train Loss: 0.03779551014304161, Test Loss: 0.08121548593044281\n",
      "Epoch 3854, Train Loss: 0.037783943116664886, Test Loss: 0.08122041821479797\n",
      "Epoch 3855, Train Loss: 0.0377732552587986, Test Loss: 0.08120530098676682\n",
      "Epoch 3856, Train Loss: 0.037762269377708435, Test Loss: 0.08120287209749222\n",
      "Epoch 3857, Train Loss: 0.03775116056203842, Test Loss: 0.08119672536849976\n",
      "Epoch 3858, Train Loss: 0.03773997351527214, Test Loss: 0.08119232207536697\n",
      "Epoch 3859, Train Loss: 0.03772931173443794, Test Loss: 0.08118113875389099\n",
      "Epoch 3860, Train Loss: 0.037718210369348526, Test Loss: 0.08118116110563278\n",
      "Epoch 3861, Train Loss: 0.037707794457674026, Test Loss: 0.08116405457258224\n",
      "Epoch 3862, Train Loss: 0.03769708797335625, Test Loss: 0.0811513140797615\n",
      "Epoch 3863, Train Loss: 0.03768560290336609, Test Loss: 0.08115848153829575\n",
      "Epoch 3864, Train Loss: 0.03767452761530876, Test Loss: 0.08116693049669266\n",
      "Epoch 3865, Train Loss: 0.037663597613573074, Test Loss: 0.0811581090092659\n",
      "Epoch 3866, Train Loss: 0.037653323262929916, Test Loss: 0.08114001154899597\n",
      "Epoch 3867, Train Loss: 0.03764234483242035, Test Loss: 0.08113455027341843\n",
      "Epoch 3868, Train Loss: 0.03763163089752197, Test Loss: 0.08112065494060516\n",
      "Epoch 3869, Train Loss: 0.03762064501643181, Test Loss: 0.08112110197544098\n",
      "Epoch 3870, Train Loss: 0.0376107431948185, Test Loss: 0.08109070360660553\n",
      "Epoch 3871, Train Loss: 0.037599045783281326, Test Loss: 0.08110108971595764\n",
      "Epoch 3872, Train Loss: 0.037588343024253845, Test Loss: 0.0810888260602951\n",
      "Epoch 3873, Train Loss: 0.03757842257618904, Test Loss: 0.0810730904340744\n",
      "Epoch 3874, Train Loss: 0.03756650537252426, Test Loss: 0.08108744025230408\n",
      "Epoch 3875, Train Loss: 0.03755585476756096, Test Loss: 0.08108481019735336\n",
      "Epoch 3876, Train Loss: 0.03754498437047005, Test Loss: 0.08107069879770279\n",
      "Epoch 3877, Train Loss: 0.03753502666950226, Test Loss: 0.08105617016553879\n",
      "Epoch 3878, Train Loss: 0.03752436861395836, Test Loss: 0.08104154467582703\n",
      "Epoch 3879, Train Loss: 0.037512850016355515, Test Loss: 0.08104784041643143\n",
      "Epoch 3880, Train Loss: 0.03750214725732803, Test Loss: 0.08105367422103882\n",
      "Epoch 3881, Train Loss: 0.037491656839847565, Test Loss: 0.08104051649570465\n",
      "Epoch 3882, Train Loss: 0.03748108074069023, Test Loss: 0.08102700859308243\n",
      "Epoch 3883, Train Loss: 0.03747030720114708, Test Loss: 0.0810253694653511\n",
      "Epoch 3884, Train Loss: 0.03745923936367035, Test Loss: 0.08102927356958389\n",
      "Epoch 3885, Train Loss: 0.0374487042427063, Test Loss: 0.08102564513683319\n",
      "Epoch 3886, Train Loss: 0.037437934428453445, Test Loss: 0.0810156911611557\n",
      "Epoch 3887, Train Loss: 0.037427887320518494, Test Loss: 0.0809965431690216\n",
      "Epoch 3888, Train Loss: 0.03741639107465744, Test Loss: 0.08100461959838867\n",
      "Epoch 3889, Train Loss: 0.03740580752491951, Test Loss: 0.0810057744383812\n",
      "Epoch 3890, Train Loss: 0.037395309656858444, Test Loss: 0.08100072294473648\n",
      "Epoch 3891, Train Loss: 0.037384387105703354, Test Loss: 0.08100134879350662\n",
      "Epoch 3892, Train Loss: 0.03737376257777214, Test Loss: 0.08098884671926498\n",
      "Epoch 3893, Train Loss: 0.03736317530274391, Test Loss: 0.08097835630178452\n",
      "Epoch 3894, Train Loss: 0.037352923303842545, Test Loss: 0.08095483481884003\n",
      "Epoch 3895, Train Loss: 0.037342432886362076, Test Loss: 0.08094832301139832\n",
      "Epoch 3896, Train Loss: 0.037331271916627884, Test Loss: 0.08096197992563248\n",
      "Epoch 3897, Train Loss: 0.03732210025191307, Test Loss: 0.08095969259738922\n",
      "Epoch 3898, Train Loss: 0.037310320883989334, Test Loss: 0.08094897866249084\n",
      "Epoch 3899, Train Loss: 0.03729965537786484, Test Loss: 0.08094266057014465\n",
      "Epoch 3900, Train Loss: 0.037289056926965714, Test Loss: 0.08093629777431488\n",
      "Epoch 3901, Train Loss: 0.0372786819934845, Test Loss: 0.08092083781957626\n",
      "Epoch 3902, Train Loss: 0.03726818412542343, Test Loss: 0.08091641217470169\n",
      "Epoch 3903, Train Loss: 0.037257738411426544, Test Loss: 0.08090927451848984\n",
      "Epoch 3904, Train Loss: 0.03724755346775055, Test Loss: 0.08091157674789429\n",
      "Epoch 3905, Train Loss: 0.03723639249801636, Test Loss: 0.08090939372777939\n",
      "Epoch 3906, Train Loss: 0.037226054817438126, Test Loss: 0.08091311901807785\n",
      "Epoch 3907, Train Loss: 0.0372156985104084, Test Loss: 0.08089672774076462\n",
      "Epoch 3908, Train Loss: 0.037204764783382416, Test Loss: 0.08089196681976318\n",
      "Epoch 3909, Train Loss: 0.03719445317983627, Test Loss: 0.08089334517717361\n",
      "Epoch 3910, Train Loss: 0.037185363471508026, Test Loss: 0.08089073747396469\n",
      "Epoch 3911, Train Loss: 0.0371738001704216, Test Loss: 0.08086991310119629\n",
      "Epoch 3912, Train Loss: 0.03716319426894188, Test Loss: 0.08086096495389938\n",
      "Epoch 3913, Train Loss: 0.03715275228023529, Test Loss: 0.08085518330335617\n",
      "Epoch 3914, Train Loss: 0.03714272379875183, Test Loss: 0.080848328769207\n",
      "Epoch 3915, Train Loss: 0.03713231161236763, Test Loss: 0.08083949238061905\n",
      "Epoch 3916, Train Loss: 0.03712170198559761, Test Loss: 0.08084339648485184\n",
      "Epoch 3917, Train Loss: 0.037112001329660416, Test Loss: 0.08083049207925797\n",
      "Epoch 3918, Train Loss: 0.0371004119515419, Test Loss: 0.08083000034093857\n",
      "Epoch 3919, Train Loss: 0.03708986937999725, Test Loss: 0.08082149177789688\n",
      "Epoch 3920, Train Loss: 0.03707955777645111, Test Loss: 0.08081650733947754\n",
      "Epoch 3921, Train Loss: 0.03706970438361168, Test Loss: 0.08080973476171494\n",
      "Epoch 3922, Train Loss: 0.03705921396613121, Test Loss: 0.08080723881721497\n",
      "Epoch 3923, Train Loss: 0.037048690021038055, Test Loss: 0.08079038560390472\n",
      "Epoch 3924, Train Loss: 0.03703831508755684, Test Loss: 0.0807836726307869\n",
      "Epoch 3925, Train Loss: 0.03702782094478607, Test Loss: 0.080778568983078\n",
      "Epoch 3926, Train Loss: 0.037017565220594406, Test Loss: 0.08078374713659286\n",
      "Epoch 3927, Train Loss: 0.03700726851820946, Test Loss: 0.08077169209718704\n",
      "Epoch 3928, Train Loss: 0.03699718043208122, Test Loss: 0.0807856097817421\n",
      "Epoch 3929, Train Loss: 0.03698631748557091, Test Loss: 0.08076503872871399\n",
      "Epoch 3930, Train Loss: 0.03697636350989342, Test Loss: 0.08075328916311264\n",
      "Epoch 3931, Train Loss: 0.03696569800376892, Test Loss: 0.0807509645819664\n",
      "Epoch 3932, Train Loss: 0.03695586696267128, Test Loss: 0.08075159043073654\n",
      "Epoch 3933, Train Loss: 0.03694511950016022, Test Loss: 0.08073969930410385\n",
      "Epoch 3934, Train Loss: 0.03693494200706482, Test Loss: 0.08072451502084732\n",
      "Epoch 3935, Train Loss: 0.03692438453435898, Test Loss: 0.08072289079427719\n",
      "Epoch 3936, Train Loss: 0.036915071308612823, Test Loss: 0.08070717006921768\n",
      "Epoch 3937, Train Loss: 0.03690427541732788, Test Loss: 0.08071243017911911\n",
      "Epoch 3938, Train Loss: 0.03689428046345711, Test Loss: 0.08070293068885803\n",
      "Epoch 3939, Train Loss: 0.03688390552997589, Test Loss: 0.08069315552711487\n",
      "Epoch 3940, Train Loss: 0.0368744432926178, Test Loss: 0.08067315816879272\n",
      "Epoch 3941, Train Loss: 0.03686455264687538, Test Loss: 0.08066254109144211\n",
      "Epoch 3942, Train Loss: 0.03685365617275238, Test Loss: 0.08066128939390182\n",
      "Epoch 3943, Train Loss: 0.03684253618121147, Test Loss: 0.08067085593938828\n",
      "Epoch 3944, Train Loss: 0.03683248907327652, Test Loss: 0.08068082481622696\n",
      "Epoch 3945, Train Loss: 0.0368221215903759, Test Loss: 0.08066350221633911\n",
      "Epoch 3946, Train Loss: 0.0368119440972805, Test Loss: 0.08066044747829437\n",
      "Epoch 3947, Train Loss: 0.03680159151554108, Test Loss: 0.08065687865018845\n",
      "Epoch 3948, Train Loss: 0.036792024970054626, Test Loss: 0.0806412547826767\n",
      "Epoch 3949, Train Loss: 0.03678200766444206, Test Loss: 0.08062617480754852\n",
      "Epoch 3950, Train Loss: 0.03677132725715637, Test Loss: 0.08063842356204987\n",
      "Epoch 3951, Train Loss: 0.03676166012883186, Test Loss: 0.08063295483589172\n",
      "Epoch 3952, Train Loss: 0.03675112500786781, Test Loss: 0.08063724637031555\n",
      "Epoch 3953, Train Loss: 0.03674083203077316, Test Loss: 0.08062557876110077\n",
      "Epoch 3954, Train Loss: 0.03673079237341881, Test Loss: 0.08062496036291122\n",
      "Epoch 3955, Train Loss: 0.03672144562005997, Test Loss: 0.08063789457082748\n",
      "Epoch 3956, Train Loss: 0.03671042248606682, Test Loss: 0.08061032742261887\n",
      "Epoch 3957, Train Loss: 0.036700356751680374, Test Loss: 0.08060799539089203\n",
      "Epoch 3958, Train Loss: 0.03669076785445213, Test Loss: 0.08058717101812363\n",
      "Epoch 3959, Train Loss: 0.036680351942777634, Test Loss: 0.0805845558643341\n",
      "Epoch 3960, Train Loss: 0.0366702526807785, Test Loss: 0.08057400584220886\n",
      "Epoch 3961, Train Loss: 0.03665997087955475, Test Loss: 0.08057618886232376\n",
      "Epoch 3962, Train Loss: 0.036650318652391434, Test Loss: 0.08056718856096268\n",
      "Epoch 3963, Train Loss: 0.03664073348045349, Test Loss: 0.08056376874446869\n",
      "Epoch 3964, Train Loss: 0.036630358546972275, Test Loss: 0.08056149631738663\n",
      "Epoch 3965, Train Loss: 0.03662003204226494, Test Loss: 0.08054842054843903\n",
      "Epoch 3966, Train Loss: 0.036609821021556854, Test Loss: 0.08054668456315994\n",
      "Epoch 3967, Train Loss: 0.03659990429878235, Test Loss: 0.08054715394973755\n",
      "Epoch 3968, Train Loss: 0.036589715629816055, Test Loss: 0.08053930848836899\n",
      "Epoch 3969, Train Loss: 0.03657981753349304, Test Loss: 0.08052437007427216\n",
      "Epoch 3970, Train Loss: 0.036570169031620026, Test Loss: 0.0805124044418335\n",
      "Epoch 3971, Train Loss: 0.036559779196977615, Test Loss: 0.08051005005836487\n",
      "Epoch 3972, Train Loss: 0.03654977306723595, Test Loss: 0.08050916343927383\n",
      "Epoch 3973, Train Loss: 0.03653976693749428, Test Loss: 0.08049818873405457\n",
      "Epoch 3974, Train Loss: 0.03653080016374588, Test Loss: 0.08048383891582489\n",
      "Epoch 3975, Train Loss: 0.036520153284072876, Test Loss: 0.08047708868980408\n",
      "Epoch 3976, Train Loss: 0.03651029244065285, Test Loss: 0.08049184083938599\n",
      "Epoch 3977, Train Loss: 0.03650069609284401, Test Loss: 0.08049631118774414\n",
      "Epoch 3978, Train Loss: 0.03649015352129936, Test Loss: 0.08048059791326523\n",
      "Epoch 3979, Train Loss: 0.036480456590652466, Test Loss: 0.08047515898942947\n",
      "Epoch 3980, Train Loss: 0.03647024556994438, Test Loss: 0.08047127723693848\n",
      "Epoch 3981, Train Loss: 0.03646093234419823, Test Loss: 0.08045937865972519\n",
      "Epoch 3982, Train Loss: 0.03645056113600731, Test Loss: 0.08044983446598053\n",
      "Epoch 3983, Train Loss: 0.03644058108329773, Test Loss: 0.08045340329408646\n",
      "Epoch 3984, Train Loss: 0.036431580781936646, Test Loss: 0.08042904734611511\n",
      "Epoch 3985, Train Loss: 0.036421388387680054, Test Loss: 0.08043093234300613\n",
      "Epoch 3986, Train Loss: 0.03641166910529137, Test Loss: 0.08041080087423325\n",
      "Epoch 3987, Train Loss: 0.0364019013941288, Test Loss: 0.08040858805179596\n",
      "Epoch 3988, Train Loss: 0.036391668021678925, Test Loss: 0.08041249215602875\n",
      "Epoch 3989, Train Loss: 0.036381807178258896, Test Loss: 0.08040262013673782\n",
      "Epoch 3990, Train Loss: 0.03637237846851349, Test Loss: 0.0803895890712738\n",
      "Epoch 3991, Train Loss: 0.036361999809741974, Test Loss: 0.08040248602628708\n",
      "Epoch 3992, Train Loss: 0.036352384835481644, Test Loss: 0.08039945363998413\n",
      "Epoch 3993, Train Loss: 0.03634236380457878, Test Loss: 0.08040541410446167\n",
      "Epoch 3994, Train Loss: 0.03633284196257591, Test Loss: 0.08038917928934097\n",
      "Epoch 3995, Train Loss: 0.03632306307554245, Test Loss: 0.08039458096027374\n",
      "Epoch 3996, Train Loss: 0.036313530057668686, Test Loss: 0.08038020133972168\n",
      "Epoch 3997, Train Loss: 0.036303192377090454, Test Loss: 0.08037441223859787\n",
      "Epoch 3998, Train Loss: 0.0362948514521122, Test Loss: 0.08036014437675476\n",
      "Epoch 3999, Train Loss: 0.03628432750701904, Test Loss: 0.08035177737474442\n",
      "Epoch 4000, Train Loss: 0.03627404198050499, Test Loss: 0.08034162223339081\n",
      "Epoch 4001, Train Loss: 0.03626425936818123, Test Loss: 0.08034065365791321\n",
      "Epoch 4002, Train Loss: 0.03625490143895149, Test Loss: 0.08033797144889832\n",
      "Epoch 4003, Train Loss: 0.03624506667256355, Test Loss: 0.0803360566496849\n",
      "Epoch 4004, Train Loss: 0.03623529151082039, Test Loss: 0.08033166825771332\n",
      "Epoch 4005, Train Loss: 0.036225732415914536, Test Loss: 0.08031795173883438\n",
      "Epoch 4006, Train Loss: 0.03621615096926689, Test Loss: 0.08030947297811508\n",
      "Epoch 4007, Train Loss: 0.03620583191514015, Test Loss: 0.08031227439641953\n",
      "Epoch 4008, Train Loss: 0.03619660809636116, Test Loss: 0.08031804114580154\n",
      "Epoch 4009, Train Loss: 0.03618651628494263, Test Loss: 0.080307237803936\n",
      "Epoch 4010, Train Loss: 0.036176737397909164, Test Loss: 0.08029697090387344\n",
      "Epoch 4011, Train Loss: 0.03616755083203316, Test Loss: 0.08029167354106903\n",
      "Epoch 4012, Train Loss: 0.03615747019648552, Test Loss: 0.08030606061220169\n",
      "Epoch 4013, Train Loss: 0.03614819794893265, Test Loss: 0.08029574900865555\n",
      "Epoch 4014, Train Loss: 0.036138713359832764, Test Loss: 0.08027594536542892\n",
      "Epoch 4015, Train Loss: 0.03612881153821945, Test Loss: 0.08026888221502304\n",
      "Epoch 4016, Train Loss: 0.03611895069479942, Test Loss: 0.08027272671461105\n",
      "Epoch 4017, Train Loss: 0.03610941022634506, Test Loss: 0.08026783913373947\n",
      "Epoch 4018, Train Loss: 0.036099888384342194, Test Loss: 0.08024854958057404\n",
      "Epoch 4019, Train Loss: 0.03608996421098709, Test Loss: 0.08024679869413376\n",
      "Epoch 4020, Train Loss: 0.036080457270145416, Test Loss: 0.08024279773235321\n",
      "Epoch 4021, Train Loss: 0.03607242554426193, Test Loss: 0.0802280455827713\n",
      "Epoch 4022, Train Loss: 0.036062173545360565, Test Loss: 0.08021989464759827\n",
      "Epoch 4023, Train Loss: 0.03605165332555771, Test Loss: 0.08022257685661316\n",
      "Epoch 4024, Train Loss: 0.03604291006922722, Test Loss: 0.08020532876253128\n",
      "Epoch 4025, Train Loss: 0.0360327772796154, Test Loss: 0.08020363003015518\n",
      "Epoch 4026, Train Loss: 0.03602343797683716, Test Loss: 0.08019816875457764\n",
      "Epoch 4027, Train Loss: 0.0360136553645134, Test Loss: 0.08019115030765533\n",
      "Epoch 4028, Train Loss: 0.03600413724780083, Test Loss: 0.08018477261066437\n",
      "Epoch 4029, Train Loss: 0.03599413484334946, Test Loss: 0.08018401265144348\n",
      "Epoch 4030, Train Loss: 0.035984959453344345, Test Loss: 0.0801825150847435\n",
      "Epoch 4031, Train Loss: 0.03597496822476387, Test Loss: 0.080183245241642\n",
      "Epoch 4032, Train Loss: 0.03596537932753563, Test Loss: 0.08018411695957184\n",
      "Epoch 4033, Train Loss: 0.03595632314682007, Test Loss: 0.08017761260271072\n",
      "Epoch 4034, Train Loss: 0.03594636544585228, Test Loss: 0.08016858994960785\n",
      "Epoch 4035, Train Loss: 0.035937003791332245, Test Loss: 0.08016860485076904\n",
      "Epoch 4036, Train Loss: 0.035927630960941315, Test Loss: 0.08015362173318863\n",
      "Epoch 4037, Train Loss: 0.03591809421777725, Test Loss: 0.08014735579490662\n",
      "Epoch 4038, Train Loss: 0.0359082855284214, Test Loss: 0.08015153557062149\n",
      "Epoch 4039, Train Loss: 0.03589945286512375, Test Loss: 0.08015000820159912\n",
      "Epoch 4040, Train Loss: 0.03588947281241417, Test Loss: 0.08013217896223068\n",
      "Epoch 4041, Train Loss: 0.03588011488318443, Test Loss: 0.08011631667613983\n",
      "Epoch 4042, Train Loss: 0.035870812833309174, Test Loss: 0.08011206984519958\n",
      "Epoch 4043, Train Loss: 0.03586115315556526, Test Loss: 0.08010105788707733\n",
      "Epoch 4044, Train Loss: 0.0358521044254303, Test Loss: 0.08010300248861313\n",
      "Epoch 4045, Train Loss: 0.03584190458059311, Test Loss: 0.08010178059339523\n",
      "Epoch 4046, Train Loss: 0.03583237901329994, Test Loss: 0.08009609580039978\n",
      "Epoch 4047, Train Loss: 0.03582283854484558, Test Loss: 0.08008801937103271\n",
      "Epoch 4048, Train Loss: 0.035814013332128525, Test Loss: 0.08006861060857773\n",
      "Epoch 4049, Train Loss: 0.03580509126186371, Test Loss: 0.08005595952272415\n",
      "Epoch 4050, Train Loss: 0.03579462692141533, Test Loss: 0.08006615936756134\n",
      "Epoch 4051, Train Loss: 0.035784922540187836, Test Loss: 0.08005990087985992\n",
      "Epoch 4052, Train Loss: 0.03577599301934242, Test Loss: 0.08005702495574951\n",
      "Epoch 4053, Train Loss: 0.03576621785759926, Test Loss: 0.08006042242050171\n",
      "Epoch 4054, Train Loss: 0.03575722873210907, Test Loss: 0.08004740625619888\n",
      "Epoch 4055, Train Loss: 0.03574761375784874, Test Loss: 0.08004288375377655\n",
      "Epoch 4056, Train Loss: 0.03573871776461601, Test Loss: 0.08003181964159012\n",
      "Epoch 4057, Train Loss: 0.03572921082377434, Test Loss: 0.08001943677663803\n",
      "Epoch 4058, Train Loss: 0.03571945056319237, Test Loss: 0.0800212174654007\n",
      "Epoch 4059, Train Loss: 0.03571011498570442, Test Loss: 0.08001147955656052\n",
      "Epoch 4060, Train Loss: 0.03570059314370155, Test Loss: 0.08001034706830978\n",
      "Epoch 4061, Train Loss: 0.03569135442376137, Test Loss: 0.07999751716852188\n",
      "Epoch 4062, Train Loss: 0.03568162024021149, Test Loss: 0.08000587671995163\n",
      "Epoch 4063, Train Loss: 0.03567236661911011, Test Loss: 0.08000430464744568\n",
      "Epoch 4064, Train Loss: 0.03566300496459007, Test Loss: 0.07999671995639801\n",
      "Epoch 4065, Train Loss: 0.035653870552778244, Test Loss: 0.08000081032514572\n",
      "Epoch 4066, Train Loss: 0.03564465418457985, Test Loss: 0.08000122755765915\n",
      "Epoch 4067, Train Loss: 0.0356355682015419, Test Loss: 0.07999887317419052\n",
      "Epoch 4068, Train Loss: 0.03562525659799576, Test Loss: 0.07998483628034592\n",
      "Epoch 4069, Train Loss: 0.03561607748270035, Test Loss: 0.07997237145900726\n",
      "Epoch 4070, Train Loss: 0.035606492310762405, Test Loss: 0.07996625453233719\n",
      "Epoch 4071, Train Loss: 0.03559740632772446, Test Loss: 0.07995415478944778\n",
      "Epoch 4072, Train Loss: 0.035587992519140244, Test Loss: 0.07995499670505524\n",
      "Epoch 4073, Train Loss: 0.035578325390815735, Test Loss: 0.07995327562093735\n",
      "Epoch 4074, Train Loss: 0.03556887432932854, Test Loss: 0.07994525134563446\n",
      "Epoch 4075, Train Loss: 0.0355600006878376, Test Loss: 0.07994135469198227\n",
      "Epoch 4076, Train Loss: 0.03555034101009369, Test Loss: 0.07994537800550461\n",
      "Epoch 4077, Train Loss: 0.03554089367389679, Test Loss: 0.07993607223033905\n",
      "Epoch 4078, Train Loss: 0.03553207591176033, Test Loss: 0.07993162423372269\n",
      "Epoch 4079, Train Loss: 0.03552234545350075, Test Loss: 0.07992325723171234\n",
      "Epoch 4080, Train Loss: 0.035513345152139664, Test Loss: 0.07991253584623337\n",
      "Epoch 4081, Train Loss: 0.03550394997000694, Test Loss: 0.07992856204509735\n",
      "Epoch 4082, Train Loss: 0.03549445420503616, Test Loss: 0.07991814613342285\n",
      "Epoch 4083, Train Loss: 0.03548509255051613, Test Loss: 0.07990414649248123\n",
      "Epoch 4084, Train Loss: 0.03547649830579758, Test Loss: 0.07990060746669769\n",
      "Epoch 4085, Train Loss: 0.035466454923152924, Test Loss: 0.07989434897899628\n",
      "Epoch 4086, Train Loss: 0.035457827150821686, Test Loss: 0.07988651096820831\n",
      "Epoch 4087, Train Loss: 0.03544885665178299, Test Loss: 0.07989580184221268\n",
      "Epoch 4088, Train Loss: 0.035439249128103256, Test Loss: 0.07986419647932053\n",
      "Epoch 4089, Train Loss: 0.035430215299129486, Test Loss: 0.07985911518335342\n",
      "Epoch 4090, Train Loss: 0.035420093685388565, Test Loss: 0.07985810190439224\n",
      "Epoch 4091, Train Loss: 0.035411592572927475, Test Loss: 0.0798485055565834\n",
      "Epoch 4092, Train Loss: 0.03540167957544327, Test Loss: 0.07984494417905807\n",
      "Epoch 4093, Train Loss: 0.035392213612794876, Test Loss: 0.0798489898443222\n",
      "Epoch 4094, Train Loss: 0.035383451730012894, Test Loss: 0.07985059171915054\n",
      "Epoch 4095, Train Loss: 0.0353742390871048, Test Loss: 0.07985643297433853\n",
      "Epoch 4096, Train Loss: 0.03536481410264969, Test Loss: 0.07984752207994461\n",
      "Epoch 4097, Train Loss: 0.035355404019355774, Test Loss: 0.07982110977172852\n",
      "Epoch 4098, Train Loss: 0.03534632548689842, Test Loss: 0.0798087939620018\n",
      "Epoch 4099, Train Loss: 0.03533654287457466, Test Loss: 0.07982254773378372\n",
      "Epoch 4100, Train Loss: 0.03532753139734268, Test Loss: 0.07981745898723602\n",
      "Epoch 4101, Train Loss: 0.035318177193403244, Test Loss: 0.07980795949697495\n",
      "Epoch 4102, Train Loss: 0.03530880808830261, Test Loss: 0.07980972528457642\n",
      "Epoch 4103, Train Loss: 0.03529999032616615, Test Loss: 0.07980701327323914\n",
      "Epoch 4104, Train Loss: 0.03529135137796402, Test Loss: 0.07978716492652893\n",
      "Epoch 4105, Train Loss: 0.03528168424963951, Test Loss: 0.07977136969566345\n",
      "Epoch 4106, Train Loss: 0.03527224808931351, Test Loss: 0.07977484911680222\n",
      "Epoch 4107, Train Loss: 0.035263191908597946, Test Loss: 0.07977304607629776\n",
      "Epoch 4108, Train Loss: 0.03525380790233612, Test Loss: 0.07976319640874863\n",
      "Epoch 4109, Train Loss: 0.03524480015039444, Test Loss: 0.07975678890943527\n",
      "Epoch 4110, Train Loss: 0.03523555397987366, Test Loss: 0.0797652006149292\n",
      "Epoch 4111, Train Loss: 0.03522627055644989, Test Loss: 0.07975741475820541\n",
      "Epoch 4112, Train Loss: 0.03521664813160896, Test Loss: 0.07974159717559814\n",
      "Epoch 4113, Train Loss: 0.035207509994506836, Test Loss: 0.07974258810281754\n",
      "Epoch 4114, Train Loss: 0.03519859537482262, Test Loss: 0.07973279058933258\n",
      "Epoch 4115, Train Loss: 0.035190265625715256, Test Loss: 0.07973098009824753\n",
      "Epoch 4116, Train Loss: 0.0351804718375206, Test Loss: 0.07972687482833862\n",
      "Epoch 4117, Train Loss: 0.03517155721783638, Test Loss: 0.07972034811973572\n",
      "Epoch 4118, Train Loss: 0.0351620577275753, Test Loss: 0.07971110194921494\n",
      "Epoch 4119, Train Loss: 0.03515278920531273, Test Loss: 0.07971712946891785\n",
      "Epoch 4120, Train Loss: 0.035143617540597916, Test Loss: 0.0797080472111702\n",
      "Epoch 4121, Train Loss: 0.035135332494974136, Test Loss: 0.07968608289957047\n",
      "Epoch 4122, Train Loss: 0.03512538969516754, Test Loss: 0.079694963991642\n",
      "Epoch 4123, Train Loss: 0.03511623293161392, Test Loss: 0.07969561964273453\n",
      "Epoch 4124, Train Loss: 0.03510703146457672, Test Loss: 0.07968129217624664\n",
      "Epoch 4125, Train Loss: 0.035098228603601456, Test Loss: 0.07967860996723175\n",
      "Epoch 4126, Train Loss: 0.035088762640953064, Test Loss: 0.07967513054609299\n",
      "Epoch 4127, Train Loss: 0.03508004918694496, Test Loss: 0.07965511828660965\n",
      "Epoch 4128, Train Loss: 0.03507085517048836, Test Loss: 0.07966902107000351\n",
      "Epoch 4129, Train Loss: 0.035061586648225784, Test Loss: 0.07965834438800812\n",
      "Epoch 4130, Train Loss: 0.03505232557654381, Test Loss: 0.07965178042650223\n",
      "Epoch 4131, Train Loss: 0.03504358232021332, Test Loss: 0.0796460434794426\n",
      "Epoch 4132, Train Loss: 0.03503470495343208, Test Loss: 0.0796407014131546\n",
      "Epoch 4133, Train Loss: 0.03502635285258293, Test Loss: 0.07964610308408737\n",
      "Epoch 4134, Train Loss: 0.035016100853681564, Test Loss: 0.0796409323811531\n",
      "Epoch 4135, Train Loss: 0.03500763326883316, Test Loss: 0.07962489873170853\n",
      "Epoch 4136, Train Loss: 0.03499817103147507, Test Loss: 0.07962319254875183\n",
      "Epoch 4137, Train Loss: 0.034989308565855026, Test Loss: 0.07962437719106674\n",
      "Epoch 4138, Train Loss: 0.03498043492436409, Test Loss: 0.07960116863250732\n",
      "Epoch 4139, Train Loss: 0.034970950335264206, Test Loss: 0.07960744202136993\n",
      "Epoch 4140, Train Loss: 0.03496243432164192, Test Loss: 0.07958794385194778\n",
      "Epoch 4141, Train Loss: 0.03495275601744652, Test Loss: 0.07959204167127609\n",
      "Epoch 4142, Train Loss: 0.03494394198060036, Test Loss: 0.07958728075027466\n",
      "Epoch 4143, Train Loss: 0.03493538126349449, Test Loss: 0.07958292216062546\n",
      "Epoch 4144, Train Loss: 0.034925706684589386, Test Loss: 0.07958397269248962\n",
      "Epoch 4145, Train Loss: 0.03491678833961487, Test Loss: 0.07956589013338089\n",
      "Epoch 4146, Train Loss: 0.03490813076496124, Test Loss: 0.07955977320671082\n",
      "Epoch 4147, Train Loss: 0.03489886596798897, Test Loss: 0.07955872267484665\n",
      "Epoch 4148, Train Loss: 0.034890227019786835, Test Loss: 0.07956216484308243\n",
      "Epoch 4149, Train Loss: 0.03488095849752426, Test Loss: 0.0795576423406601\n",
      "Epoch 4150, Train Loss: 0.03487187251448631, Test Loss: 0.07954858988523483\n",
      "Epoch 4151, Train Loss: 0.034862734377384186, Test Loss: 0.07953531295061111\n",
      "Epoch 4152, Train Loss: 0.03485407307744026, Test Loss: 0.07953131943941116\n",
      "Epoch 4153, Train Loss: 0.03484562784433365, Test Loss: 0.0795314833521843\n",
      "Epoch 4154, Train Loss: 0.034835878759622574, Test Loss: 0.0795268714427948\n",
      "Epoch 4155, Train Loss: 0.034827105700969696, Test Loss: 0.07951894402503967\n",
      "Epoch 4156, Train Loss: 0.03481798619031906, Test Loss: 0.07952792197465897\n",
      "Epoch 4157, Train Loss: 0.03480943664908409, Test Loss: 0.0795077532529831\n",
      "Epoch 4158, Train Loss: 0.03480043262243271, Test Loss: 0.0795162245631218\n",
      "Epoch 4159, Train Loss: 0.0347917303442955, Test Loss: 0.07950683683156967\n",
      "Epoch 4160, Train Loss: 0.03478219360113144, Test Loss: 0.07950450479984283\n",
      "Epoch 4161, Train Loss: 0.03477375954389572, Test Loss: 0.07949232310056686\n",
      "Epoch 4162, Train Loss: 0.0347653292119503, Test Loss: 0.07950073480606079\n",
      "Epoch 4163, Train Loss: 0.03475567325949669, Test Loss: 0.07948865741491318\n",
      "Epoch 4164, Train Loss: 0.03474651649594307, Test Loss: 0.07948779314756393\n",
      "Epoch 4165, Train Loss: 0.03473789244890213, Test Loss: 0.0794861912727356\n",
      "Epoch 4166, Train Loss: 0.03472926467657089, Test Loss: 0.07946707308292389\n",
      "Epoch 4167, Train Loss: 0.03471998870372772, Test Loss: 0.07947984337806702\n",
      "Epoch 4168, Train Loss: 0.034711387008428574, Test Loss: 0.07948140799999237\n",
      "Epoch 4169, Train Loss: 0.03470207005739212, Test Loss: 0.07946915179491043\n",
      "Epoch 4170, Train Loss: 0.03469303250312805, Test Loss: 0.07946450263261795\n",
      "Epoch 4171, Train Loss: 0.03468482196331024, Test Loss: 0.07944037020206451\n",
      "Epoch 4172, Train Loss: 0.034675482660532, Test Loss: 0.07944568991661072\n",
      "Epoch 4173, Train Loss: 0.034666746854782104, Test Loss: 0.07944171875715256\n",
      "Epoch 4174, Train Loss: 0.03465791046619415, Test Loss: 0.07942815870046616\n",
      "Epoch 4175, Train Loss: 0.03464886546134949, Test Loss: 0.07943025231361389\n",
      "Epoch 4176, Train Loss: 0.03463999554514885, Test Loss: 0.07942397147417068\n",
      "Epoch 4177, Train Loss: 0.03463119640946388, Test Loss: 0.07942216843366623\n",
      "Epoch 4178, Train Loss: 0.03462238609790802, Test Loss: 0.07942427694797516\n",
      "Epoch 4179, Train Loss: 0.03461365029215813, Test Loss: 0.07941147685050964\n",
      "Epoch 4180, Train Loss: 0.03460466116666794, Test Loss: 0.07939263433218002\n",
      "Epoch 4181, Train Loss: 0.03459613397717476, Test Loss: 0.07938939332962036\n",
      "Epoch 4182, Train Loss: 0.03458753228187561, Test Loss: 0.07937706261873245\n",
      "Epoch 4183, Train Loss: 0.03457840159535408, Test Loss: 0.07938696444034576\n",
      "Epoch 4184, Train Loss: 0.03456930071115494, Test Loss: 0.0793788805603981\n",
      "Epoch 4185, Train Loss: 0.03456166386604309, Test Loss: 0.07938426733016968\n",
      "Epoch 4186, Train Loss: 0.03455156460404396, Test Loss: 0.07937150448560715\n",
      "Epoch 4187, Train Loss: 0.03454272449016571, Test Loss: 0.07936883717775345\n",
      "Epoch 4188, Train Loss: 0.03453422710299492, Test Loss: 0.07935415953397751\n",
      "Epoch 4189, Train Loss: 0.03452490270137787, Test Loss: 0.07935818284749985\n",
      "Epoch 4190, Train Loss: 0.03451606258749962, Test Loss: 0.07935911417007446\n",
      "Epoch 4191, Train Loss: 0.03450759872794151, Test Loss: 0.07935426384210587\n",
      "Epoch 4192, Train Loss: 0.03449862822890282, Test Loss: 0.07934743911027908\n",
      "Epoch 4193, Train Loss: 0.034489706158638, Test Loss: 0.07934606075286865\n",
      "Epoch 4194, Train Loss: 0.034481581300497055, Test Loss: 0.07931962609291077\n",
      "Epoch 4195, Train Loss: 0.03447265923023224, Test Loss: 0.07932502031326294\n",
      "Epoch 4196, Train Loss: 0.03446320444345474, Test Loss: 0.07931935787200928\n",
      "Epoch 4197, Train Loss: 0.03445475175976753, Test Loss: 0.07931346446275711\n",
      "Epoch 4198, Train Loss: 0.03444619104266167, Test Loss: 0.07931776344776154\n",
      "Epoch 4199, Train Loss: 0.03443746268749237, Test Loss: 0.07930189371109009\n",
      "Epoch 4200, Train Loss: 0.03442870453000069, Test Loss: 0.07930781692266464\n",
      "Epoch 4201, Train Loss: 0.034419480711221695, Test Loss: 0.07930231839418411\n",
      "Epoch 4202, Train Loss: 0.034411050379276276, Test Loss: 0.07929468154907227\n",
      "Epoch 4203, Train Loss: 0.03440240025520325, Test Loss: 0.07928439229726791\n",
      "Epoch 4204, Train Loss: 0.034393686801195145, Test Loss: 0.07928202301263809\n",
      "Epoch 4205, Train Loss: 0.03438475355505943, Test Loss: 0.07927600294351578\n",
      "Epoch 4206, Train Loss: 0.03437606245279312, Test Loss: 0.0792737752199173\n",
      "Epoch 4207, Train Loss: 0.03436737880110741, Test Loss: 0.0792657658457756\n",
      "Epoch 4208, Train Loss: 0.034358374774456024, Test Loss: 0.0792650505900383\n",
      "Epoch 4209, Train Loss: 0.03434989973902702, Test Loss: 0.07925684005022049\n",
      "Epoch 4210, Train Loss: 0.034341324120759964, Test Loss: 0.07924642413854599\n",
      "Epoch 4211, Train Loss: 0.0343322679400444, Test Loss: 0.07925055176019669\n",
      "Epoch 4212, Train Loss: 0.03432369604706764, Test Loss: 0.07926338165998459\n",
      "Epoch 4213, Train Loss: 0.034314919263124466, Test Loss: 0.07925739139318466\n",
      "Epoch 4214, Train Loss: 0.03430597484111786, Test Loss: 0.07924605160951614\n",
      "Epoch 4215, Train Loss: 0.03429753705859184, Test Loss: 0.07923729717731476\n",
      "Epoch 4216, Train Loss: 0.03428905829787254, Test Loss: 0.07924066483974457\n",
      "Epoch 4217, Train Loss: 0.03427997604012489, Test Loss: 0.07923326641321182\n",
      "Epoch 4218, Train Loss: 0.03427156060934067, Test Loss: 0.07923342287540436\n",
      "Epoch 4219, Train Loss: 0.03426283597946167, Test Loss: 0.07921668142080307\n",
      "Epoch 4220, Train Loss: 0.0342538021504879, Test Loss: 0.07920771092176437\n",
      "Epoch 4221, Train Loss: 0.03424539417028427, Test Loss: 0.07919460535049438\n",
      "Epoch 4222, Train Loss: 0.03423706069588661, Test Loss: 0.07918808609247208\n",
      "Epoch 4223, Train Loss: 0.034228067845106125, Test Loss: 0.0791923999786377\n",
      "Epoch 4224, Train Loss: 0.03421933203935623, Test Loss: 0.07918503880500793\n",
      "Epoch 4225, Train Loss: 0.03421049937605858, Test Loss: 0.07917863130569458\n",
      "Epoch 4226, Train Loss: 0.03420183062553406, Test Loss: 0.07918577641248703\n",
      "Epoch 4227, Train Loss: 0.03419319912791252, Test Loss: 0.07916777580976486\n",
      "Epoch 4228, Train Loss: 0.03418448939919472, Test Loss: 0.07915923744440079\n",
      "Epoch 4229, Train Loss: 0.034175943583250046, Test Loss: 0.07915385812520981\n",
      "Epoch 4230, Train Loss: 0.034167464822530746, Test Loss: 0.07915927469730377\n",
      "Epoch 4231, Train Loss: 0.03415895998477936, Test Loss: 0.07915051281452179\n",
      "Epoch 4232, Train Loss: 0.03415052592754364, Test Loss: 0.07914094626903534\n",
      "Epoch 4233, Train Loss: 0.034141816198825836, Test Loss: 0.07912962883710861\n",
      "Epoch 4234, Train Loss: 0.034132715314626694, Test Loss: 0.07912328094244003\n",
      "Epoch 4235, Train Loss: 0.034124936908483505, Test Loss: 0.07910944521427155\n",
      "Epoch 4236, Train Loss: 0.03411582484841347, Test Loss: 0.07911184430122375\n",
      "Epoch 4237, Train Loss: 0.03410730138421059, Test Loss: 0.07911128550767899\n",
      "Epoch 4238, Train Loss: 0.03409859538078308, Test Loss: 0.07909738272428513\n",
      "Epoch 4239, Train Loss: 0.03408985212445259, Test Loss: 0.07910779863595963\n",
      "Epoch 4240, Train Loss: 0.03408131003379822, Test Loss: 0.07911001890897751\n",
      "Epoch 4241, Train Loss: 0.03407403826713562, Test Loss: 0.07911507040262222\n",
      "Epoch 4242, Train Loss: 0.034063901752233505, Test Loss: 0.07910235971212387\n",
      "Epoch 4243, Train Loss: 0.03405548259615898, Test Loss: 0.07909244298934937\n",
      "Epoch 4244, Train Loss: 0.03404692932963371, Test Loss: 0.07908309996128082\n",
      "Epoch 4245, Train Loss: 0.034038759768009186, Test Loss: 0.07907752692699432\n",
      "Epoch 4246, Train Loss: 0.03402958810329437, Test Loss: 0.07907199114561081\n",
      "Epoch 4247, Train Loss: 0.03402150794863701, Test Loss: 0.07905859500169754\n",
      "Epoch 4248, Train Loss: 0.03401317447423935, Test Loss: 0.07905667275190353\n",
      "Epoch 4249, Train Loss: 0.034004226326942444, Test Loss: 0.07906918972730637\n",
      "Epoch 4250, Train Loss: 0.033995985984802246, Test Loss: 0.07907455414533615\n",
      "Epoch 4251, Train Loss: 0.033987224102020264, Test Loss: 0.07906141132116318\n",
      "Epoch 4252, Train Loss: 0.033979013562202454, Test Loss: 0.07904651015996933\n",
      "Epoch 4253, Train Loss: 0.03397023305296898, Test Loss: 0.07905595004558563\n",
      "Epoch 4254, Train Loss: 0.033961642533540726, Test Loss: 0.07904455065727234\n",
      "Epoch 4255, Train Loss: 0.03395333141088486, Test Loss: 0.07904946804046631\n",
      "Epoch 4256, Train Loss: 0.03394486382603645, Test Loss: 0.07902776449918747\n",
      "Epoch 4257, Train Loss: 0.03393667936325073, Test Loss: 0.07901491224765778\n",
      "Epoch 4258, Train Loss: 0.03392782062292099, Test Loss: 0.07901427149772644\n",
      "Epoch 4259, Train Loss: 0.033919092267751694, Test Loss: 0.07902058959007263\n",
      "Epoch 4260, Train Loss: 0.03391147777438164, Test Loss: 0.07900908589363098\n",
      "Epoch 4261, Train Loss: 0.033902306109666824, Test Loss: 0.07900364696979523\n",
      "Epoch 4262, Train Loss: 0.03389400616288185, Test Loss: 0.07899719476699829\n",
      "Epoch 4263, Train Loss: 0.03388534486293793, Test Loss: 0.07899603247642517\n",
      "Epoch 4264, Train Loss: 0.0338764563202858, Test Loss: 0.07899440079927444\n",
      "Epoch 4265, Train Loss: 0.03386860713362694, Test Loss: 0.07898222655057907\n",
      "Epoch 4266, Train Loss: 0.033859983086586, Test Loss: 0.07896889001131058\n",
      "Epoch 4267, Train Loss: 0.03385114297270775, Test Loss: 0.078978531062603\n",
      "Epoch 4268, Train Loss: 0.03384323790669441, Test Loss: 0.07898079603910446\n",
      "Epoch 4269, Train Loss: 0.033834557980298996, Test Loss: 0.0789865255355835\n",
      "Epoch 4270, Train Loss: 0.03382599726319313, Test Loss: 0.07897508144378662\n",
      "Epoch 4271, Train Loss: 0.03381743282079697, Test Loss: 0.07895984500646591\n",
      "Epoch 4272, Train Loss: 0.033809222280979156, Test Loss: 0.07895799726247787\n",
      "Epoch 4273, Train Loss: 0.0338004045188427, Test Loss: 0.07895134389400482\n",
      "Epoch 4274, Train Loss: 0.03379185497760773, Test Loss: 0.07893683016300201\n",
      "Epoch 4275, Train Loss: 0.033783577382564545, Test Loss: 0.07893768697977066\n",
      "Epoch 4276, Train Loss: 0.03377658873796463, Test Loss: 0.07892931997776031\n",
      "Epoch 4277, Train Loss: 0.03376799076795578, Test Loss: 0.07891938835382462\n",
      "Epoch 4278, Train Loss: 0.03375834599137306, Test Loss: 0.07892578095197678\n",
      "Epoch 4279, Train Loss: 0.03375020623207092, Test Loss: 0.07890601456165314\n",
      "Epoch 4280, Train Loss: 0.03374234214425087, Test Loss: 0.07891364395618439\n",
      "Epoch 4281, Train Loss: 0.03373297303915024, Test Loss: 0.07891832292079926\n",
      "Epoch 4282, Train Loss: 0.03372543677687645, Test Loss: 0.07890550792217255\n",
      "Epoch 4283, Train Loss: 0.03371613100171089, Test Loss: 0.07890135049819946\n",
      "Epoch 4284, Train Loss: 0.03370830789208412, Test Loss: 0.07889355719089508\n",
      "Epoch 4285, Train Loss: 0.03369991481304169, Test Loss: 0.07890390604734421\n",
      "Epoch 4286, Train Loss: 0.03369110822677612, Test Loss: 0.07889246195554733\n",
      "Epoch 4287, Train Loss: 0.03368241339921951, Test Loss: 0.07890230417251587\n",
      "Epoch 4288, Train Loss: 0.03367403522133827, Test Loss: 0.07888826727867126\n",
      "Epoch 4289, Train Loss: 0.033665817230939865, Test Loss: 0.07888088375329971\n",
      "Epoch 4290, Train Loss: 0.03365722671151161, Test Loss: 0.07887538522481918\n",
      "Epoch 4291, Train Loss: 0.03364891558885574, Test Loss: 0.07886756956577301\n",
      "Epoch 4292, Train Loss: 0.033640772104263306, Test Loss: 0.07886174321174622\n",
      "Epoch 4293, Train Loss: 0.03363235667347908, Test Loss: 0.07886334508657455\n",
      "Epoch 4294, Train Loss: 0.03362439200282097, Test Loss: 0.07886172831058502\n",
      "Epoch 4295, Train Loss: 0.03361617028713226, Test Loss: 0.07885643094778061\n",
      "Epoch 4296, Train Loss: 0.03360765054821968, Test Loss: 0.0788603276014328\n",
      "Epoch 4297, Train Loss: 0.033599380403757095, Test Loss: 0.07886301726102829\n",
      "Epoch 4298, Train Loss: 0.033590905368328094, Test Loss: 0.07886354625225067\n",
      "Epoch 4299, Train Loss: 0.03358244523406029, Test Loss: 0.07884511351585388\n",
      "Epoch 4300, Train Loss: 0.03357425704598427, Test Loss: 0.07881757616996765\n",
      "Epoch 4301, Train Loss: 0.03356590494513512, Test Loss: 0.07881700992584229\n",
      "Epoch 4302, Train Loss: 0.03355775773525238, Test Loss: 0.07880739867687225\n",
      "Epoch 4303, Train Loss: 0.03354862332344055, Test Loss: 0.07881443202495575\n",
      "Epoch 4304, Train Loss: 0.0335407480597496, Test Loss: 0.07879111915826797\n",
      "Epoch 4305, Train Loss: 0.033532336354255676, Test Loss: 0.0787988156080246\n",
      "Epoch 4306, Train Loss: 0.033523812890052795, Test Loss: 0.0788072720170021\n",
      "Epoch 4307, Train Loss: 0.03351568058133125, Test Loss: 0.07879053801298141\n",
      "Epoch 4308, Train Loss: 0.0335078202188015, Test Loss: 0.07878230512142181\n",
      "Epoch 4309, Train Loss: 0.03349946811795235, Test Loss: 0.07878777384757996\n",
      "Epoch 4310, Train Loss: 0.033490847796201706, Test Loss: 0.07878488302230835\n",
      "Epoch 4311, Train Loss: 0.03348243609070778, Test Loss: 0.0787811204791069\n",
      "Epoch 4312, Train Loss: 0.033474456518888474, Test Loss: 0.0787835642695427\n",
      "Epoch 4313, Train Loss: 0.03346659988164902, Test Loss: 0.07877326011657715\n",
      "Epoch 4314, Train Loss: 0.033458124846220016, Test Loss: 0.07877252995967865\n",
      "Epoch 4315, Train Loss: 0.03345036879181862, Test Loss: 0.07876439392566681\n",
      "Epoch 4316, Train Loss: 0.03344130143523216, Test Loss: 0.07876946032047272\n",
      "Epoch 4317, Train Loss: 0.0334329679608345, Test Loss: 0.07876010239124298\n",
      "Epoch 4318, Train Loss: 0.033424630761146545, Test Loss: 0.07875409722328186\n",
      "Epoch 4319, Train Loss: 0.033416587859392166, Test Loss: 0.0787462666630745\n",
      "Epoch 4320, Train Loss: 0.033408597111701965, Test Loss: 0.078737273812294\n",
      "Epoch 4321, Train Loss: 0.033400870859622955, Test Loss: 0.07873035967350006\n",
      "Epoch 4322, Train Loss: 0.033391762524843216, Test Loss: 0.07873500138521194\n",
      "Epoch 4323, Train Loss: 0.03338354825973511, Test Loss: 0.0787297710776329\n",
      "Epoch 4324, Train Loss: 0.03337593376636505, Test Loss: 0.07872654497623444\n",
      "Epoch 4325, Train Loss: 0.03336796164512634, Test Loss: 0.07872099429368973\n",
      "Epoch 4326, Train Loss: 0.03335955739021301, Test Loss: 0.07871540635824203\n",
      "Epoch 4327, Train Loss: 0.03335069492459297, Test Loss: 0.07871563732624054\n",
      "Epoch 4328, Train Loss: 0.0333426296710968, Test Loss: 0.07869639992713928\n",
      "Epoch 4329, Train Loss: 0.033334605395793915, Test Loss: 0.07870160043239594\n",
      "Epoch 4330, Train Loss: 0.033326197415590286, Test Loss: 0.07870212197303772\n",
      "Epoch 4331, Train Loss: 0.033318232744932175, Test Loss: 0.07868761569261551\n",
      "Epoch 4332, Train Loss: 0.033309727907180786, Test Loss: 0.07868147641420364\n",
      "Epoch 4333, Train Loss: 0.03330155089497566, Test Loss: 0.07868532836437225\n",
      "Epoch 4334, Train Loss: 0.03329354152083397, Test Loss: 0.07867320626974106\n",
      "Epoch 4335, Train Loss: 0.033285487443208694, Test Loss: 0.07867036759853363\n",
      "Epoch 4336, Train Loss: 0.0332769937813282, Test Loss: 0.07867458462715149\n",
      "Epoch 4337, Train Loss: 0.03326905146241188, Test Loss: 0.07866126298904419\n",
      "Epoch 4338, Train Loss: 0.03326046094298363, Test Loss: 0.07866431772708893\n",
      "Epoch 4339, Train Loss: 0.03325255587697029, Test Loss: 0.07865887880325317\n",
      "Epoch 4340, Train Loss: 0.033244188874959946, Test Loss: 0.07866745442152023\n",
      "Epoch 4341, Train Loss: 0.03323616459965706, Test Loss: 0.07865304499864578\n",
      "Epoch 4342, Train Loss: 0.03322773799300194, Test Loss: 0.07864945381879807\n",
      "Epoch 4343, Train Loss: 0.033219873905181885, Test Loss: 0.07864060997962952\n",
      "Epoch 4344, Train Loss: 0.03321147337555885, Test Loss: 0.07863771170377731\n",
      "Epoch 4345, Train Loss: 0.033203400671482086, Test Loss: 0.07862836122512817\n",
      "Epoch 4346, Train Loss: 0.033195432275533676, Test Loss: 0.07861971855163574\n",
      "Epoch 4347, Train Loss: 0.03318736329674721, Test Loss: 0.07860947400331497\n",
      "Epoch 4348, Train Loss: 0.03317977115511894, Test Loss: 0.07861248403787613\n",
      "Epoch 4349, Train Loss: 0.033170804381370544, Test Loss: 0.07861052453517914\n",
      "Epoch 4350, Train Loss: 0.033162813633680344, Test Loss: 0.07861558347940445\n",
      "Epoch 4351, Train Loss: 0.03315472975373268, Test Loss: 0.07860741019248962\n",
      "Epoch 4352, Train Loss: 0.033146657049655914, Test Loss: 0.07859096676111221\n",
      "Epoch 4353, Train Loss: 0.03313884511590004, Test Loss: 0.07859226316213608\n",
      "Epoch 4354, Train Loss: 0.03313051164150238, Test Loss: 0.07857512682676315\n",
      "Epoch 4355, Train Loss: 0.033122073858976364, Test Loss: 0.07858089357614517\n",
      "Epoch 4356, Train Loss: 0.03311414644122124, Test Loss: 0.07857123017311096\n",
      "Epoch 4357, Train Loss: 0.033105894923210144, Test Loss: 0.07856285572052002\n",
      "Epoch 4358, Train Loss: 0.03309768810868263, Test Loss: 0.07856793701648712\n",
      "Epoch 4359, Train Loss: 0.03308962658047676, Test Loss: 0.07857335358858109\n",
      "Epoch 4360, Train Loss: 0.03308133780956268, Test Loss: 0.0785655826330185\n",
      "Epoch 4361, Train Loss: 0.03307383880019188, Test Loss: 0.07855384796857834\n",
      "Epoch 4362, Train Loss: 0.03306615725159645, Test Loss: 0.0785498172044754\n",
      "Epoch 4363, Train Loss: 0.033058591187000275, Test Loss: 0.07854939997196198\n",
      "Epoch 4364, Train Loss: 0.033049143850803375, Test Loss: 0.07854953408241272\n",
      "Epoch 4365, Train Loss: 0.03304110839962959, Test Loss: 0.07855246216058731\n",
      "Epoch 4366, Train Loss: 0.03303277865052223, Test Loss: 0.07854950428009033\n",
      "Epoch 4367, Train Loss: 0.033024776726961136, Test Loss: 0.07854212075471878\n",
      "Epoch 4368, Train Loss: 0.03301817551255226, Test Loss: 0.07853127270936966\n",
      "Epoch 4369, Train Loss: 0.03300917148590088, Test Loss: 0.07852974534034729\n",
      "Epoch 4370, Train Loss: 0.03300038352608681, Test Loss: 0.07853483408689499\n",
      "Epoch 4371, Train Loss: 0.03299238160252571, Test Loss: 0.07852866500616074\n",
      "Epoch 4372, Train Loss: 0.03298428654670715, Test Loss: 0.0785217434167862\n",
      "Epoch 4373, Train Loss: 0.03297622874379158, Test Loss: 0.078517846763134\n",
      "Epoch 4374, Train Loss: 0.03296811133623123, Test Loss: 0.0785052627325058\n",
      "Epoch 4375, Train Loss: 0.03295992314815521, Test Loss: 0.07850528508424759\n",
      "Epoch 4376, Train Loss: 0.03295208886265755, Test Loss: 0.07848682999610901\n",
      "Epoch 4377, Train Loss: 0.03294409066438675, Test Loss: 0.07848554104566574\n",
      "Epoch 4378, Train Loss: 0.032935984432697296, Test Loss: 0.07848551124334335\n",
      "Epoch 4379, Train Loss: 0.03292791172862053, Test Loss: 0.07847565412521362\n",
      "Epoch 4380, Train Loss: 0.0329195037484169, Test Loss: 0.07848060876131058\n",
      "Epoch 4381, Train Loss: 0.03291204199194908, Test Loss: 0.07847116887569427\n",
      "Epoch 4382, Train Loss: 0.032903656363487244, Test Loss: 0.07846575230360031\n",
      "Epoch 4383, Train Loss: 0.03289583697915077, Test Loss: 0.07847986370325089\n",
      "Epoch 4384, Train Loss: 0.032887592911720276, Test Loss: 0.0784754827618599\n",
      "Epoch 4385, Train Loss: 0.032879579812288284, Test Loss: 0.07845744490623474\n",
      "Epoch 4386, Train Loss: 0.03287183493375778, Test Loss: 0.07845401018857956\n",
      "Epoch 4387, Train Loss: 0.03286391496658325, Test Loss: 0.07844426482915878\n",
      "Epoch 4388, Train Loss: 0.032856497913599014, Test Loss: 0.078450046479702\n",
      "Epoch 4389, Train Loss: 0.03284765034914017, Test Loss: 0.07844501733779907\n",
      "Epoch 4390, Train Loss: 0.03283967822790146, Test Loss: 0.07843352109193802\n",
      "Epoch 4391, Train Loss: 0.032831382006406784, Test Loss: 0.07842785865068436\n",
      "Epoch 4392, Train Loss: 0.03282338008284569, Test Loss: 0.0784202441573143\n",
      "Epoch 4393, Train Loss: 0.032815415412187576, Test Loss: 0.07842397689819336\n",
      "Epoch 4394, Train Loss: 0.03280767425894737, Test Loss: 0.07841850072145462\n",
      "Epoch 4395, Train Loss: 0.032799504697322845, Test Loss: 0.07840525358915329\n",
      "Epoch 4396, Train Loss: 0.03279182314872742, Test Loss: 0.07839841395616531\n",
      "Epoch 4397, Train Loss: 0.032783329486846924, Test Loss: 0.0784006342291832\n",
      "Epoch 4398, Train Loss: 0.032775383442640305, Test Loss: 0.07840251177549362\n",
      "Epoch 4399, Train Loss: 0.03276798501610756, Test Loss: 0.07839471101760864\n",
      "Epoch 4400, Train Loss: 0.03275914490222931, Test Loss: 0.07839975506067276\n",
      "Epoch 4401, Train Loss: 0.032751306891441345, Test Loss: 0.07839974015951157\n",
      "Epoch 4402, Train Loss: 0.03274355083703995, Test Loss: 0.07838774472475052\n",
      "Epoch 4403, Train Loss: 0.032735828310251236, Test Loss: 0.07837828248739243\n",
      "Epoch 4404, Train Loss: 0.03272736445069313, Test Loss: 0.0783800259232521\n",
      "Epoch 4405, Train Loss: 0.03271939978003502, Test Loss: 0.07837220281362534\n",
      "Epoch 4406, Train Loss: 0.0327116958796978, Test Loss: 0.07836315035820007\n",
      "Epoch 4407, Train Loss: 0.032703615725040436, Test Loss: 0.07836810499429703\n",
      "Epoch 4408, Train Loss: 0.0326961949467659, Test Loss: 0.07834836095571518\n",
      "Epoch 4409, Train Loss: 0.03268756344914436, Test Loss: 0.07835337519645691\n",
      "Epoch 4410, Train Loss: 0.032680362462997437, Test Loss: 0.07834511250257492\n",
      "Epoch 4411, Train Loss: 0.032672517001628876, Test Loss: 0.0783534124493599\n",
      "Epoch 4412, Train Loss: 0.0326639860868454, Test Loss: 0.07834360003471375\n",
      "Epoch 4413, Train Loss: 0.03265611082315445, Test Loss: 0.07833816111087799\n",
      "Epoch 4414, Train Loss: 0.03264806047081947, Test Loss: 0.0783405527472496\n",
      "Epoch 4415, Train Loss: 0.0326400026679039, Test Loss: 0.07834754139184952\n",
      "Epoch 4416, Train Loss: 0.03263210505247116, Test Loss: 0.07834900170564651\n",
      "Epoch 4417, Train Loss: 0.03262440860271454, Test Loss: 0.07833458483219147\n",
      "Epoch 4418, Train Loss: 0.03261595964431763, Test Loss: 0.07833248376846313\n",
      "Epoch 4419, Train Loss: 0.0326082818210125, Test Loss: 0.07832826673984528\n",
      "Epoch 4420, Train Loss: 0.03260069340467453, Test Loss: 0.07833030074834824\n",
      "Epoch 4421, Train Loss: 0.03259287402033806, Test Loss: 0.0783069059252739\n",
      "Epoch 4422, Train Loss: 0.03258437290787697, Test Loss: 0.07830997556447983\n",
      "Epoch 4423, Train Loss: 0.032576821744441986, Test Loss: 0.07831797003746033\n",
      "Epoch 4424, Train Loss: 0.03256875276565552, Test Loss: 0.07831604033708572\n",
      "Epoch 4425, Train Loss: 0.032560884952545166, Test Loss: 0.07830893993377686\n",
      "Epoch 4426, Train Loss: 0.032552845776081085, Test Loss: 0.07829436659812927\n",
      "Epoch 4427, Train Loss: 0.032545458525419235, Test Loss: 0.07829533517360687\n",
      "Epoch 4428, Train Loss: 0.03253760188817978, Test Loss: 0.07828384637832642\n",
      "Epoch 4429, Train Loss: 0.03252920135855675, Test Loss: 0.07828221470117569\n",
      "Epoch 4430, Train Loss: 0.032521940767765045, Test Loss: 0.07827121764421463\n",
      "Epoch 4431, Train Loss: 0.03251371905207634, Test Loss: 0.07826965302228928\n",
      "Epoch 4432, Train Loss: 0.03250635415315628, Test Loss: 0.07826270908117294\n",
      "Epoch 4433, Train Loss: 0.032497961074113846, Test Loss: 0.07826118916273117\n",
      "Epoch 4434, Train Loss: 0.032490380108356476, Test Loss: 0.07826068252325058\n",
      "Epoch 4435, Train Loss: 0.03248242288827896, Test Loss: 0.07825426012277603\n",
      "Epoch 4436, Train Loss: 0.03247445076704025, Test Loss: 0.07824820280075073\n",
      "Epoch 4437, Train Loss: 0.03246660903096199, Test Loss: 0.07824244350194931\n",
      "Epoch 4438, Train Loss: 0.0324588268995285, Test Loss: 0.0782397985458374\n",
      "Epoch 4439, Train Loss: 0.032450996339321136, Test Loss: 0.07824724912643433\n",
      "Epoch 4440, Train Loss: 0.032442983239889145, Test Loss: 0.07823993265628815\n",
      "Epoch 4441, Train Loss: 0.03243601694703102, Test Loss: 0.07823816686868668\n",
      "Epoch 4442, Train Loss: 0.03242740035057068, Test Loss: 0.07823371142148972\n",
      "Epoch 4443, Train Loss: 0.032419603317976, Test Loss: 0.07823247462511063\n",
      "Epoch 4444, Train Loss: 0.03241244703531265, Test Loss: 0.07823077589273453\n",
      "Epoch 4445, Train Loss: 0.032404497265815735, Test Loss: 0.07821467518806458\n",
      "Epoch 4446, Train Loss: 0.03239619731903076, Test Loss: 0.07822572439908981\n",
      "Epoch 4447, Train Loss: 0.032388199120759964, Test Loss: 0.07821380347013474\n",
      "Epoch 4448, Train Loss: 0.032381340861320496, Test Loss: 0.07821521162986755\n",
      "Epoch 4449, Train Loss: 0.03237280249595642, Test Loss: 0.0782066211104393\n",
      "Epoch 4450, Train Loss: 0.032365091145038605, Test Loss: 0.07820215821266174\n",
      "Epoch 4451, Train Loss: 0.032357290387153625, Test Loss: 0.07819022983312607\n",
      "Epoch 4452, Train Loss: 0.03234913945198059, Test Loss: 0.07820113003253937\n",
      "Epoch 4453, Train Loss: 0.032341890037059784, Test Loss: 0.07817988842725754\n",
      "Epoch 4454, Train Loss: 0.032333750277757645, Test Loss: 0.07818131893873215\n",
      "Epoch 4455, Train Loss: 0.03232602775096893, Test Loss: 0.07819043844938278\n",
      "Epoch 4456, Train Loss: 0.03231850638985634, Test Loss: 0.07818615436553955\n",
      "Epoch 4457, Train Loss: 0.03231063112616539, Test Loss: 0.07818103581666946\n",
      "Epoch 4458, Train Loss: 0.03230341151356697, Test Loss: 0.07817167043685913\n",
      "Epoch 4459, Train Loss: 0.032295044511556625, Test Loss: 0.07817245274782181\n",
      "Epoch 4460, Train Loss: 0.03228744491934776, Test Loss: 0.07816290110349655\n",
      "Epoch 4461, Train Loss: 0.032279059290885925, Test Loss: 0.07817266881465912\n",
      "Epoch 4462, Train Loss: 0.032272882759571075, Test Loss: 0.07816912978887558\n",
      "Epoch 4463, Train Loss: 0.03226405754685402, Test Loss: 0.07815328985452652\n",
      "Epoch 4464, Train Loss: 0.032255981117486954, Test Loss: 0.07815971970558167\n",
      "Epoch 4465, Train Loss: 0.03224889561533928, Test Loss: 0.07814418524503708\n",
      "Epoch 4466, Train Loss: 0.032241061329841614, Test Loss: 0.078133225440979\n",
      "Epoch 4467, Train Loss: 0.03223322331905365, Test Loss: 0.07811889797449112\n",
      "Epoch 4468, Train Loss: 0.03222488611936569, Test Loss: 0.07812482118606567\n",
      "Epoch 4469, Train Loss: 0.032217610627412796, Test Loss: 0.07811636477708817\n",
      "Epoch 4470, Train Loss: 0.03221031278371811, Test Loss: 0.07812200486660004\n",
      "Epoch 4471, Train Loss: 0.03220188617706299, Test Loss: 0.07811714708805084\n",
      "Epoch 4472, Train Loss: 0.0321941003203392, Test Loss: 0.0781145840883255\n",
      "Epoch 4473, Train Loss: 0.032187096774578094, Test Loss: 0.07810159027576447\n",
      "Epoch 4474, Train Loss: 0.03217900171875954, Test Loss: 0.07810104638338089\n",
      "Epoch 4475, Train Loss: 0.03217114508152008, Test Loss: 0.07809974998235703\n",
      "Epoch 4476, Train Loss: 0.03216352313756943, Test Loss: 0.07809098064899445\n",
      "Epoch 4477, Train Loss: 0.03215545788407326, Test Loss: 0.07809488475322723\n",
      "Epoch 4478, Train Loss: 0.03214806318283081, Test Loss: 0.0780915915966034\n",
      "Epoch 4479, Train Loss: 0.03214022144675255, Test Loss: 0.07809150218963623\n",
      "Epoch 4480, Train Loss: 0.032132700085639954, Test Loss: 0.07808073610067368\n",
      "Epoch 4481, Train Loss: 0.03212488442659378, Test Loss: 0.07807905972003937\n",
      "Epoch 4482, Train Loss: 0.032117024064064026, Test Loss: 0.07808572053909302\n",
      "Epoch 4483, Train Loss: 0.03210937976837158, Test Loss: 0.07809486985206604\n",
      "Epoch 4484, Train Loss: 0.032101694494485855, Test Loss: 0.07808089256286621\n",
      "Epoch 4485, Train Loss: 0.03209418058395386, Test Loss: 0.07807174324989319\n",
      "Epoch 4486, Train Loss: 0.03208618983626366, Test Loss: 0.07806987315416336\n",
      "Epoch 4487, Train Loss: 0.032078515738248825, Test Loss: 0.07805847376585007\n",
      "Epoch 4488, Train Loss: 0.032070841640233994, Test Loss: 0.07806620746850967\n",
      "Epoch 4489, Train Loss: 0.032063160091638565, Test Loss: 0.07805198431015015\n",
      "Epoch 4490, Train Loss: 0.03205560892820358, Test Loss: 0.07806093245744705\n",
      "Epoch 4491, Train Loss: 0.032047707587480545, Test Loss: 0.07805369794368744\n",
      "Epoch 4492, Train Loss: 0.032040853053331375, Test Loss: 0.07805278152227402\n",
      "Epoch 4493, Train Loss: 0.032033152878284454, Test Loss: 0.0780390128493309\n",
      "Epoch 4494, Train Loss: 0.032025113701820374, Test Loss: 0.0780409723520279\n",
      "Epoch 4495, Train Loss: 0.032017502933740616, Test Loss: 0.07802823185920715\n",
      "Epoch 4496, Train Loss: 0.03201012685894966, Test Loss: 0.07801772654056549\n",
      "Epoch 4497, Train Loss: 0.03200249746441841, Test Loss: 0.07801889628171921\n",
      "Epoch 4498, Train Loss: 0.0319947749376297, Test Loss: 0.07801562547683716\n",
      "Epoch 4499, Train Loss: 0.03198760002851486, Test Loss: 0.07800398766994476\n",
      "Epoch 4500, Train Loss: 0.031979501247406006, Test Loss: 0.07800749689340591\n",
      "Epoch 4501, Train Loss: 0.031973015516996384, Test Loss: 0.07801324129104614\n",
      "Epoch 4502, Train Loss: 0.03196392580866814, Test Loss: 0.07800654321908951\n",
      "Epoch 4503, Train Loss: 0.03195655718445778, Test Loss: 0.07800150662660599\n",
      "Epoch 4504, Train Loss: 0.031948722898960114, Test Loss: 0.07800152897834778\n",
      "Epoch 4505, Train Loss: 0.03194088116288185, Test Loss: 0.07800573855638504\n",
      "Epoch 4506, Train Loss: 0.03193371742963791, Test Loss: 0.07798340171575546\n",
      "Epoch 4507, Train Loss: 0.03192596510052681, Test Loss: 0.0779825747013092\n",
      "Epoch 4508, Train Loss: 0.03191835805773735, Test Loss: 0.07798992097377777\n",
      "Epoch 4509, Train Loss: 0.031910963356494904, Test Loss: 0.07798076421022415\n",
      "Epoch 4510, Train Loss: 0.031903643161058426, Test Loss: 0.07796341180801392\n",
      "Epoch 4511, Train Loss: 0.03189530596137047, Test Loss: 0.07796889543533325\n",
      "Epoch 4512, Train Loss: 0.031887784600257874, Test Loss: 0.0779748409986496\n",
      "Epoch 4513, Train Loss: 0.031880274415016174, Test Loss: 0.07796380668878555\n",
      "Epoch 4514, Train Loss: 0.03187284991145134, Test Loss: 0.07796058803796768\n",
      "Epoch 4515, Train Loss: 0.031865183264017105, Test Loss: 0.07795495539903641\n",
      "Epoch 4516, Train Loss: 0.03185725957155228, Test Loss: 0.0779610350728035\n",
      "Epoch 4517, Train Loss: 0.0318499431014061, Test Loss: 0.07795574516057968\n",
      "Epoch 4518, Train Loss: 0.03184274584054947, Test Loss: 0.0779530331492424\n",
      "Epoch 4519, Train Loss: 0.031834736466407776, Test Loss: 0.0779581218957901\n",
      "Epoch 4520, Train Loss: 0.03182775899767876, Test Loss: 0.07794792205095291\n",
      "Epoch 4521, Train Loss: 0.031819798052310944, Test Loss: 0.0779530331492424\n",
      "Epoch 4522, Train Loss: 0.0318123921751976, Test Loss: 0.07794103026390076\n",
      "Epoch 4523, Train Loss: 0.03180467709898949, Test Loss: 0.0779305174946785\n",
      "Epoch 4524, Train Loss: 0.03179692476987839, Test Loss: 0.07793951779603958\n",
      "Epoch 4525, Train Loss: 0.031789809465408325, Test Loss: 0.07793274521827698\n",
      "Epoch 4526, Train Loss: 0.03178213909268379, Test Loss: 0.07791613787412643\n",
      "Epoch 4527, Train Loss: 0.03177510201931, Test Loss: 0.07791843265295029\n",
      "Epoch 4528, Train Loss: 0.03176706284284592, Test Loss: 0.0779033824801445\n",
      "Epoch 4529, Train Loss: 0.03175922855734825, Test Loss: 0.07790420204401016\n",
      "Epoch 4530, Train Loss: 0.03175199031829834, Test Loss: 0.07790745794773102\n",
      "Epoch 4531, Train Loss: 0.03174459561705589, Test Loss: 0.07788922637701035\n",
      "Epoch 4532, Train Loss: 0.03173688426613808, Test Loss: 0.07788638770580292\n",
      "Epoch 4533, Train Loss: 0.031729720532894135, Test Loss: 0.07788461446762085\n",
      "Epoch 4534, Train Loss: 0.031721677631139755, Test Loss: 0.07788603007793427\n",
      "Epoch 4535, Train Loss: 0.03171481564640999, Test Loss: 0.07787466049194336\n",
      "Epoch 4536, Train Loss: 0.03170657902956009, Test Loss: 0.07788368314504623\n",
      "Epoch 4537, Train Loss: 0.0316990502178669, Test Loss: 0.07787544280290604\n",
      "Epoch 4538, Train Loss: 0.0316917821764946, Test Loss: 0.0778692215681076\n",
      "Epoch 4539, Train Loss: 0.03168438374996185, Test Loss: 0.07786351442337036\n",
      "Epoch 4540, Train Loss: 0.0316770114004612, Test Loss: 0.07786376029253006\n",
      "Epoch 4541, Train Loss: 0.03166932240128517, Test Loss: 0.07784764468669891\n",
      "Epoch 4542, Train Loss: 0.031661976128816605, Test Loss: 0.07784324884414673\n",
      "Epoch 4543, Train Loss: 0.031654007732868195, Test Loss: 0.07784733176231384\n",
      "Epoch 4544, Train Loss: 0.03164660930633545, Test Loss: 0.07784850150346756\n",
      "Epoch 4545, Train Loss: 0.03163948282599449, Test Loss: 0.07783529162406921\n",
      "Epoch 4546, Train Loss: 0.03163183480501175, Test Loss: 0.07784610241651535\n",
      "Epoch 4547, Train Loss: 0.03162478655576706, Test Loss: 0.07783900946378708\n",
      "Epoch 4548, Train Loss: 0.031616922467947006, Test Loss: 0.0778425857424736\n",
      "Epoch 4549, Train Loss: 0.03160995617508888, Test Loss: 0.07783191651105881\n",
      "Epoch 4550, Train Loss: 0.031602371484041214, Test Loss: 0.0778222605586052\n",
      "Epoch 4551, Train Loss: 0.03159491345286369, Test Loss: 0.07781454920768738\n",
      "Epoch 4552, Train Loss: 0.03158734366297722, Test Loss: 0.07780986279249191\n",
      "Epoch 4553, Train Loss: 0.03157954290509224, Test Loss: 0.07781089842319489\n",
      "Epoch 4554, Train Loss: 0.03157229721546173, Test Loss: 0.07780733704566956\n",
      "Epoch 4555, Train Loss: 0.03156517818570137, Test Loss: 0.07779844850301743\n",
      "Epoch 4556, Train Loss: 0.031557630747556686, Test Loss: 0.07779489457607269\n",
      "Epoch 4557, Train Loss: 0.03155037388205528, Test Loss: 0.07778775691986084\n",
      "Epoch 4558, Train Loss: 0.03154265508055687, Test Loss: 0.07779773324728012\n",
      "Epoch 4559, Train Loss: 0.031535129994153976, Test Loss: 0.07778897136449814\n",
      "Epoch 4560, Train Loss: 0.03152808919548988, Test Loss: 0.07778330147266388\n",
      "Epoch 4561, Train Loss: 0.0315212719142437, Test Loss: 0.07777729630470276\n",
      "Epoch 4562, Train Loss: 0.031513214111328125, Test Loss: 0.0777742937207222\n",
      "Epoch 4563, Train Loss: 0.03150567412376404, Test Loss: 0.0777815729379654\n",
      "Epoch 4564, Train Loss: 0.03149879351258278, Test Loss: 0.07778371125459671\n",
      "Epoch 4565, Train Loss: 0.03149097040295601, Test Loss: 0.07777202874422073\n",
      "Epoch 4566, Train Loss: 0.031483881175518036, Test Loss: 0.0777665376663208\n",
      "Epoch 4567, Train Loss: 0.03147628530859947, Test Loss: 0.07775774598121643\n",
      "Epoch 4568, Train Loss: 0.031468916684389114, Test Loss: 0.07777254283428192\n",
      "Epoch 4569, Train Loss: 0.03146112710237503, Test Loss: 0.07775967568159103\n",
      "Epoch 4570, Train Loss: 0.03145454823970795, Test Loss: 0.07776965945959091\n",
      "Epoch 4571, Train Loss: 0.0314469151198864, Test Loss: 0.07775258272886276\n",
      "Epoch 4572, Train Loss: 0.031439024955034256, Test Loss: 0.07774916291236877\n",
      "Epoch 4573, Train Loss: 0.031432345509529114, Test Loss: 0.07773735374212265\n",
      "Epoch 4574, Train Loss: 0.03142435476183891, Test Loss: 0.0777408629655838\n",
      "Epoch 4575, Train Loss: 0.03141721338033676, Test Loss: 0.0777345597743988\n",
      "Epoch 4576, Train Loss: 0.03141026571393013, Test Loss: 0.07772897183895111\n",
      "Epoch 4577, Train Loss: 0.031402457505464554, Test Loss: 0.07773143798112869\n",
      "Epoch 4578, Train Loss: 0.031395308673381805, Test Loss: 0.07773088663816452\n",
      "Epoch 4579, Train Loss: 0.0313885472714901, Test Loss: 0.0777241513133049\n",
      "Epoch 4580, Train Loss: 0.03137994557619095, Test Loss: 0.07772158831357956\n",
      "Epoch 4581, Train Loss: 0.03137265518307686, Test Loss: 0.07772979140281677\n",
      "Epoch 4582, Train Loss: 0.031365666538476944, Test Loss: 0.07771851867437363\n",
      "Epoch 4583, Train Loss: 0.03135807067155838, Test Loss: 0.07770447432994843\n",
      "Epoch 4584, Train Loss: 0.03135092929005623, Test Loss: 0.07769438624382019\n",
      "Epoch 4585, Train Loss: 0.03134353086352348, Test Loss: 0.0776875764131546\n",
      "Epoch 4586, Train Loss: 0.03133603557944298, Test Loss: 0.07769505679607391\n",
      "Epoch 4587, Train Loss: 0.03132878988981247, Test Loss: 0.07770051807165146\n",
      "Epoch 4588, Train Loss: 0.03132125362753868, Test Loss: 0.07769985496997833\n",
      "Epoch 4589, Train Loss: 0.03131404519081116, Test Loss: 0.07769417017698288\n",
      "Epoch 4590, Train Loss: 0.031306542456150055, Test Loss: 0.07769045233726501\n",
      "Epoch 4591, Train Loss: 0.03129956126213074, Test Loss: 0.07768791913986206\n",
      "Epoch 4592, Train Loss: 0.031291935592889786, Test Loss: 0.07768614590167999\n",
      "Epoch 4593, Train Loss: 0.03128453344106674, Test Loss: 0.07767896354198456\n",
      "Epoch 4594, Train Loss: 0.03127714991569519, Test Loss: 0.07766720652580261\n",
      "Epoch 4595, Train Loss: 0.031270015984773636, Test Loss: 0.0776582881808281\n",
      "Epoch 4596, Train Loss: 0.03126272186636925, Test Loss: 0.07765140384435654\n",
      "Epoch 4597, Train Loss: 0.03125597909092903, Test Loss: 0.07764650136232376\n",
      "Epoch 4598, Train Loss: 0.031249241903424263, Test Loss: 0.07763756066560745\n",
      "Epoch 4599, Train Loss: 0.031241051852703094, Test Loss: 0.07765260338783264\n",
      "Epoch 4600, Train Loss: 0.031233910471200943, Test Loss: 0.07765033096075058\n",
      "Epoch 4601, Train Loss: 0.03122630901634693, Test Loss: 0.07763791084289551\n",
      "Epoch 4602, Train Loss: 0.031218938529491425, Test Loss: 0.07762723416090012\n",
      "Epoch 4603, Train Loss: 0.031212320551276207, Test Loss: 0.07761900871992111\n",
      "Epoch 4604, Train Loss: 0.0312042236328125, Test Loss: 0.07762489467859268\n",
      "Epoch 4605, Train Loss: 0.031196866184473038, Test Loss: 0.07762624323368073\n",
      "Epoch 4606, Train Loss: 0.031190281733870506, Test Loss: 0.07763300091028214\n",
      "Epoch 4607, Train Loss: 0.031182382255792618, Test Loss: 0.07761626690626144\n",
      "Epoch 4608, Train Loss: 0.0311758853495121, Test Loss: 0.07759936898946762\n",
      "Epoch 4609, Train Loss: 0.03116818331182003, Test Loss: 0.07759939134120941\n",
      "Epoch 4610, Train Loss: 0.031160658225417137, Test Loss: 0.0775955468416214\n",
      "Epoch 4611, Train Loss: 0.03115321509540081, Test Loss: 0.07760395854711533\n",
      "Epoch 4612, Train Loss: 0.031146084889769554, Test Loss: 0.07759743183851242\n",
      "Epoch 4613, Train Loss: 0.03113924339413643, Test Loss: 0.07758849114179611\n",
      "Epoch 4614, Train Loss: 0.031132277101278305, Test Loss: 0.07758776098489761\n",
      "Epoch 4615, Train Loss: 0.031125694513320923, Test Loss: 0.07757286727428436\n",
      "Epoch 4616, Train Loss: 0.0311170294880867, Test Loss: 0.07758262753486633\n",
      "Epoch 4617, Train Loss: 0.031110353767871857, Test Loss: 0.07756175845861435\n",
      "Epoch 4618, Train Loss: 0.031104082241654396, Test Loss: 0.07756239920854568\n",
      "Epoch 4619, Train Loss: 0.03109510987997055, Test Loss: 0.07756398618221283\n",
      "Epoch 4620, Train Loss: 0.031088152900338173, Test Loss: 0.07757344841957092\n",
      "Epoch 4621, Train Loss: 0.03108111210167408, Test Loss: 0.07755861431360245\n",
      "Epoch 4622, Train Loss: 0.031073473393917084, Test Loss: 0.07757335156202316\n",
      "Epoch 4623, Train Loss: 0.031066769734025, Test Loss: 0.07756543159484863\n",
      "Epoch 4624, Train Loss: 0.03105977363884449, Test Loss: 0.07754528522491455\n",
      "Epoch 4625, Train Loss: 0.031051594763994217, Test Loss: 0.07755097001791\n",
      "Epoch 4626, Train Loss: 0.03104453720152378, Test Loss: 0.07754968851804733\n",
      "Epoch 4627, Train Loss: 0.031037861481308937, Test Loss: 0.07753179222345352\n",
      "Epoch 4628, Train Loss: 0.031030336394906044, Test Loss: 0.07753897458314896\n",
      "Epoch 4629, Train Loss: 0.031022923067212105, Test Loss: 0.07753204554319382\n",
      "Epoch 4630, Train Loss: 0.03101615607738495, Test Loss: 0.07752888649702072\n",
      "Epoch 4631, Train Loss: 0.031008576974272728, Test Loss: 0.07753244787454605\n",
      "Epoch 4632, Train Loss: 0.03100234642624855, Test Loss: 0.07752177864313126\n",
      "Epoch 4633, Train Loss: 0.030994031578302383, Test Loss: 0.077521912753582\n",
      "Epoch 4634, Train Loss: 0.030987953767180443, Test Loss: 0.0775243267416954\n",
      "Epoch 4635, Train Loss: 0.03098074533045292, Test Loss: 0.07751710712909698\n",
      "Epoch 4636, Train Loss: 0.03097253106534481, Test Loss: 0.07751231640577316\n",
      "Epoch 4637, Train Loss: 0.030965682119131088, Test Loss: 0.07752290368080139\n",
      "Epoch 4638, Train Loss: 0.03095824643969536, Test Loss: 0.07752042263746262\n",
      "Epoch 4639, Train Loss: 0.030951283872127533, Test Loss: 0.07751912623643875\n",
      "Epoch 4640, Train Loss: 0.030944298952817917, Test Loss: 0.07752104103565216\n",
      "Epoch 4641, Train Loss: 0.03093690238893032, Test Loss: 0.07751281559467316\n",
      "Epoch 4642, Train Loss: 0.03092958964407444, Test Loss: 0.07750654965639114\n",
      "Epoch 4643, Train Loss: 0.03092283383011818, Test Loss: 0.07751014828681946\n",
      "Epoch 4644, Train Loss: 0.030915100127458572, Test Loss: 0.07749774307012558\n",
      "Epoch 4645, Train Loss: 0.030908389016985893, Test Loss: 0.07748153805732727\n",
      "Epoch 4646, Train Loss: 0.03090130351483822, Test Loss: 0.07748366892337799\n",
      "Epoch 4647, Train Loss: 0.030893657356500626, Test Loss: 0.07748203724622726\n",
      "Epoch 4648, Train Loss: 0.030886443331837654, Test Loss: 0.07748370617628098\n",
      "Epoch 4649, Train Loss: 0.03087959811091423, Test Loss: 0.0774836465716362\n",
      "Epoch 4650, Train Loss: 0.03087260201573372, Test Loss: 0.07747063040733337\n",
      "Epoch 4651, Train Loss: 0.030865000560879707, Test Loss: 0.07746972143650055\n",
      "Epoch 4652, Train Loss: 0.03085782378911972, Test Loss: 0.07746883481740952\n",
      "Epoch 4653, Train Loss: 0.030851468443870544, Test Loss: 0.07744840532541275\n",
      "Epoch 4654, Train Loss: 0.030844375491142273, Test Loss: 0.07744289189577103\n",
      "Epoch 4655, Train Loss: 0.03083624131977558, Test Loss: 0.07744575291872025\n",
      "Epoch 4656, Train Loss: 0.030829476192593575, Test Loss: 0.07744549959897995\n",
      "Epoch 4657, Train Loss: 0.030822351574897766, Test Loss: 0.07743362337350845\n",
      "Epoch 4658, Train Loss: 0.030815057456493378, Test Loss: 0.07744013518095016\n",
      "Epoch 4659, Train Loss: 0.030808307230472565, Test Loss: 0.0774449035525322\n",
      "Epoch 4660, Train Loss: 0.030801059678196907, Test Loss: 0.07742845267057419\n",
      "Epoch 4661, Train Loss: 0.030793610960245132, Test Loss: 0.07742029428482056\n",
      "Epoch 4662, Train Loss: 0.030786532908678055, Test Loss: 0.07742135971784592\n",
      "Epoch 4663, Train Loss: 0.030779240652918816, Test Loss: 0.07742717117071152\n",
      "Epoch 4664, Train Loss: 0.030772283673286438, Test Loss: 0.07742387801408768\n",
      "Epoch 4665, Train Loss: 0.030765080824494362, Test Loss: 0.07741189748048782\n",
      "Epoch 4666, Train Loss: 0.030758000910282135, Test Loss: 0.07740480452775955\n",
      "Epoch 4667, Train Loss: 0.030750682577490807, Test Loss: 0.0773998573422432\n",
      "Epoch 4668, Train Loss: 0.030743828043341637, Test Loss: 0.07741725444793701\n",
      "Epoch 4669, Train Loss: 0.030736710876226425, Test Loss: 0.07740164548158646\n",
      "Epoch 4670, Train Loss: 0.030729802325367928, Test Loss: 0.0774005800485611\n",
      "Epoch 4671, Train Loss: 0.03072243556380272, Test Loss: 0.07739634811878204\n",
      "Epoch 4672, Train Loss: 0.030716702342033386, Test Loss: 0.07737228274345398\n",
      "Epoch 4673, Train Loss: 0.030708428472280502, Test Loss: 0.07738388329744339\n",
      "Epoch 4674, Train Loss: 0.030701497569680214, Test Loss: 0.07738089561462402\n",
      "Epoch 4675, Train Loss: 0.03069402649998665, Test Loss: 0.0773935467004776\n",
      "Epoch 4676, Train Loss: 0.030687982216477394, Test Loss: 0.07737788558006287\n",
      "Epoch 4677, Train Loss: 0.030680179595947266, Test Loss: 0.0773811787366867\n",
      "Epoch 4678, Train Loss: 0.030672941356897354, Test Loss: 0.07737839221954346\n",
      "Epoch 4679, Train Loss: 0.0306658074259758, Test Loss: 0.07736912369728088\n",
      "Epoch 4680, Train Loss: 0.030658993870019913, Test Loss: 0.07736135274171829\n",
      "Epoch 4681, Train Loss: 0.030651802197098732, Test Loss: 0.07735247910022736\n",
      "Epoch 4682, Train Loss: 0.030645085498690605, Test Loss: 0.07735384255647659\n",
      "Epoch 4683, Train Loss: 0.030638128519058228, Test Loss: 0.07733947783708572\n",
      "Epoch 4684, Train Loss: 0.030630750581622124, Test Loss: 0.07734236121177673\n",
      "Epoch 4685, Train Loss: 0.030623944476246834, Test Loss: 0.0773409977555275\n",
      "Epoch 4686, Train Loss: 0.03061750903725624, Test Loss: 0.07735132426023483\n",
      "Epoch 4687, Train Loss: 0.03060995601117611, Test Loss: 0.0773482546210289\n",
      "Epoch 4688, Train Loss: 0.030603494495153427, Test Loss: 0.07734718918800354\n",
      "Epoch 4689, Train Loss: 0.03059573471546173, Test Loss: 0.07733325660228729\n",
      "Epoch 4690, Train Loss: 0.030588969588279724, Test Loss: 0.07733827829360962\n",
      "Epoch 4691, Train Loss: 0.030581844970583916, Test Loss: 0.07733581960201263\n",
      "Epoch 4692, Train Loss: 0.03057580627501011, Test Loss: 0.07731993496417999\n",
      "Epoch 4693, Train Loss: 0.03056780807673931, Test Loss: 0.07732806354761124\n",
      "Epoch 4694, Train Loss: 0.030561070889234543, Test Loss: 0.07732861489057541\n",
      "Epoch 4695, Train Loss: 0.030553920194506645, Test Loss: 0.07730332016944885\n",
      "Epoch 4696, Train Loss: 0.030547412112355232, Test Loss: 0.07729790359735489\n",
      "Epoch 4697, Train Loss: 0.030540157109498978, Test Loss: 0.07730447500944138\n",
      "Epoch 4698, Train Loss: 0.030532795935869217, Test Loss: 0.07730957120656967\n",
      "Epoch 4699, Train Loss: 0.030526116490364075, Test Loss: 0.07731304317712784\n",
      "Epoch 4700, Train Loss: 0.030519532039761543, Test Loss: 0.07731027901172638\n",
      "Epoch 4701, Train Loss: 0.030512209981679916, Test Loss: 0.07729917764663696\n",
      "Epoch 4702, Train Loss: 0.030506132170557976, Test Loss: 0.07729052752256393\n",
      "Epoch 4703, Train Loss: 0.030498739331960678, Test Loss: 0.07727238535881042\n",
      "Epoch 4704, Train Loss: 0.030491536483168602, Test Loss: 0.07726916670799255\n",
      "Epoch 4705, Train Loss: 0.03048447147011757, Test Loss: 0.07727484405040741\n",
      "Epoch 4706, Train Loss: 0.030477363616228104, Test Loss: 0.07726489752531052\n",
      "Epoch 4707, Train Loss: 0.03047047182917595, Test Loss: 0.0772584080696106\n",
      "Epoch 4708, Train Loss: 0.030463585630059242, Test Loss: 0.07725110650062561\n",
      "Epoch 4709, Train Loss: 0.030456596985459328, Test Loss: 0.07725775241851807\n",
      "Epoch 4710, Train Loss: 0.03044995665550232, Test Loss: 0.07725822925567627\n",
      "Epoch 4711, Train Loss: 0.03044407069683075, Test Loss: 0.0772412121295929\n",
      "Epoch 4712, Train Loss: 0.030436009168624878, Test Loss: 0.07724147289991379\n",
      "Epoch 4713, Train Loss: 0.030428780242800713, Test Loss: 0.07724626362323761\n",
      "Epoch 4714, Train Loss: 0.030422361567616463, Test Loss: 0.07723145186901093\n",
      "Epoch 4715, Train Loss: 0.030414871871471405, Test Loss: 0.07723751664161682\n",
      "Epoch 4716, Train Loss: 0.030408460646867752, Test Loss: 0.07722951471805573\n",
      "Epoch 4717, Train Loss: 0.030401460826396942, Test Loss: 0.07722534984350204\n",
      "Epoch 4718, Train Loss: 0.03039458952844143, Test Loss: 0.07723288238048553\n",
      "Epoch 4719, Train Loss: 0.03038734570145607, Test Loss: 0.07722534984350204\n",
      "Epoch 4720, Train Loss: 0.03038046322762966, Test Loss: 0.07723119109869003\n",
      "Epoch 4721, Train Loss: 0.030373573303222656, Test Loss: 0.07722829282283783\n",
      "Epoch 4722, Train Loss: 0.030367037281394005, Test Loss: 0.07720756530761719\n",
      "Epoch 4723, Train Loss: 0.03035997785627842, Test Loss: 0.07722117751836777\n",
      "Epoch 4724, Train Loss: 0.03035302460193634, Test Loss: 0.07721695303916931\n",
      "Epoch 4725, Train Loss: 0.030346471816301346, Test Loss: 0.077208511531353\n",
      "Epoch 4726, Train Loss: 0.030340071767568588, Test Loss: 0.07719166576862335\n",
      "Epoch 4727, Train Loss: 0.030332395806908607, Test Loss: 0.07719948142766953\n",
      "Epoch 4728, Train Loss: 0.03032560460269451, Test Loss: 0.07720008492469788\n",
      "Epoch 4729, Train Loss: 0.03031848929822445, Test Loss: 0.07719682902097702\n",
      "Epoch 4730, Train Loss: 0.030311543494462967, Test Loss: 0.0772005021572113\n",
      "Epoch 4731, Train Loss: 0.030304810032248497, Test Loss: 0.07720024883747101\n",
      "Epoch 4732, Train Loss: 0.030298219993710518, Test Loss: 0.07719195634126663\n",
      "Epoch 4733, Train Loss: 0.030290761962532997, Test Loss: 0.07718407362699509\n",
      "Epoch 4734, Train Loss: 0.03028474561870098, Test Loss: 0.07716717571020126\n",
      "Epoch 4735, Train Loss: 0.030277512967586517, Test Loss: 0.07717079669237137\n",
      "Epoch 4736, Train Loss: 0.030270319432020187, Test Loss: 0.07717429101467133\n",
      "Epoch 4737, Train Loss: 0.030263958498835564, Test Loss: 0.07715387642383575\n",
      "Epoch 4738, Train Loss: 0.030257022008299828, Test Loss: 0.07715871185064316\n",
      "Epoch 4739, Train Loss: 0.030249761417508125, Test Loss: 0.07715799659490585\n",
      "Epoch 4740, Train Loss: 0.0302432831376791, Test Loss: 0.07715509831905365\n",
      "Epoch 4741, Train Loss: 0.030236152932047844, Test Loss: 0.07715066522359848\n",
      "Epoch 4742, Train Loss: 0.030229268595576286, Test Loss: 0.07716076821088791\n",
      "Epoch 4743, Train Loss: 0.03022228367626667, Test Loss: 0.07716324925422668\n",
      "Epoch 4744, Train Loss: 0.030215566977858543, Test Loss: 0.0771501436829567\n",
      "Epoch 4745, Train Loss: 0.030208658427000046, Test Loss: 0.0771445631980896\n",
      "Epoch 4746, Train Loss: 0.030202841386198997, Test Loss: 0.0771271213889122\n",
      "Epoch 4747, Train Loss: 0.03019610419869423, Test Loss: 0.07711727917194366\n",
      "Epoch 4748, Train Loss: 0.030188316479325294, Test Loss: 0.07712150365114212\n",
      "Epoch 4749, Train Loss: 0.030181603506207466, Test Loss: 0.07712014764547348\n",
      "Epoch 4750, Train Loss: 0.030174894258379936, Test Loss: 0.07711662352085114\n",
      "Epoch 4751, Train Loss: 0.03016786277294159, Test Loss: 0.07711926102638245\n",
      "Epoch 4752, Train Loss: 0.030160928145051003, Test Loss: 0.07712266594171524\n",
      "Epoch 4753, Train Loss: 0.030154317617416382, Test Loss: 0.07712181657552719\n",
      "Epoch 4754, Train Loss: 0.030148174613714218, Test Loss: 0.077113576233387\n",
      "Epoch 4755, Train Loss: 0.030140701681375504, Test Loss: 0.07710042595863342\n",
      "Epoch 4756, Train Loss: 0.030134467408061028, Test Loss: 0.07708647102117538\n",
      "Epoch 4757, Train Loss: 0.030126772820949554, Test Loss: 0.07709914445877075\n",
      "Epoch 4758, Train Loss: 0.030120348557829857, Test Loss: 0.07709605991840363\n",
      "Epoch 4759, Train Loss: 0.030113693326711655, Test Loss: 0.07709527760744095\n",
      "Epoch 4760, Train Loss: 0.030106721445918083, Test Loss: 0.07708358764648438\n",
      "Epoch 4761, Train Loss: 0.030099906027317047, Test Loss: 0.07709648460149765\n",
      "Epoch 4762, Train Loss: 0.030093256384134293, Test Loss: 0.07709099352359772\n",
      "Epoch 4763, Train Loss: 0.03008672408759594, Test Loss: 0.07707422226667404\n",
      "Epoch 4764, Train Loss: 0.03007936105132103, Test Loss: 0.07707969099283218\n",
      "Epoch 4765, Train Loss: 0.03007318452000618, Test Loss: 0.07708295434713364\n",
      "Epoch 4766, Train Loss: 0.030066106468439102, Test Loss: 0.07707487791776657\n",
      "Epoch 4767, Train Loss: 0.03005916438996792, Test Loss: 0.07706990838050842\n",
      "Epoch 4768, Train Loss: 0.030052635818719864, Test Loss: 0.07707635313272476\n",
      "Epoch 4769, Train Loss: 0.03004639223217964, Test Loss: 0.07706303894519806\n",
      "Epoch 4770, Train Loss: 0.030039018020033836, Test Loss: 0.0770576223731041\n",
      "Epoch 4771, Train Loss: 0.030032269656658173, Test Loss: 0.07704970985651016\n",
      "Epoch 4772, Train Loss: 0.030025580897927284, Test Loss: 0.07703899592161179\n",
      "Epoch 4773, Train Loss: 0.030018700286746025, Test Loss: 0.07705697417259216\n",
      "Epoch 4774, Train Loss: 0.03001212701201439, Test Loss: 0.07706143707036972\n",
      "Epoch 4775, Train Loss: 0.030005402863025665, Test Loss: 0.07704654335975647\n",
      "Epoch 4776, Train Loss: 0.029998328536748886, Test Loss: 0.07705327123403549\n",
      "Epoch 4777, Train Loss: 0.029991673305630684, Test Loss: 0.07703500986099243\n",
      "Epoch 4778, Train Loss: 0.0299850981682539, Test Loss: 0.07703112065792084\n",
      "Epoch 4779, Train Loss: 0.029978249222040176, Test Loss: 0.07702773809432983\n",
      "Epoch 4780, Train Loss: 0.029971864074468613, Test Loss: 0.07701893895864487\n",
      "Epoch 4781, Train Loss: 0.029965901747345924, Test Loss: 0.07701633125543594\n",
      "Epoch 4782, Train Loss: 0.029958896338939667, Test Loss: 0.07700436562299728\n",
      "Epoch 4783, Train Loss: 0.029951510950922966, Test Loss: 0.07701439410448074\n",
      "Epoch 4784, Train Loss: 0.029944898560643196, Test Loss: 0.0770067423582077\n",
      "Epoch 4785, Train Loss: 0.02993815392255783, Test Loss: 0.07701633125543594\n",
      "Epoch 4786, Train Loss: 0.02993091754615307, Test Loss: 0.07701466977596283\n",
      "Epoch 4787, Train Loss: 0.029924610629677773, Test Loss: 0.0770157054066658\n",
      "Epoch 4788, Train Loss: 0.029917791485786438, Test Loss: 0.07701293379068375\n",
      "Epoch 4789, Train Loss: 0.029911942780017853, Test Loss: 0.0770099088549614\n",
      "Epoch 4790, Train Loss: 0.029904255643486977, Test Loss: 0.07700969278812408\n",
      "Epoch 4791, Train Loss: 0.029897643253207207, Test Loss: 0.07700981944799423\n",
      "Epoch 4792, Train Loss: 0.029891900718212128, Test Loss: 0.07701283693313599\n",
      "Epoch 4793, Train Loss: 0.02988434210419655, Test Loss: 0.07700614631175995\n",
      "Epoch 4794, Train Loss: 0.029878243803977966, Test Loss: 0.07699136435985565\n",
      "Epoch 4795, Train Loss: 0.029870908707380295, Test Loss: 0.07698693871498108\n",
      "Epoch 4796, Train Loss: 0.029864223673939705, Test Loss: 0.07697713375091553\n",
      "Epoch 4797, Train Loss: 0.029857823625206947, Test Loss: 0.07697690278291702\n",
      "Epoch 4798, Train Loss: 0.029850510880351067, Test Loss: 0.07697322219610214\n",
      "Epoch 4799, Train Loss: 0.029845286160707474, Test Loss: 0.07697951793670654\n",
      "Epoch 4800, Train Loss: 0.02983729913830757, Test Loss: 0.07696918398141861\n",
      "Epoch 4801, Train Loss: 0.02983076311647892, Test Loss: 0.07697229087352753\n",
      "Epoch 4802, Train Loss: 0.029823873192071915, Test Loss: 0.07696884870529175\n",
      "Epoch 4803, Train Loss: 0.029818065464496613, Test Loss: 0.0769711509346962\n",
      "Epoch 4804, Train Loss: 0.02981049194931984, Test Loss: 0.07695656269788742\n",
      "Epoch 4805, Train Loss: 0.029804015532135963, Test Loss: 0.0769445076584816\n",
      "Epoch 4806, Train Loss: 0.029797907918691635, Test Loss: 0.07693242281675339\n",
      "Epoch 4807, Train Loss: 0.02979060634970665, Test Loss: 0.07693268358707428\n",
      "Epoch 4808, Train Loss: 0.029783928766846657, Test Loss: 0.07694252580404282\n",
      "Epoch 4809, Train Loss: 0.02977706678211689, Test Loss: 0.0769399031996727\n",
      "Epoch 4810, Train Loss: 0.029771344736218452, Test Loss: 0.07693901658058167\n",
      "Epoch 4811, Train Loss: 0.029764071106910706, Test Loss: 0.07693354040384293\n",
      "Epoch 4812, Train Loss: 0.029757564887404442, Test Loss: 0.07692038267850876\n",
      "Epoch 4813, Train Loss: 0.029750721529126167, Test Loss: 0.07692166417837143\n",
      "Epoch 4814, Train Loss: 0.029744569212198257, Test Loss: 0.07693333923816681\n",
      "Epoch 4815, Train Loss: 0.02973705157637596, Test Loss: 0.07692119479179382\n",
      "Epoch 4816, Train Loss: 0.029730450361967087, Test Loss: 0.07691952586174011\n",
      "Epoch 4817, Train Loss: 0.029725147411227226, Test Loss: 0.07691282033920288\n",
      "Epoch 4818, Train Loss: 0.029717134311795235, Test Loss: 0.07691070437431335\n",
      "Epoch 4819, Train Loss: 0.02971053309738636, Test Loss: 0.07691942155361176\n",
      "Epoch 4820, Train Loss: 0.029704347252845764, Test Loss: 0.07692079246044159\n",
      "Epoch 4821, Train Loss: 0.029697483405470848, Test Loss: 0.07690270990133286\n",
      "Epoch 4822, Train Loss: 0.029690399765968323, Test Loss: 0.07692122459411621\n",
      "Epoch 4823, Train Loss: 0.029684502631425858, Test Loss: 0.07691627740859985\n",
      "Epoch 4824, Train Loss: 0.029678048565983772, Test Loss: 0.0768975242972374\n",
      "Epoch 4825, Train Loss: 0.029672089964151382, Test Loss: 0.07688678801059723\n",
      "Epoch 4826, Train Loss: 0.029663776978850365, Test Loss: 0.07688982039690018\n",
      "Epoch 4827, Train Loss: 0.029657211154699326, Test Loss: 0.07688391208648682\n",
      "Epoch 4828, Train Loss: 0.029650947079062462, Test Loss: 0.07688018679618835\n",
      "Epoch 4829, Train Loss: 0.02964436449110508, Test Loss: 0.0768708661198616\n",
      "Epoch 4830, Train Loss: 0.029637163504958153, Test Loss: 0.0768701359629631\n",
      "Epoch 4831, Train Loss: 0.02963079884648323, Test Loss: 0.07686341553926468\n",
      "Epoch 4832, Train Loss: 0.02962411195039749, Test Loss: 0.07686160504817963\n",
      "Epoch 4833, Train Loss: 0.02961733005940914, Test Loss: 0.07687241584062576\n",
      "Epoch 4834, Train Loss: 0.029611414298415184, Test Loss: 0.07686296105384827\n",
      "Epoch 4835, Train Loss: 0.029604662209749222, Test Loss: 0.07685135304927826\n",
      "Epoch 4836, Train Loss: 0.029597986489534378, Test Loss: 0.076854407787323\n",
      "Epoch 4837, Train Loss: 0.029591184109449387, Test Loss: 0.0768633484840393\n",
      "Epoch 4838, Train Loss: 0.029584506526589394, Test Loss: 0.07686556130647659\n",
      "Epoch 4839, Train Loss: 0.029577787965536118, Test Loss: 0.07685748487710953\n",
      "Epoch 4840, Train Loss: 0.02957126311957836, Test Loss: 0.07686014473438263\n",
      "Epoch 4841, Train Loss: 0.029564764350652695, Test Loss: 0.07685127854347229\n",
      "Epoch 4842, Train Loss: 0.029558295384049416, Test Loss: 0.07683197408914566\n",
      "Epoch 4843, Train Loss: 0.02955155447125435, Test Loss: 0.0768427848815918\n",
      "Epoch 4844, Train Loss: 0.02954520285129547, Test Loss: 0.07683271169662476\n",
      "Epoch 4845, Train Loss: 0.029538022354245186, Test Loss: 0.07683715969324112\n",
      "Epoch 4846, Train Loss: 0.029531987383961678, Test Loss: 0.07682286202907562\n",
      "Epoch 4847, Train Loss: 0.029525376856327057, Test Loss: 0.07682634145021439\n",
      "Epoch 4848, Train Loss: 0.02951851673424244, Test Loss: 0.07683483511209488\n",
      "Epoch 4849, Train Loss: 0.029512178152799606, Test Loss: 0.07683313637971878\n",
      "Epoch 4850, Train Loss: 0.029505426064133644, Test Loss: 0.07682307809591293\n",
      "Epoch 4851, Train Loss: 0.029498986899852753, Test Loss: 0.07681242376565933\n",
      "Epoch 4852, Train Loss: 0.029492128640413284, Test Loss: 0.07680509239435196\n",
      "Epoch 4853, Train Loss: 0.029485132545232773, Test Loss: 0.07680432498455048\n",
      "Epoch 4854, Train Loss: 0.029478805139660835, Test Loss: 0.07679422944784164\n",
      "Epoch 4855, Train Loss: 0.029472164809703827, Test Loss: 0.0767897218465805\n",
      "Epoch 4856, Train Loss: 0.02946643717586994, Test Loss: 0.0767938643693924\n",
      "Epoch 4857, Train Loss: 0.029459325596690178, Test Loss: 0.0767955332994461\n",
      "Epoch 4858, Train Loss: 0.029452675953507423, Test Loss: 0.07678922265768051\n",
      "Epoch 4859, Train Loss: 0.029445791617035866, Test Loss: 0.07679632306098938\n",
      "Epoch 4860, Train Loss: 0.029439235106110573, Test Loss: 0.07678815722465515\n",
      "Epoch 4861, Train Loss: 0.02943248860538006, Test Loss: 0.07678472995758057\n",
      "Epoch 4862, Train Loss: 0.029426101595163345, Test Loss: 0.07676605880260468\n",
      "Epoch 4863, Train Loss: 0.029419416561722755, Test Loss: 0.07677134871482849\n",
      "Epoch 4864, Train Loss: 0.029413163661956787, Test Loss: 0.07676491886377335\n",
      "Epoch 4865, Train Loss: 0.029407965019345284, Test Loss: 0.07676365971565247\n",
      "Epoch 4866, Train Loss: 0.029400281608104706, Test Loss: 0.07675609737634659\n",
      "Epoch 4867, Train Loss: 0.029393471777439117, Test Loss: 0.07676459103822708\n",
      "Epoch 4868, Train Loss: 0.02938654273748398, Test Loss: 0.07675805687904358\n",
      "Epoch 4869, Train Loss: 0.02938047982752323, Test Loss: 0.07674874365329742\n",
      "Epoch 4870, Train Loss: 0.029373787343502045, Test Loss: 0.0767456516623497\n",
      "Epoch 4871, Train Loss: 0.02936759777367115, Test Loss: 0.07673880457878113\n",
      "Epoch 4872, Train Loss: 0.029360614717006683, Test Loss: 0.07673697173595428\n",
      "Epoch 4873, Train Loss: 0.029354404658079147, Test Loss: 0.07673593610525131\n",
      "Epoch 4874, Train Loss: 0.029347611591219902, Test Loss: 0.07673216611146927\n",
      "Epoch 4875, Train Loss: 0.029341507703065872, Test Loss: 0.07672763615846634\n",
      "Epoch 4876, Train Loss: 0.02933450974524021, Test Loss: 0.07673951983451843\n",
      "Epoch 4877, Train Loss: 0.029328007251024246, Test Loss: 0.07673551887273788\n",
      "Epoch 4878, Train Loss: 0.029321618378162384, Test Loss: 0.07672581076622009\n",
      "Epoch 4879, Train Loss: 0.029315674677491188, Test Loss: 0.07673219591379166\n",
      "Epoch 4880, Train Loss: 0.02930893376469612, Test Loss: 0.07670781761407852\n",
      "Epoch 4881, Train Loss: 0.029301626607775688, Test Loss: 0.07671112567186356\n",
      "Epoch 4882, Train Loss: 0.029296305030584335, Test Loss: 0.07670324295759201\n",
      "Epoch 4883, Train Loss: 0.029288947582244873, Test Loss: 0.07670380175113678\n",
      "Epoch 4884, Train Loss: 0.02928258292376995, Test Loss: 0.07669433206319809\n",
      "Epoch 4885, Train Loss: 0.029275890439748764, Test Loss: 0.07669754326343536\n",
      "Epoch 4886, Train Loss: 0.02927033044397831, Test Loss: 0.07668883353471756\n",
      "Epoch 4887, Train Loss: 0.029263166710734367, Test Loss: 0.07669325917959213\n",
      "Epoch 4888, Train Loss: 0.029256338253617287, Test Loss: 0.07669724524021149\n",
      "Epoch 4889, Train Loss: 0.02925078384578228, Test Loss: 0.07668835669755936\n",
      "Epoch 4890, Train Loss: 0.029243839904665947, Test Loss: 0.07668135315179825\n",
      "Epoch 4891, Train Loss: 0.029237184673547745, Test Loss: 0.07668197154998779\n",
      "Epoch 4892, Train Loss: 0.02923048473894596, Test Loss: 0.07668707519769669\n",
      "Epoch 4893, Train Loss: 0.029224608093500137, Test Loss: 0.07667744904756546\n",
      "Epoch 4894, Train Loss: 0.02921813167631626, Test Loss: 0.07667611539363861\n",
      "Epoch 4895, Train Loss: 0.029211116954684258, Test Loss: 0.07667674124240875\n",
      "Epoch 4896, Train Loss: 0.02920478954911232, Test Loss: 0.07668305933475494\n",
      "Epoch 4897, Train Loss: 0.0291991475969553, Test Loss: 0.07668256014585495\n",
      "Epoch 4898, Train Loss: 0.029191792011260986, Test Loss: 0.07667602598667145\n",
      "Epoch 4899, Train Loss: 0.02918514423072338, Test Loss: 0.07667131721973419\n",
      "Epoch 4900, Train Loss: 0.029178772121667862, Test Loss: 0.07667125016450882\n",
      "Epoch 4901, Train Loss: 0.029172509908676147, Test Loss: 0.07665719836950302\n",
      "Epoch 4902, Train Loss: 0.029166528955101967, Test Loss: 0.0766453966498375\n",
      "Epoch 4903, Train Loss: 0.029159128665924072, Test Loss: 0.07665616273880005\n",
      "Epoch 4904, Train Loss: 0.029153188690543175, Test Loss: 0.0766478031873703\n",
      "Epoch 4905, Train Loss: 0.029146337881684303, Test Loss: 0.0766487792134285\n",
      "Epoch 4906, Train Loss: 0.029140233993530273, Test Loss: 0.07663480192422867\n",
      "Epoch 4907, Train Loss: 0.029133837670087814, Test Loss: 0.07663831114768982\n",
      "Epoch 4908, Train Loss: 0.029128631576895714, Test Loss: 0.07663799822330475\n",
      "Epoch 4909, Train Loss: 0.029121052473783493, Test Loss: 0.07664195448160172\n",
      "Epoch 4910, Train Loss: 0.02911457233130932, Test Loss: 0.07663566619157791\n",
      "Epoch 4911, Train Loss: 0.029109006747603416, Test Loss: 0.07664725184440613\n",
      "Epoch 4912, Train Loss: 0.029101544991135597, Test Loss: 0.07662463188171387\n",
      "Epoch 4913, Train Loss: 0.029095277190208435, Test Loss: 0.07661353796720505\n",
      "Epoch 4914, Train Loss: 0.029088804498314857, Test Loss: 0.07662177830934525\n",
      "Epoch 4915, Train Loss: 0.02908257395029068, Test Loss: 0.0766109824180603\n",
      "Epoch 4916, Train Loss: 0.02907576970756054, Test Loss: 0.07662353664636612\n",
      "Epoch 4917, Train Loss: 0.029069585725665092, Test Loss: 0.07661772519350052\n",
      "Epoch 4918, Train Loss: 0.029062824323773384, Test Loss: 0.07661163806915283\n",
      "Epoch 4919, Train Loss: 0.02905665710568428, Test Loss: 0.07661014050245285\n",
      "Epoch 4920, Train Loss: 0.02905109152197838, Test Loss: 0.07659837603569031\n",
      "Epoch 4921, Train Loss: 0.029043806716799736, Test Loss: 0.07659191638231277\n",
      "Epoch 4922, Train Loss: 0.029037024825811386, Test Loss: 0.07658792287111282\n",
      "Epoch 4923, Train Loss: 0.029030947014689445, Test Loss: 0.0765824019908905\n",
      "Epoch 4924, Train Loss: 0.02902470901608467, Test Loss: 0.07658441364765167\n",
      "Epoch 4925, Train Loss: 0.029018031433224678, Test Loss: 0.07659406214952469\n",
      "Epoch 4926, Train Loss: 0.029012169688940048, Test Loss: 0.07658229768276215\n",
      "Epoch 4927, Train Loss: 0.029005372896790504, Test Loss: 0.07658849656581879\n",
      "Epoch 4928, Train Loss: 0.029000168666243553, Test Loss: 0.07658214867115021\n",
      "Epoch 4929, Train Loss: 0.028992919251322746, Test Loss: 0.07656826078891754\n",
      "Epoch 4930, Train Loss: 0.02898597903549671, Test Loss: 0.07656998187303543\n",
      "Epoch 4931, Train Loss: 0.02898051030933857, Test Loss: 0.07657203823328018\n",
      "Epoch 4932, Train Loss: 0.028973424807190895, Test Loss: 0.07656427472829819\n",
      "Epoch 4933, Train Loss: 0.028967054560780525, Test Loss: 0.07656794786453247\n",
      "Epoch 4934, Train Loss: 0.02896055392920971, Test Loss: 0.07656848430633545\n",
      "Epoch 4935, Train Loss: 0.028954660519957542, Test Loss: 0.07656917721033096\n",
      "Epoch 4936, Train Loss: 0.028948534280061722, Test Loss: 0.0765647292137146\n",
      "Epoch 4937, Train Loss: 0.028941888362169266, Test Loss: 0.0765516385436058\n",
      "Epoch 4938, Train Loss: 0.028935158625245094, Test Loss: 0.07653920352458954\n",
      "Epoch 4939, Train Loss: 0.028928956016898155, Test Loss: 0.0765315592288971\n",
      "Epoch 4940, Train Loss: 0.02892221137881279, Test Loss: 0.07653935253620148\n",
      "Epoch 4941, Train Loss: 0.028915848582983017, Test Loss: 0.07653827220201492\n",
      "Epoch 4942, Train Loss: 0.02891082502901554, Test Loss: 0.07654581218957901\n",
      "Epoch 4943, Train Loss: 0.028903203085064888, Test Loss: 0.07654094696044922\n",
      "Epoch 4944, Train Loss: 0.02889692410826683, Test Loss: 0.07653295993804932\n",
      "Epoch 4945, Train Loss: 0.028890827670693398, Test Loss: 0.07652997225522995\n",
      "Epoch 4946, Train Loss: 0.028884148225188255, Test Loss: 0.07653654366731644\n",
      "Epoch 4947, Train Loss: 0.028878193348646164, Test Loss: 0.07652492821216583\n",
      "Epoch 4948, Train Loss: 0.028871431946754456, Test Loss: 0.07652343064546585\n",
      "Epoch 4949, Train Loss: 0.028865816071629524, Test Loss: 0.07651296257972717\n",
      "Epoch 4950, Train Loss: 0.028858985751867294, Test Loss: 0.07651966065168381\n",
      "Epoch 4951, Train Loss: 0.028852812945842743, Test Loss: 0.07650038599967957\n",
      "Epoch 4952, Train Loss: 0.028845932334661484, Test Loss: 0.07650981843471527\n",
      "Epoch 4953, Train Loss: 0.028840322047472, Test Loss: 0.07651180028915405\n",
      "Epoch 4954, Train Loss: 0.02883381024003029, Test Loss: 0.07649823278188705\n",
      "Epoch 4955, Train Loss: 0.028826747089624405, Test Loss: 0.07650858163833618\n",
      "Epoch 4956, Train Loss: 0.028821397572755814, Test Loss: 0.0764891728758812\n",
      "Epoch 4957, Train Loss: 0.02881423942744732, Test Loss: 0.07650424540042877\n",
      "Epoch 4958, Train Loss: 0.028808334842324257, Test Loss: 0.0765041783452034\n",
      "Epoch 4959, Train Loss: 0.028801798820495605, Test Loss: 0.07649986445903778\n",
      "Epoch 4960, Train Loss: 0.028795339167118073, Test Loss: 0.0764860138297081\n",
      "Epoch 4961, Train Loss: 0.028789306059479713, Test Loss: 0.07648435980081558\n",
      "Epoch 4962, Train Loss: 0.0287824347615242, Test Loss: 0.07648176699876785\n",
      "Epoch 4963, Train Loss: 0.02877633087337017, Test Loss: 0.07647553831338882\n",
      "Epoch 4964, Train Loss: 0.028770986944437027, Test Loss: 0.07647620141506195\n",
      "Epoch 4965, Train Loss: 0.028763676062226295, Test Loss: 0.07647054642438889\n",
      "Epoch 4966, Train Loss: 0.028757378458976746, Test Loss: 0.07647784799337387\n",
      "Epoch 4967, Train Loss: 0.028751187026500702, Test Loss: 0.07647202163934708\n",
      "Epoch 4968, Train Loss: 0.028745030984282494, Test Loss: 0.07645884156227112\n",
      "Epoch 4969, Train Loss: 0.028738904744386673, Test Loss: 0.076465904712677\n",
      "Epoch 4970, Train Loss: 0.02873261831700802, Test Loss: 0.07644856721162796\n",
      "Epoch 4971, Train Loss: 0.028725942596793175, Test Loss: 0.07645410299301147\n",
      "Epoch 4972, Train Loss: 0.02872123382985592, Test Loss: 0.07645580172538757\n",
      "Epoch 4973, Train Loss: 0.028713610023260117, Test Loss: 0.07644824683666229\n",
      "Epoch 4974, Train Loss: 0.02870701439678669, Test Loss: 0.07644663751125336\n",
      "Epoch 4975, Train Loss: 0.02870084159076214, Test Loss: 0.0764421746134758\n",
      "Epoch 4976, Train Loss: 0.02869480662047863, Test Loss: 0.07643838226795197\n",
      "Epoch 4977, Train Loss: 0.028688248246908188, Test Loss: 0.07643172144889832\n",
      "Epoch 4978, Train Loss: 0.028681932017207146, Test Loss: 0.07643786817789078\n",
      "Epoch 4979, Train Loss: 0.02867712266743183, Test Loss: 0.07641224563121796\n",
      "Epoch 4980, Train Loss: 0.02866988442838192, Test Loss: 0.07640627026557922\n",
      "Epoch 4981, Train Loss: 0.0286636371165514, Test Loss: 0.07641337066888809\n",
      "Epoch 4982, Train Loss: 0.028657149523496628, Test Loss: 0.07640942931175232\n",
      "Epoch 4983, Train Loss: 0.028650764375925064, Test Loss: 0.0764087364077568\n",
      "Epoch 4984, Train Loss: 0.02864433079957962, Test Loss: 0.07641196995973587\n",
      "Epoch 4985, Train Loss: 0.028638379648327827, Test Loss: 0.07641768455505371\n",
      "Epoch 4986, Train Loss: 0.02863186039030552, Test Loss: 0.0764249712228775\n",
      "Epoch 4987, Train Loss: 0.02862594649195671, Test Loss: 0.07642808556556702\n",
      "Epoch 4988, Train Loss: 0.028619663789868355, Test Loss: 0.07640020549297333\n",
      "Epoch 4989, Train Loss: 0.028613455593585968, Test Loss: 0.07641429454088211\n",
      "Epoch 4990, Train Loss: 0.028606897220015526, Test Loss: 0.07641180604696274\n",
      "Epoch 4991, Train Loss: 0.02860085479915142, Test Loss: 0.0764017254114151\n",
      "Epoch 4992, Train Loss: 0.02859473042190075, Test Loss: 0.07639142125844955\n",
      "Epoch 4993, Train Loss: 0.02858823537826538, Test Loss: 0.07639162987470627\n",
      "Epoch 4994, Train Loss: 0.028582584112882614, Test Loss: 0.07639435678720474\n",
      "Epoch 4995, Train Loss: 0.028577253222465515, Test Loss: 0.07638687640428543\n",
      "Epoch 4996, Train Loss: 0.028570882976055145, Test Loss: 0.07637365907430649\n",
      "Epoch 4997, Train Loss: 0.028563395142555237, Test Loss: 0.0763770341873169\n",
      "Epoch 4998, Train Loss: 0.028557339683175087, Test Loss: 0.07637947797775269\n",
      "Epoch 4999, Train Loss: 0.02855103090405464, Test Loss: 0.07638508826494217\n",
      "Epoch 5000, Train Loss: 0.028544722124934196, Test Loss: 0.07638371735811234\n",
      "Epoch 5001, Train Loss: 0.028538541868329048, Test Loss: 0.0763801857829094\n",
      "Epoch 5002, Train Loss: 0.028532862663269043, Test Loss: 0.07638604193925858\n",
      "Epoch 5003, Train Loss: 0.028526104986667633, Test Loss: 0.07637695968151093\n",
      "Epoch 5004, Train Loss: 0.028520088642835617, Test Loss: 0.07636981457471848\n",
      "Epoch 5005, Train Loss: 0.02851380966603756, Test Loss: 0.07637732475996017\n",
      "Epoch 5006, Train Loss: 0.028507467359304428, Test Loss: 0.07636608183383942\n",
      "Epoch 5007, Train Loss: 0.028501136228442192, Test Loss: 0.07637131214141846\n",
      "Epoch 5008, Train Loss: 0.028495365753769875, Test Loss: 0.07636913657188416\n",
      "Epoch 5009, Train Loss: 0.02848880924284458, Test Loss: 0.07636646181344986\n",
      "Epoch 5010, Train Loss: 0.028482552617788315, Test Loss: 0.07634776830673218\n",
      "Epoch 5011, Train Loss: 0.028476383537054062, Test Loss: 0.07635036110877991\n",
      "Epoch 5012, Train Loss: 0.028470227494835854, Test Loss: 0.0763542503118515\n",
      "Epoch 5013, Train Loss: 0.028464151546359062, Test Loss: 0.07634763419628143\n",
      "Epoch 5014, Train Loss: 0.028458015993237495, Test Loss: 0.07634317129850388\n",
      "Epoch 5015, Train Loss: 0.028451718389987946, Test Loss: 0.07634644210338593\n",
      "Epoch 5016, Train Loss: 0.02844572253525257, Test Loss: 0.07633760571479797\n",
      "Epoch 5017, Train Loss: 0.028439417481422424, Test Loss: 0.07633768022060394\n",
      "Epoch 5018, Train Loss: 0.028433645144104958, Test Loss: 0.07633302360773087\n",
      "Epoch 5019, Train Loss: 0.028427639976143837, Test Loss: 0.07632563263177872\n",
      "Epoch 5020, Train Loss: 0.028421076014637947, Test Loss: 0.0763283520936966\n",
      "Epoch 5021, Train Loss: 0.028415264561772346, Test Loss: 0.07632234692573547\n",
      "Epoch 5022, Train Loss: 0.028408624231815338, Test Loss: 0.07631140202283859\n",
      "Epoch 5023, Train Loss: 0.028402378782629967, Test Loss: 0.07631640881299973\n",
      "Epoch 5024, Train Loss: 0.028396757319569588, Test Loss: 0.0763113722205162\n",
      "Epoch 5025, Train Loss: 0.0283907912671566, Test Loss: 0.07630427926778793\n",
      "Epoch 5026, Train Loss: 0.028384391218423843, Test Loss: 0.07629183679819107\n",
      "Epoch 5027, Train Loss: 0.028378881514072418, Test Loss: 0.07629314064979553\n",
      "Epoch 5028, Train Loss: 0.028372449800372124, Test Loss: 0.07628410309553146\n",
      "Epoch 5029, Train Loss: 0.028366150334477425, Test Loss: 0.07629233598709106\n",
      "Epoch 5030, Train Loss: 0.028360456228256226, Test Loss: 0.07628701627254486\n",
      "Epoch 5031, Train Loss: 0.028354013338685036, Test Loss: 0.07630699872970581\n",
      "Epoch 5032, Train Loss: 0.028347156941890717, Test Loss: 0.07629191130399704\n",
      "Epoch 5033, Train Loss: 0.028341399505734444, Test Loss: 0.07627782225608826\n",
      "Epoch 5034, Train Loss: 0.028335290029644966, Test Loss: 0.07628358900547028\n",
      "Epoch 5035, Train Loss: 0.028329720720648766, Test Loss: 0.07628326863050461\n",
      "Epoch 5036, Train Loss: 0.02832394652068615, Test Loss: 0.07627927511930466\n",
      "Epoch 5037, Train Loss: 0.028317028656601906, Test Loss: 0.07627163827419281\n",
      "Epoch 5038, Train Loss: 0.028310762718319893, Test Loss: 0.07628151029348373\n",
      "Epoch 5039, Train Loss: 0.028305329382419586, Test Loss: 0.07625345885753632\n",
      "Epoch 5040, Train Loss: 0.028298595920205116, Test Loss: 0.07626013457775116\n",
      "Epoch 5041, Train Loss: 0.02829309180378914, Test Loss: 0.07626263052225113\n",
      "Epoch 5042, Train Loss: 0.02828684076666832, Test Loss: 0.07626157253980637\n",
      "Epoch 5043, Train Loss: 0.028280694037675858, Test Loss: 0.07625487446784973\n",
      "Epoch 5044, Train Loss: 0.02827400155365467, Test Loss: 0.0762593150138855\n",
      "Epoch 5045, Train Loss: 0.028268126770853996, Test Loss: 0.07625608891248703\n",
      "Epoch 5046, Train Loss: 0.028262242674827576, Test Loss: 0.07624587416648865\n",
      "Epoch 5047, Train Loss: 0.028256257995963097, Test Loss: 0.07624826580286026\n",
      "Epoch 5048, Train Loss: 0.028250139206647873, Test Loss: 0.07625216990709305\n",
      "Epoch 5049, Train Loss: 0.0282443817704916, Test Loss: 0.07623925805091858\n",
      "Epoch 5050, Train Loss: 0.02823786810040474, Test Loss: 0.07624240219593048\n",
      "Epoch 5051, Train Loss: 0.028231950476765633, Test Loss: 0.07623860985040665\n",
      "Epoch 5052, Train Loss: 0.028225401416420937, Test Loss: 0.07625097781419754\n",
      "Epoch 5053, Train Loss: 0.02822055108845234, Test Loss: 0.07623247057199478\n",
      "Epoch 5054, Train Loss: 0.028213707730174065, Test Loss: 0.07622494548559189\n",
      "Epoch 5055, Train Loss: 0.028207937255501747, Test Loss: 0.07621695101261139\n",
      "Epoch 5056, Train Loss: 0.0282012689858675, Test Loss: 0.07621683925390244\n",
      "Epoch 5057, Train Loss: 0.0281953327357769, Test Loss: 0.07621081918478012\n",
      "Epoch 5058, Train Loss: 0.02818922884762287, Test Loss: 0.0762152150273323\n",
      "Epoch 5059, Train Loss: 0.028182974085211754, Test Loss: 0.07621587067842484\n",
      "Epoch 5060, Train Loss: 0.028177183121442795, Test Loss: 0.07620059698820114\n",
      "Epoch 5061, Train Loss: 0.028170721605420113, Test Loss: 0.0762174129486084\n",
      "Epoch 5062, Train Loss: 0.028164593502879143, Test Loss: 0.0762118250131607\n",
      "Epoch 5063, Train Loss: 0.02815885841846466, Test Loss: 0.07620611041784286\n",
      "Epoch 5064, Train Loss: 0.028152665123343468, Test Loss: 0.07620661705732346\n",
      "Epoch 5065, Train Loss: 0.028146514669060707, Test Loss: 0.07620381563901901\n",
      "Epoch 5066, Train Loss: 0.028140977025032043, Test Loss: 0.0762142762541771\n",
      "Epoch 5067, Train Loss: 0.028134366497397423, Test Loss: 0.07620657980442047\n",
      "Epoch 5068, Train Loss: 0.028128575533628464, Test Loss: 0.07619883865118027\n",
      "Epoch 5069, Train Loss: 0.02812258154153824, Test Loss: 0.07619308680295944\n",
      "Epoch 5070, Train Loss: 0.028116803616285324, Test Loss: 0.07618846744298935\n",
      "Epoch 5071, Train Loss: 0.028110066428780556, Test Loss: 0.07618986070156097\n",
      "Epoch 5072, Train Loss: 0.02810431644320488, Test Loss: 0.07619164884090424\n",
      "Epoch 5073, Train Loss: 0.028098180890083313, Test Loss: 0.07619469612836838\n",
      "Epoch 5074, Train Loss: 0.028091968968510628, Test Loss: 0.07619680464267731\n",
      "Epoch 5075, Train Loss: 0.028085904195904732, Test Loss: 0.07618547230958939\n",
      "Epoch 5076, Train Loss: 0.028079984709620476, Test Loss: 0.07619056850671768\n",
      "Epoch 5077, Train Loss: 0.02807513251900673, Test Loss: 0.07617568969726562\n",
      "Epoch 5078, Train Loss: 0.02806803584098816, Test Loss: 0.07616785913705826\n",
      "Epoch 5079, Train Loss: 0.028062045574188232, Test Loss: 0.07616405934095383\n",
      "Epoch 5080, Train Loss: 0.02805595099925995, Test Loss: 0.07616428285837173\n",
      "Epoch 5081, Train Loss: 0.028050335124135017, Test Loss: 0.07615827769041061\n",
      "Epoch 5082, Train Loss: 0.028043951839208603, Test Loss: 0.0761590376496315\n",
      "Epoch 5083, Train Loss: 0.028038952499628067, Test Loss: 0.07613839954137802\n",
      "Epoch 5084, Train Loss: 0.02803194336593151, Test Loss: 0.07614484429359436\n",
      "Epoch 5085, Train Loss: 0.028025640174746513, Test Loss: 0.07616160809993744\n",
      "Epoch 5086, Train Loss: 0.028019914403557777, Test Loss: 0.07614213973283768\n",
      "Epoch 5087, Train Loss: 0.028013940900564194, Test Loss: 0.07614099234342575\n",
      "Epoch 5088, Train Loss: 0.028007593005895615, Test Loss: 0.07614090293645859\n",
      "Epoch 5089, Train Loss: 0.02800256572663784, Test Loss: 0.0761326253414154\n",
      "Epoch 5090, Train Loss: 0.027995996177196503, Test Loss: 0.07613318413496017\n",
      "Epoch 5091, Train Loss: 0.02798939123749733, Test Loss: 0.07613926380872726\n",
      "Epoch 5092, Train Loss: 0.027984078973531723, Test Loss: 0.07614082843065262\n",
      "Epoch 5093, Train Loss: 0.02797841466963291, Test Loss: 0.07613047957420349\n",
      "Epoch 5094, Train Loss: 0.02797219157218933, Test Loss: 0.07612522691488266\n",
      "Epoch 5095, Train Loss: 0.027965867891907692, Test Loss: 0.07612725347280502\n",
      "Epoch 5096, Train Loss: 0.027960076928138733, Test Loss: 0.07613012939691544\n",
      "Epoch 5097, Train Loss: 0.02795405313372612, Test Loss: 0.07612964510917664\n",
      "Epoch 5098, Train Loss: 0.027948224917054176, Test Loss: 0.07611782848834991\n",
      "Epoch 5099, Train Loss: 0.0279416311532259, Test Loss: 0.0761251449584961\n",
      "Epoch 5100, Train Loss: 0.02793549746274948, Test Loss: 0.07611951231956482\n",
      "Epoch 5101, Train Loss: 0.02792963944375515, Test Loss: 0.07612116634845734\n",
      "Epoch 5102, Train Loss: 0.027923721820116043, Test Loss: 0.07612146437168121\n",
      "Epoch 5103, Train Loss: 0.027917614206671715, Test Loss: 0.07612588256597519\n",
      "Epoch 5104, Train Loss: 0.02791205793619156, Test Loss: 0.07612363249063492\n",
      "Epoch 5105, Train Loss: 0.027905594557523727, Test Loss: 0.07611015439033508\n",
      "Epoch 5106, Train Loss: 0.02789986878633499, Test Loss: 0.07609040290117264\n",
      "Epoch 5107, Train Loss: 0.027894513681530952, Test Loss: 0.07608850300312042\n",
      "Epoch 5108, Train Loss: 0.02788776159286499, Test Loss: 0.07608970999717712\n",
      "Epoch 5109, Train Loss: 0.027881478890776634, Test Loss: 0.07608902454376221\n",
      "Epoch 5110, Train Loss: 0.027875877916812897, Test Loss: 0.07610264420509338\n",
      "Epoch 5111, Train Loss: 0.02786976844072342, Test Loss: 0.07609403133392334\n",
      "Epoch 5112, Train Loss: 0.027863701805472374, Test Loss: 0.07609445601701736\n",
      "Epoch 5113, Train Loss: 0.027857894077897072, Test Loss: 0.07608861476182938\n",
      "Epoch 5114, Train Loss: 0.027851765975356102, Test Loss: 0.07608587294816971\n",
      "Epoch 5115, Train Loss: 0.02784634940326214, Test Loss: 0.07607797533273697\n",
      "Epoch 5116, Train Loss: 0.027840878814458847, Test Loss: 0.07608803361654282\n",
      "Epoch 5117, Train Loss: 0.027834180742502213, Test Loss: 0.07607108354568481\n",
      "Epoch 5118, Train Loss: 0.02782856673002243, Test Loss: 0.07606787234544754\n",
      "Epoch 5119, Train Loss: 0.02782226912677288, Test Loss: 0.07606606185436249\n",
      "Epoch 5120, Train Loss: 0.02781626023352146, Test Loss: 0.0760616809129715\n",
      "Epoch 5121, Train Loss: 0.027810119092464447, Test Loss: 0.07605899125337601\n",
      "Epoch 5122, Train Loss: 0.027804140001535416, Test Loss: 0.07605328410863876\n",
      "Epoch 5123, Train Loss: 0.027798738330602646, Test Loss: 0.07603541761636734\n",
      "Epoch 5124, Train Loss: 0.02779235690832138, Test Loss: 0.07605058699846268\n",
      "Epoch 5125, Train Loss: 0.027786124497652054, Test Loss: 0.07605373859405518\n",
      "Epoch 5126, Train Loss: 0.02778046950697899, Test Loss: 0.07604990154504776\n",
      "Epoch 5127, Train Loss: 0.02777421660721302, Test Loss: 0.07605275511741638\n",
      "Epoch 5128, Train Loss: 0.027768772095441818, Test Loss: 0.07606213539838791\n",
      "Epoch 5129, Train Loss: 0.027762262150645256, Test Loss: 0.07605212926864624\n",
      "Epoch 5130, Train Loss: 0.02775667794048786, Test Loss: 0.07604176551103592\n",
      "Epoch 5131, Train Loss: 0.02775074541568756, Test Loss: 0.07604056596755981\n",
      "Epoch 5132, Train Loss: 0.0277450792491436, Test Loss: 0.07603238523006439\n",
      "Epoch 5133, Train Loss: 0.02773880586028099, Test Loss: 0.07602563500404358\n",
      "Epoch 5134, Train Loss: 0.027733001857995987, Test Loss: 0.07603120803833008\n",
      "Epoch 5135, Train Loss: 0.02772735431790352, Test Loss: 0.07601597905158997\n",
      "Epoch 5136, Train Loss: 0.027721703052520752, Test Loss: 0.07600940018892288\n",
      "Epoch 5137, Train Loss: 0.027714967727661133, Test Loss: 0.07602068781852722\n",
      "Epoch 5138, Train Loss: 0.0277098398655653, Test Loss: 0.07601220905780792\n",
      "Epoch 5139, Train Loss: 0.02770381048321724, Test Loss: 0.07601480931043625\n",
      "Epoch 5140, Train Loss: 0.02769835852086544, Test Loss: 0.07601012289524078\n",
      "Epoch 5141, Train Loss: 0.027691606432199478, Test Loss: 0.07601164281368256\n",
      "Epoch 5142, Train Loss: 0.02768556773662567, Test Loss: 0.07601246982812881\n",
      "Epoch 5143, Train Loss: 0.027680061757564545, Test Loss: 0.07599862664937973\n",
      "Epoch 5144, Train Loss: 0.027674874290823936, Test Loss: 0.07599063217639923\n",
      "Epoch 5145, Train Loss: 0.027668893337249756, Test Loss: 0.07599235326051712\n",
      "Epoch 5146, Train Loss: 0.02766238898038864, Test Loss: 0.07598518580198288\n",
      "Epoch 5147, Train Loss: 0.02765682525932789, Test Loss: 0.07597950845956802\n",
      "Epoch 5148, Train Loss: 0.027650995180010796, Test Loss: 0.07597681134939194\n",
      "Epoch 5149, Train Loss: 0.027644407004117966, Test Loss: 0.07598060369491577\n",
      "Epoch 5150, Train Loss: 0.027638623490929604, Test Loss: 0.07597827911376953\n",
      "Epoch 5151, Train Loss: 0.027632704004645348, Test Loss: 0.0759810358285904\n",
      "Epoch 5152, Train Loss: 0.02762678451836109, Test Loss: 0.07597670704126358\n",
      "Epoch 5153, Train Loss: 0.027621785178780556, Test Loss: 0.07598827034235\n",
      "Epoch 5154, Train Loss: 0.02761540375649929, Test Loss: 0.07599198073148727\n",
      "Epoch 5155, Train Loss: 0.027609284967184067, Test Loss: 0.07597235590219498\n",
      "Epoch 5156, Train Loss: 0.027604637667536736, Test Loss: 0.07596419006586075\n",
      "Epoch 5157, Train Loss: 0.027598019689321518, Test Loss: 0.07596427202224731\n",
      "Epoch 5158, Train Loss: 0.027592122554779053, Test Loss: 0.0759643092751503\n",
      "Epoch 5159, Train Loss: 0.02758605405688286, Test Loss: 0.07595810294151306\n",
      "Epoch 5160, Train Loss: 0.027580050751566887, Test Loss: 0.07595968246459961\n",
      "Epoch 5161, Train Loss: 0.027573896571993828, Test Loss: 0.07596849650144577\n",
      "Epoch 5162, Train Loss: 0.02756834588944912, Test Loss: 0.07596764713525772\n",
      "Epoch 5163, Train Loss: 0.027562402188777924, Test Loss: 0.07596362382173538\n",
      "Epoch 5164, Train Loss: 0.027556486427783966, Test Loss: 0.07596255838871002\n",
      "Epoch 5165, Train Loss: 0.02755075879395008, Test Loss: 0.07595537602901459\n",
      "Epoch 5166, Train Loss: 0.027544934302568436, Test Loss: 0.0759432464838028\n",
      "Epoch 5167, Train Loss: 0.027538860216736794, Test Loss: 0.07594786584377289\n",
      "Epoch 5168, Train Loss: 0.027533017098903656, Test Loss: 0.07595379650592804\n",
      "Epoch 5169, Train Loss: 0.02752716653048992, Test Loss: 0.07594499737024307\n",
      "Epoch 5170, Train Loss: 0.0275215245783329, Test Loss: 0.07594000548124313\n",
      "Epoch 5171, Train Loss: 0.027515580877661705, Test Loss: 0.07593937963247299\n",
      "Epoch 5172, Train Loss: 0.027509771287441254, Test Loss: 0.07593099027872086\n",
      "Epoch 5173, Train Loss: 0.027503671124577522, Test Loss: 0.07592944800853729\n",
      "Epoch 5174, Train Loss: 0.02749801054596901, Test Loss: 0.07593037188053131\n",
      "Epoch 5175, Train Loss: 0.02749229595065117, Test Loss: 0.07592956721782684\n",
      "Epoch 5176, Train Loss: 0.027486680075526237, Test Loss: 0.07592195272445679\n",
      "Epoch 5177, Train Loss: 0.027480734512209892, Test Loss: 0.07591928541660309\n",
      "Epoch 5178, Train Loss: 0.027474671602249146, Test Loss: 0.07592513412237167\n",
      "Epoch 5179, Train Loss: 0.027469612658023834, Test Loss: 0.07592994719743729\n",
      "Epoch 5180, Train Loss: 0.027463316917419434, Test Loss: 0.07593754678964615\n",
      "Epoch 5181, Train Loss: 0.02745717018842697, Test Loss: 0.0759202167391777\n",
      "Epoch 5182, Train Loss: 0.02745136246085167, Test Loss: 0.07591458410024643\n",
      "Epoch 5183, Train Loss: 0.02744559943675995, Test Loss: 0.07591120898723602\n",
      "Epoch 5184, Train Loss: 0.027439691126346588, Test Loss: 0.07591403275728226\n",
      "Epoch 5185, Train Loss: 0.02743409015238285, Test Loss: 0.07590267062187195\n",
      "Epoch 5186, Train Loss: 0.0274282768368721, Test Loss: 0.07591667771339417\n",
      "Epoch 5187, Train Loss: 0.027422765269875526, Test Loss: 0.07590923458337784\n",
      "Epoch 5188, Train Loss: 0.027416687458753586, Test Loss: 0.07590318471193314\n",
      "Epoch 5189, Train Loss: 0.027410801500082016, Test Loss: 0.07589346915483475\n",
      "Epoch 5190, Train Loss: 0.02740498073399067, Test Loss: 0.07589765638113022\n",
      "Epoch 5191, Train Loss: 0.02739947848021984, Test Loss: 0.07587894052267075\n",
      "Epoch 5192, Train Loss: 0.02739339880645275, Test Loss: 0.07587829977273941\n",
      "Epoch 5193, Train Loss: 0.027387432754039764, Test Loss: 0.07588282972574234\n",
      "Epoch 5194, Train Loss: 0.02738271839916706, Test Loss: 0.07587127387523651\n",
      "Epoch 5195, Train Loss: 0.027376726269721985, Test Loss: 0.07587284594774246\n",
      "Epoch 5196, Train Loss: 0.027370747178792953, Test Loss: 0.07587534189224243\n",
      "Epoch 5197, Train Loss: 0.02736499160528183, Test Loss: 0.07585941255092621\n",
      "Epoch 5198, Train Loss: 0.027359094470739365, Test Loss: 0.07585816085338593\n",
      "Epoch 5199, Train Loss: 0.02735285274684429, Test Loss: 0.07586701214313507\n",
      "Epoch 5200, Train Loss: 0.027346579357981682, Test Loss: 0.07587239146232605\n",
      "Epoch 5201, Train Loss: 0.02734118327498436, Test Loss: 0.07586783170700073\n",
      "Epoch 5202, Train Loss: 0.027335351333022118, Test Loss: 0.07586401700973511\n",
      "Epoch 5203, Train Loss: 0.02732991613447666, Test Loss: 0.07585573196411133\n",
      "Epoch 5204, Train Loss: 0.027324864640831947, Test Loss: 0.07586212456226349\n",
      "Epoch 5205, Train Loss: 0.02731776051223278, Test Loss: 0.07586449384689331\n",
      "Epoch 5206, Train Loss: 0.027312109246850014, Test Loss: 0.07584851235151291\n",
      "Epoch 5207, Train Loss: 0.02730635553598404, Test Loss: 0.07584456354379654\n",
      "Epoch 5208, Train Loss: 0.027300916612148285, Test Loss: 0.07583549618721008\n",
      "Epoch 5209, Train Loss: 0.027295904234051704, Test Loss: 0.07582257688045502\n",
      "Epoch 5210, Train Loss: 0.027289075776934624, Test Loss: 0.0758315846323967\n",
      "Epoch 5211, Train Loss: 0.0272841714322567, Test Loss: 0.07582999020814896\n",
      "Epoch 5212, Train Loss: 0.027278698980808258, Test Loss: 0.07581903785467148\n",
      "Epoch 5213, Train Loss: 0.027272848412394524, Test Loss: 0.07582266628742218\n",
      "Epoch 5214, Train Loss: 0.02726590819656849, Test Loss: 0.07582337409257889\n",
      "Epoch 5215, Train Loss: 0.02726014144718647, Test Loss: 0.0758238285779953\n",
      "Epoch 5216, Train Loss: 0.027254551649093628, Test Loss: 0.0758228525519371\n",
      "Epoch 5217, Train Loss: 0.027248701080679893, Test Loss: 0.07582429051399231\n",
      "Epoch 5218, Train Loss: 0.027242962270975113, Test Loss: 0.07582010328769684\n",
      "Epoch 5219, Train Loss: 0.02723737061023712, Test Loss: 0.07582035660743713\n",
      "Epoch 5220, Train Loss: 0.027231499552726746, Test Loss: 0.07581326365470886\n",
      "Epoch 5221, Train Loss: 0.027225801721215248, Test Loss: 0.07580603659152985\n",
      "Epoch 5222, Train Loss: 0.027220116928219795, Test Loss: 0.07581138610839844\n",
      "Epoch 5223, Train Loss: 0.02721455879509449, Test Loss: 0.07580655068159103\n",
      "Epoch 5224, Train Loss: 0.0272090882062912, Test Loss: 0.0758199393749237\n",
      "Epoch 5225, Train Loss: 0.02720288373529911, Test Loss: 0.0758131742477417\n",
      "Epoch 5226, Train Loss: 0.027196913957595825, Test Loss: 0.07580508291721344\n",
      "Epoch 5227, Train Loss: 0.0271915290504694, Test Loss: 0.07581059634685516\n",
      "Epoch 5228, Train Loss: 0.02718541957437992, Test Loss: 0.0758105218410492\n",
      "Epoch 5229, Train Loss: 0.027180055156350136, Test Loss: 0.07581321895122528\n",
      "Epoch 5230, Train Loss: 0.027175335213541985, Test Loss: 0.07581353187561035\n",
      "Epoch 5231, Train Loss: 0.02716837078332901, Test Loss: 0.0758003443479538\n",
      "Epoch 5232, Train Loss: 0.02716257981956005, Test Loss: 0.07579401135444641\n",
      "Epoch 5233, Train Loss: 0.027156800031661987, Test Loss: 0.0757930800318718\n",
      "Epoch 5234, Train Loss: 0.02715109847486019, Test Loss: 0.07578254491090775\n",
      "Epoch 5235, Train Loss: 0.027145475149154663, Test Loss: 0.07578249275684357\n",
      "Epoch 5236, Train Loss: 0.027139943093061447, Test Loss: 0.07577314972877502\n",
      "Epoch 5237, Train Loss: 0.02713393233716488, Test Loss: 0.07577622681856155\n",
      "Epoch 5238, Train Loss: 0.027128420770168304, Test Loss: 0.07577083259820938\n",
      "Epoch 5239, Train Loss: 0.027123942971229553, Test Loss: 0.07577431946992874\n",
      "Epoch 5240, Train Loss: 0.027117466554045677, Test Loss: 0.07577607780694962\n",
      "Epoch 5241, Train Loss: 0.0271109901368618, Test Loss: 0.0757649689912796\n",
      "Epoch 5242, Train Loss: 0.02710608020424843, Test Loss: 0.07575783133506775\n",
      "Epoch 5243, Train Loss: 0.027100222185254097, Test Loss: 0.07576072216033936\n",
      "Epoch 5244, Train Loss: 0.027094461023807526, Test Loss: 0.07576242089271545\n",
      "Epoch 5245, Train Loss: 0.027088366448879242, Test Loss: 0.07576102018356323\n",
      "Epoch 5246, Train Loss: 0.027082428336143494, Test Loss: 0.07575687766075134\n",
      "Epoch 5247, Train Loss: 0.02707681991159916, Test Loss: 0.07576707750558853\n",
      "Epoch 5248, Train Loss: 0.027071399614214897, Test Loss: 0.07576663047075272\n",
      "Epoch 5249, Train Loss: 0.027065113186836243, Test Loss: 0.07575017213821411\n",
      "Epoch 5250, Train Loss: 0.027059895917773247, Test Loss: 0.07574488967657089\n",
      "Epoch 5251, Train Loss: 0.0270535945892334, Test Loss: 0.07574670761823654\n",
      "Epoch 5252, Train Loss: 0.02704840898513794, Test Loss: 0.07575297355651855\n",
      "Epoch 5253, Train Loss: 0.027042951434850693, Test Loss: 0.07575444877147675\n",
      "Epoch 5254, Train Loss: 0.0270371250808239, Test Loss: 0.07574011385440826\n",
      "Epoch 5255, Train Loss: 0.02703090012073517, Test Loss: 0.07574132084846497\n",
      "Epoch 5256, Train Loss: 0.02702655829489231, Test Loss: 0.0757262334227562\n",
      "Epoch 5257, Train Loss: 0.0270195621997118, Test Loss: 0.07573273032903671\n",
      "Epoch 5258, Train Loss: 0.027013828977942467, Test Loss: 0.07572443783283234\n",
      "Epoch 5259, Train Loss: 0.02700864151120186, Test Loss: 0.07571479678153992\n",
      "Epoch 5260, Train Loss: 0.027002794668078423, Test Loss: 0.07571721822023392\n",
      "Epoch 5261, Train Loss: 0.026997201144695282, Test Loss: 0.07571680843830109\n",
      "Epoch 5262, Train Loss: 0.026991236954927444, Test Loss: 0.0757175087928772\n",
      "Epoch 5263, Train Loss: 0.02698562853038311, Test Loss: 0.07570742815732956\n",
      "Epoch 5264, Train Loss: 0.0269795972853899, Test Loss: 0.07571744173765182\n",
      "Epoch 5265, Train Loss: 0.026973970234394073, Test Loss: 0.07572618871927261\n",
      "Epoch 5266, Train Loss: 0.026968374848365784, Test Loss: 0.0757201686501503\n",
      "Epoch 5267, Train Loss: 0.02696297876536846, Test Loss: 0.07571469247341156\n",
      "Epoch 5268, Train Loss: 0.02695714682340622, Test Loss: 0.07570816576480865\n",
      "Epoch 5269, Train Loss: 0.026951326057314873, Test Loss: 0.07570834457874298\n",
      "Epoch 5270, Train Loss: 0.026946421712636948, Test Loss: 0.07572783529758453\n",
      "Epoch 5271, Train Loss: 0.02694009058177471, Test Loss: 0.07571261376142502\n",
      "Epoch 5272, Train Loss: 0.02693442814052105, Test Loss: 0.07570165395736694\n",
      "Epoch 5273, Train Loss: 0.026928823441267014, Test Loss: 0.07570967823266983\n",
      "Epoch 5274, Train Loss: 0.026923255994915962, Test Loss: 0.07569452375173569\n",
      "Epoch 5275, Train Loss: 0.0269173551350832, Test Loss: 0.07568638771772385\n",
      "Epoch 5276, Train Loss: 0.026911744847893715, Test Loss: 0.07569950819015503\n",
      "Epoch 5277, Train Loss: 0.02690613456070423, Test Loss: 0.07569398730993271\n",
      "Epoch 5278, Train Loss: 0.026901256293058395, Test Loss: 0.07568565011024475\n",
      "Epoch 5279, Train Loss: 0.026894928887486458, Test Loss: 0.07566560059785843\n",
      "Epoch 5280, Train Loss: 0.026890715584158897, Test Loss: 0.07565688341856003\n",
      "Epoch 5281, Train Loss: 0.02688411809504032, Test Loss: 0.07566355168819427\n",
      "Epoch 5282, Train Loss: 0.026877516880631447, Test Loss: 0.0756649374961853\n",
      "Epoch 5283, Train Loss: 0.02687234804034233, Test Loss: 0.07565828412771225\n",
      "Epoch 5284, Train Loss: 0.026867294684052467, Test Loss: 0.07564955204725266\n",
      "Epoch 5285, Train Loss: 0.02686072140932083, Test Loss: 0.07565484195947647\n",
      "Epoch 5286, Train Loss: 0.02685600332915783, Test Loss: 0.07564886659383774\n",
      "Epoch 5287, Train Loss: 0.026849597692489624, Test Loss: 0.0756574273109436\n",
      "Epoch 5288, Train Loss: 0.026844074949622154, Test Loss: 0.07565304636955261\n",
      "Epoch 5289, Train Loss: 0.026838207617402077, Test Loss: 0.07566292583942413\n",
      "Epoch 5290, Train Loss: 0.02683287113904953, Test Loss: 0.07565322518348694\n",
      "Epoch 5291, Train Loss: 0.026827780529856682, Test Loss: 0.07564107328653336\n",
      "Epoch 5292, Train Loss: 0.026821624487638474, Test Loss: 0.07564428448677063\n",
      "Epoch 5293, Train Loss: 0.026815656572580338, Test Loss: 0.07564977556467056\n",
      "Epoch 5294, Train Loss: 0.026810137555003166, Test Loss: 0.07564888149499893\n",
      "Epoch 5295, Train Loss: 0.026804719120264053, Test Loss: 0.07564908266067505\n",
      "Epoch 5296, Train Loss: 0.0267989169806242, Test Loss: 0.0756407082080841\n",
      "Epoch 5297, Train Loss: 0.02679365873336792, Test Loss: 0.07562759518623352\n",
      "Epoch 5298, Train Loss: 0.026787538081407547, Test Loss: 0.07563000917434692\n",
      "Epoch 5299, Train Loss: 0.026781875640153885, Test Loss: 0.07564637064933777\n",
      "Epoch 5300, Train Loss: 0.02677636407315731, Test Loss: 0.07563366740942001\n",
      "Epoch 5301, Train Loss: 0.02677058055996895, Test Loss: 0.0756240114569664\n",
      "Epoch 5302, Train Loss: 0.026764769107103348, Test Loss: 0.07562076300382614\n",
      "Epoch 5303, Train Loss: 0.026759270578622818, Test Loss: 0.07563047856092453\n",
      "Epoch 5304, Train Loss: 0.026753665879368782, Test Loss: 0.07561657577753067\n",
      "Epoch 5305, Train Loss: 0.02674788609147072, Test Loss: 0.07562550157308578\n",
      "Epoch 5306, Train Loss: 0.026742545887827873, Test Loss: 0.07563038170337677\n",
      "Epoch 5307, Train Loss: 0.02673683688044548, Test Loss: 0.07561983168125153\n",
      "Epoch 5308, Train Loss: 0.026731343939900398, Test Loss: 0.0756264254450798\n",
      "Epoch 5309, Train Loss: 0.02672562561929226, Test Loss: 0.07561859488487244\n",
      "Epoch 5310, Train Loss: 0.026720933616161346, Test Loss: 0.07561500370502472\n",
      "Epoch 5311, Train Loss: 0.02671455591917038, Test Loss: 0.075611911714077\n",
      "Epoch 5312, Train Loss: 0.026708561927080154, Test Loss: 0.07561138272285461\n",
      "Epoch 5313, Train Loss: 0.02670343592762947, Test Loss: 0.07559321820735931\n",
      "Epoch 5314, Train Loss: 0.02669745311141014, Test Loss: 0.07559110224246979\n",
      "Epoch 5315, Train Loss: 0.026692425832152367, Test Loss: 0.07558827847242355\n",
      "Epoch 5316, Train Loss: 0.026686199009418488, Test Loss: 0.07559119164943695\n",
      "Epoch 5317, Train Loss: 0.026681287214159966, Test Loss: 0.07558876276016235\n",
      "Epoch 5318, Train Loss: 0.026674805209040642, Test Loss: 0.07559118419885635\n",
      "Epoch 5319, Train Loss: 0.026669176295399666, Test Loss: 0.0755896046757698\n",
      "Epoch 5320, Train Loss: 0.02666366659104824, Test Loss: 0.07558270543813705\n",
      "Epoch 5321, Train Loss: 0.026658516377210617, Test Loss: 0.07557884603738785\n",
      "Epoch 5322, Train Loss: 0.02665269747376442, Test Loss: 0.07558754086494446\n",
      "Epoch 5323, Train Loss: 0.026647137477993965, Test Loss: 0.07558548450469971\n",
      "Epoch 5324, Train Loss: 0.026641368865966797, Test Loss: 0.07557634264230728\n",
      "Epoch 5325, Train Loss: 0.026636231690645218, Test Loss: 0.07557377219200134\n",
      "Epoch 5326, Train Loss: 0.026630129665136337, Test Loss: 0.07557821273803711\n",
      "Epoch 5327, Train Loss: 0.026625171303749084, Test Loss: 0.07557698339223862\n",
      "Epoch 5328, Train Loss: 0.02661936730146408, Test Loss: 0.07556590437889099\n",
      "Epoch 5329, Train Loss: 0.02661343663930893, Test Loss: 0.07556188106536865\n",
      "Epoch 5330, Train Loss: 0.02660871297121048, Test Loss: 0.07556357979774475\n",
      "Epoch 5331, Train Loss: 0.026603279635310173, Test Loss: 0.07556498795747757\n",
      "Epoch 5332, Train Loss: 0.02659681625664234, Test Loss: 0.07555805146694183\n",
      "Epoch 5333, Train Loss: 0.026591168716549873, Test Loss: 0.07554822415113449\n",
      "Epoch 5334, Train Loss: 0.026585934683680534, Test Loss: 0.07555363327264786\n",
      "Epoch 5335, Train Loss: 0.026580695062875748, Test Loss: 0.07553334534168243\n",
      "Epoch 5336, Train Loss: 0.02657492272555828, Test Loss: 0.07553523778915405\n",
      "Epoch 5337, Train Loss: 0.026568856090307236, Test Loss: 0.07553847134113312\n",
      "Epoch 5338, Train Loss: 0.02656366303563118, Test Loss: 0.0755264163017273\n",
      "Epoch 5339, Train Loss: 0.026559928432106972, Test Loss: 0.07551804184913635\n",
      "Epoch 5340, Train Loss: 0.02655227668583393, Test Loss: 0.07553282380104065\n",
      "Epoch 5341, Train Loss: 0.026546895503997803, Test Loss: 0.07555419951677322\n",
      "Epoch 5342, Train Loss: 0.026541246101260185, Test Loss: 0.07554177939891815\n",
      "Epoch 5343, Train Loss: 0.026535721495747566, Test Loss: 0.07553404569625854\n",
      "Epoch 5344, Train Loss: 0.026530131697654724, Test Loss: 0.07552661746740341\n",
      "Epoch 5345, Train Loss: 0.026524534448981285, Test Loss: 0.07552403956651688\n",
      "Epoch 5346, Train Loss: 0.02651873417198658, Test Loss: 0.07553145289421082\n",
      "Epoch 5347, Train Loss: 0.026513520628213882, Test Loss: 0.07552466541528702\n",
      "Epoch 5348, Train Loss: 0.026507684960961342, Test Loss: 0.07553006708621979\n",
      "Epoch 5349, Train Loss: 0.02650318294763565, Test Loss: 0.07553576678037643\n",
      "Epoch 5350, Train Loss: 0.02649739198386669, Test Loss: 0.07553532719612122\n",
      "Epoch 5351, Train Loss: 0.026491127908229828, Test Loss: 0.07552317529916763\n",
      "Epoch 5352, Train Loss: 0.026486221700906754, Test Loss: 0.07552545517683029\n",
      "Epoch 5353, Train Loss: 0.02648009918630123, Test Loss: 0.0755135789513588\n",
      "Epoch 5354, Train Loss: 0.02647477388381958, Test Loss: 0.0755198523402214\n",
      "Epoch 5355, Train Loss: 0.02646924927830696, Test Loss: 0.07551100850105286\n",
      "Epoch 5356, Train Loss: 0.02646324597299099, Test Loss: 0.07550861686468124\n",
      "Epoch 5357, Train Loss: 0.026458466425538063, Test Loss: 0.07552114874124527\n",
      "Epoch 5358, Train Loss: 0.026452358812093735, Test Loss: 0.07551323622465134\n",
      "Epoch 5359, Train Loss: 0.02644745074212551, Test Loss: 0.07551342993974686\n",
      "Epoch 5360, Train Loss: 0.0264416690915823, Test Loss: 0.07550360262393951\n",
      "Epoch 5361, Train Loss: 0.026436135172843933, Test Loss: 0.07549449801445007\n",
      "Epoch 5362, Train Loss: 0.026430245488882065, Test Loss: 0.07550328224897385\n",
      "Epoch 5363, Train Loss: 0.026424920186400414, Test Loss: 0.0754980519413948\n",
      "Epoch 5364, Train Loss: 0.02641933225095272, Test Loss: 0.07548779249191284\n",
      "Epoch 5365, Train Loss: 0.026413701474666595, Test Loss: 0.07548578828573227\n",
      "Epoch 5366, Train Loss: 0.026409048587083817, Test Loss: 0.07547539472579956\n",
      "Epoch 5367, Train Loss: 0.02640300802886486, Test Loss: 0.07548554241657257\n",
      "Epoch 5368, Train Loss: 0.026397116482257843, Test Loss: 0.07547509670257568\n",
      "Epoch 5369, Train Loss: 0.02639154903590679, Test Loss: 0.07547437399625778\n",
      "Epoch 5370, Train Loss: 0.026386359706521034, Test Loss: 0.07548509538173676\n",
      "Epoch 5371, Train Loss: 0.02638065442442894, Test Loss: 0.07547003775835037\n",
      "Epoch 5372, Train Loss: 0.026375161483883858, Test Loss: 0.07546660304069519\n",
      "Epoch 5373, Train Loss: 0.026370679959654808, Test Loss: 0.0754571259021759\n",
      "Epoch 5374, Train Loss: 0.026364240795373917, Test Loss: 0.0754655972123146\n",
      "Epoch 5375, Train Loss: 0.02635861374437809, Test Loss: 0.0754687711596489\n",
      "Epoch 5376, Train Loss: 0.02635430358350277, Test Loss: 0.0754452794790268\n",
      "Epoch 5377, Train Loss: 0.026348818093538284, Test Loss: 0.07544799894094467\n",
      "Epoch 5378, Train Loss: 0.026342034339904785, Test Loss: 0.07546133548021317\n",
      "Epoch 5379, Train Loss: 0.02633729763329029, Test Loss: 0.07545555382966995\n",
      "Epoch 5380, Train Loss: 0.026331035420298576, Test Loss: 0.07544832676649094\n",
      "Epoch 5381, Train Loss: 0.026326343417167664, Test Loss: 0.07544539868831635\n",
      "Epoch 5382, Train Loss: 0.026320327073335648, Test Loss: 0.07542931288480759\n",
      "Epoch 5383, Train Loss: 0.026314985007047653, Test Loss: 0.07544684410095215\n",
      "Epoch 5384, Train Loss: 0.026309816166758537, Test Loss: 0.07542244344949722\n",
      "Epoch 5385, Train Loss: 0.02630370482802391, Test Loss: 0.07543600350618362\n",
      "Epoch 5386, Train Loss: 0.02629840560257435, Test Loss: 0.07543453574180603\n",
      "Epoch 5387, Train Loss: 0.0262928269803524, Test Loss: 0.07542745023965836\n",
      "Epoch 5388, Train Loss: 0.026287496089935303, Test Loss: 0.07542195916175842\n",
      "Epoch 5389, Train Loss: 0.02628222107887268, Test Loss: 0.07542618364095688\n",
      "Epoch 5390, Train Loss: 0.026276197284460068, Test Loss: 0.07542456686496735\n",
      "Epoch 5391, Train Loss: 0.026270797476172447, Test Loss: 0.0754224881529808\n",
      "Epoch 5392, Train Loss: 0.026265261694788933, Test Loss: 0.0754222720861435\n",
      "Epoch 5393, Train Loss: 0.026260580867528915, Test Loss: 0.07541710883378983\n",
      "Epoch 5394, Train Loss: 0.02625424787402153, Test Loss: 0.07541894912719727\n",
      "Epoch 5395, Train Loss: 0.026248786598443985, Test Loss: 0.07541454583406448\n",
      "Epoch 5396, Train Loss: 0.02624371089041233, Test Loss: 0.07540981471538544\n",
      "Epoch 5397, Train Loss: 0.026238171383738518, Test Loss: 0.07540804147720337\n",
      "Epoch 5398, Train Loss: 0.026232460513710976, Test Loss: 0.07540448009967804\n",
      "Epoch 5399, Train Loss: 0.026228245347738266, Test Loss: 0.07540882378816605\n",
      "Epoch 5400, Train Loss: 0.026221558451652527, Test Loss: 0.07541223615407944\n",
      "Epoch 5401, Train Loss: 0.026216285303235054, Test Loss: 0.07540255039930344\n",
      "Epoch 5402, Train Loss: 0.02621055208146572, Test Loss: 0.07540905475616455\n",
      "Epoch 5403, Train Loss: 0.026205051690340042, Test Loss: 0.07540090382099152\n",
      "Epoch 5404, Train Loss: 0.026199357584118843, Test Loss: 0.07539895921945572\n",
      "Epoch 5405, Train Loss: 0.026194268837571144, Test Loss: 0.07539023458957672\n",
      "Epoch 5406, Train Loss: 0.026188481599092484, Test Loss: 0.07538948953151703\n",
      "Epoch 5407, Train Loss: 0.02618308737874031, Test Loss: 0.07539431750774384\n",
      "Epoch 5408, Train Loss: 0.026178013533353806, Test Loss: 0.07538358122110367\n",
      "Epoch 5409, Train Loss: 0.02617235668003559, Test Loss: 0.07538960874080658\n",
      "Epoch 5410, Train Loss: 0.026167327538132668, Test Loss: 0.07537416368722916\n",
      "Epoch 5411, Train Loss: 0.026161637157201767, Test Loss: 0.07536976784467697\n",
      "Epoch 5412, Train Loss: 0.026155713945627213, Test Loss: 0.07537826895713806\n",
      "Epoch 5413, Train Loss: 0.026151051744818687, Test Loss: 0.07538248598575592\n",
      "Epoch 5414, Train Loss: 0.02614511549472809, Test Loss: 0.07537438720464706\n",
      "Epoch 5415, Train Loss: 0.026139989495277405, Test Loss: 0.07537834346294403\n",
      "Epoch 5416, Train Loss: 0.02613423578441143, Test Loss: 0.07537179440259933\n",
      "Epoch 5417, Train Loss: 0.026128774508833885, Test Loss: 0.07536886632442474\n",
      "Epoch 5418, Train Loss: 0.026124203577637672, Test Loss: 0.07534491270780563\n",
      "Epoch 5419, Train Loss: 0.02611795999109745, Test Loss: 0.07535731047391891\n",
      "Epoch 5420, Train Loss: 0.026112249121069908, Test Loss: 0.07536271959543228\n",
      "Epoch 5421, Train Loss: 0.02610691264271736, Test Loss: 0.07534630596637726\n",
      "Epoch 5422, Train Loss: 0.026101678609848022, Test Loss: 0.07534275949001312\n",
      "Epoch 5423, Train Loss: 0.026095887646079063, Test Loss: 0.07535304129123688\n",
      "Epoch 5424, Train Loss: 0.026090996339917183, Test Loss: 0.07535175234079361\n",
      "Epoch 5425, Train Loss: 0.02608553133904934, Test Loss: 0.07534097135066986\n",
      "Epoch 5426, Train Loss: 0.026079587638378143, Test Loss: 0.07533694058656693\n",
      "Epoch 5427, Train Loss: 0.02607409469783306, Test Loss: 0.07534824311733246\n",
      "Epoch 5428, Train Loss: 0.026068612933158875, Test Loss: 0.0753428041934967\n",
      "Epoch 5429, Train Loss: 0.02606363780796528, Test Loss: 0.07533024996519089\n",
      "Epoch 5430, Train Loss: 0.02605801820755005, Test Loss: 0.07532435655593872\n",
      "Epoch 5431, Train Loss: 0.026052605360746384, Test Loss: 0.07533348351716995\n",
      "Epoch 5432, Train Loss: 0.026046890765428543, Test Loss: 0.07532431185245514\n",
      "Epoch 5433, Train Loss: 0.026041356846690178, Test Loss: 0.07532601803541183\n",
      "Epoch 5434, Train Loss: 0.026036104187369347, Test Loss: 0.07532305270433426\n",
      "Epoch 5435, Train Loss: 0.026030827313661575, Test Loss: 0.07531802356243134\n",
      "Epoch 5436, Train Loss: 0.026025334373116493, Test Loss: 0.07532127946615219\n",
      "Epoch 5437, Train Loss: 0.026019608601927757, Test Loss: 0.07531515508890152\n",
      "Epoch 5438, Train Loss: 0.026014769449830055, Test Loss: 0.0753079205751419\n",
      "Epoch 5439, Train Loss: 0.02600945718586445, Test Loss: 0.07531726360321045\n",
      "Epoch 5440, Train Loss: 0.026003655046224594, Test Loss: 0.07531633973121643\n",
      "Epoch 5441, Train Loss: 0.02599860168993473, Test Loss: 0.07529512047767639\n",
      "Epoch 5442, Train Loss: 0.025992348790168762, Test Loss: 0.07529844343662262\n",
      "Epoch 5443, Train Loss: 0.025987081229686737, Test Loss: 0.07529975473880768\n",
      "Epoch 5444, Train Loss: 0.025981483981013298, Test Loss: 0.07530186325311661\n",
      "Epoch 5445, Train Loss: 0.025976117700338364, Test Loss: 0.0752992257475853\n",
      "Epoch 5446, Train Loss: 0.02597068063914776, Test Loss: 0.07529278844594955\n",
      "Epoch 5447, Train Loss: 0.025965318083763123, Test Loss: 0.07529304176568985\n",
      "Epoch 5448, Train Loss: 0.025959845632314682, Test Loss: 0.07529312372207642\n",
      "Epoch 5449, Train Loss: 0.025954274460673332, Test Loss: 0.07529715448617935\n",
      "Epoch 5450, Train Loss: 0.02594890631735325, Test Loss: 0.07530432194471359\n",
      "Epoch 5451, Train Loss: 0.02594372257590294, Test Loss: 0.07528799027204514\n",
      "Epoch 5452, Train Loss: 0.025938373059034348, Test Loss: 0.07529405504465103\n",
      "Epoch 5453, Train Loss: 0.025932738557457924, Test Loss: 0.07529544085264206\n",
      "Epoch 5454, Train Loss: 0.025928135961294174, Test Loss: 0.07528185844421387\n",
      "Epoch 5455, Train Loss: 0.025922013446688652, Test Loss: 0.07527808099985123\n",
      "Epoch 5456, Train Loss: 0.025916367769241333, Test Loss: 0.07528041303157806\n",
      "Epoch 5457, Train Loss: 0.025911414995789528, Test Loss: 0.07526326179504395\n",
      "Epoch 5458, Train Loss: 0.02590557560324669, Test Loss: 0.07527590543031693\n",
      "Epoch 5459, Train Loss: 0.02590034529566765, Test Loss: 0.07526680827140808\n",
      "Epoch 5460, Train Loss: 0.025894982740283012, Test Loss: 0.07526187598705292\n",
      "Epoch 5461, Train Loss: 0.025890009477734566, Test Loss: 0.07526738941669464\n",
      "Epoch 5462, Train Loss: 0.025883879512548447, Test Loss: 0.07525992393493652\n",
      "Epoch 5463, Train Loss: 0.025879422202706337, Test Loss: 0.07525818049907684\n",
      "Epoch 5464, Train Loss: 0.025873400270938873, Test Loss: 0.07524587213993073\n",
      "Epoch 5465, Train Loss: 0.025868207216262817, Test Loss: 0.07524867355823517\n",
      "Epoch 5466, Train Loss: 0.025862881913781166, Test Loss: 0.07525856792926788\n",
      "Epoch 5467, Train Loss: 0.025857513770461082, Test Loss: 0.07525999844074249\n",
      "Epoch 5468, Train Loss: 0.025852037593722343, Test Loss: 0.07524131238460541\n",
      "Epoch 5469, Train Loss: 0.025846848264336586, Test Loss: 0.07523541897535324\n",
      "Epoch 5470, Train Loss: 0.02584085613489151, Test Loss: 0.07523391395807266\n",
      "Epoch 5471, Train Loss: 0.025835908949375153, Test Loss: 0.07522943615913391\n",
      "Epoch 5472, Train Loss: 0.025830479338765144, Test Loss: 0.07522818446159363\n",
      "Epoch 5473, Train Loss: 0.025825725868344307, Test Loss: 0.07523258030414581\n",
      "Epoch 5474, Train Loss: 0.025819582864642143, Test Loss: 0.07522600144147873\n",
      "Epoch 5475, Train Loss: 0.025813797488808632, Test Loss: 0.07522330433130264\n",
      "Epoch 5476, Train Loss: 0.025808487087488174, Test Loss: 0.07522646337747574\n",
      "Epoch 5477, Train Loss: 0.025802770629525185, Test Loss: 0.07522618025541306\n",
      "Epoch 5478, Train Loss: 0.025797521695494652, Test Loss: 0.0752154067158699\n",
      "Epoch 5479, Train Loss: 0.025792870670557022, Test Loss: 0.07520605623722076\n",
      "Epoch 5480, Train Loss: 0.02578703872859478, Test Loss: 0.07521943747997284\n",
      "Epoch 5481, Train Loss: 0.02578149549663067, Test Loss: 0.07521730661392212\n",
      "Epoch 5482, Train Loss: 0.02577575109899044, Test Loss: 0.07521433383226395\n",
      "Epoch 5483, Train Loss: 0.025771141052246094, Test Loss: 0.07522723078727722\n",
      "Epoch 5484, Train Loss: 0.025765007361769676, Test Loss: 0.07521786540746689\n",
      "Epoch 5485, Train Loss: 0.025760402902960777, Test Loss: 0.0752159059047699\n",
      "Epoch 5486, Train Loss: 0.025754421949386597, Test Loss: 0.07520826905965805\n",
      "Epoch 5487, Train Loss: 0.025748997926712036, Test Loss: 0.07521357387304306\n",
      "Epoch 5488, Train Loss: 0.025743382051587105, Test Loss: 0.07522009313106537\n",
      "Epoch 5489, Train Loss: 0.02573816105723381, Test Loss: 0.07520513981580734\n",
      "Epoch 5490, Train Loss: 0.025733064860105515, Test Loss: 0.07519645988941193\n",
      "Epoch 5491, Train Loss: 0.025727491825819016, Test Loss: 0.075193852186203\n",
      "Epoch 5492, Train Loss: 0.02572210133075714, Test Loss: 0.07518573105335236\n",
      "Epoch 5493, Train Loss: 0.025716546922922134, Test Loss: 0.07519060373306274\n",
      "Epoch 5494, Train Loss: 0.025711029767990112, Test Loss: 0.07518400996923447\n",
      "Epoch 5495, Train Loss: 0.025706447660923004, Test Loss: 0.07517475634813309\n",
      "Epoch 5496, Train Loss: 0.025701314210891724, Test Loss: 0.07517591863870621\n",
      "Epoch 5497, Train Loss: 0.025695031508803368, Test Loss: 0.07518255710601807\n",
      "Epoch 5498, Train Loss: 0.02568945474922657, Test Loss: 0.07518454641103745\n",
      "Epoch 5499, Train Loss: 0.025684406980872154, Test Loss: 0.0751875638961792\n",
      "Epoch 5500, Train Loss: 0.02567899413406849, Test Loss: 0.07517189532518387\n",
      "Epoch 5501, Train Loss: 0.02567364275455475, Test Loss: 0.07518018037080765\n",
      "Epoch 5502, Train Loss: 0.025668533518910408, Test Loss: 0.07517163455486298\n",
      "Epoch 5503, Train Loss: 0.02566252462565899, Test Loss: 0.07517353445291519\n",
      "Epoch 5504, Train Loss: 0.02565746009349823, Test Loss: 0.07515976577997208\n",
      "Epoch 5505, Train Loss: 0.025652702897787094, Test Loss: 0.07517058402299881\n",
      "Epoch 5506, Train Loss: 0.02564646676182747, Test Loss: 0.0751635804772377\n",
      "Epoch 5507, Train Loss: 0.025641703978180885, Test Loss: 0.0751475840806961\n",
      "Epoch 5508, Train Loss: 0.02563653513789177, Test Loss: 0.07514069229364395\n",
      "Epoch 5509, Train Loss: 0.02563057467341423, Test Loss: 0.07514932751655579\n",
      "Epoch 5510, Train Loss: 0.025625459849834442, Test Loss: 0.07514190673828125\n",
      "Epoch 5511, Train Loss: 0.025620168074965477, Test Loss: 0.07514261454343796\n",
      "Epoch 5512, Train Loss: 0.02561398781836033, Test Loss: 0.07515327632427216\n",
      "Epoch 5513, Train Loss: 0.025609174743294716, Test Loss: 0.0751582682132721\n",
      "Epoch 5514, Train Loss: 0.02560368925333023, Test Loss: 0.07513942569494247\n",
      "Epoch 5515, Train Loss: 0.02559829130768776, Test Loss: 0.07513698935508728\n",
      "Epoch 5516, Train Loss: 0.025592481717467308, Test Loss: 0.07514294981956482\n",
      "Epoch 5517, Train Loss: 0.025588292628526688, Test Loss: 0.07514163106679916\n",
      "Epoch 5518, Train Loss: 0.02558188885450363, Test Loss: 0.07513315230607986\n",
      "Epoch 5519, Train Loss: 0.02557729370892048, Test Loss: 0.07514237612485886\n",
      "Epoch 5520, Train Loss: 0.025571594014763832, Test Loss: 0.0751432254910469\n",
      "Epoch 5521, Train Loss: 0.025566663593053818, Test Loss: 0.07512885332107544\n",
      "Epoch 5522, Train Loss: 0.025562124326825142, Test Loss: 0.0751257836818695\n",
      "Epoch 5523, Train Loss: 0.025555431842803955, Test Loss: 0.0751253291964531\n",
      "Epoch 5524, Train Loss: 0.025550175458192825, Test Loss: 0.07511822879314423\n",
      "Epoch 5525, Train Loss: 0.02554529719054699, Test Loss: 0.07512988895177841\n",
      "Epoch 5526, Train Loss: 0.025539277121424675, Test Loss: 0.07512890547513962\n",
      "Epoch 5527, Train Loss: 0.025533925741910934, Test Loss: 0.07511615753173828\n",
      "Epoch 5528, Train Loss: 0.025528784841299057, Test Loss: 0.07510805130004883\n",
      "Epoch 5529, Train Loss: 0.02552308700978756, Test Loss: 0.07510998845100403\n",
      "Epoch 5530, Train Loss: 0.025517789646983147, Test Loss: 0.0751069188117981\n",
      "Epoch 5531, Train Loss: 0.025512658059597015, Test Loss: 0.07509733736515045\n",
      "Epoch 5532, Train Loss: 0.025506997480988503, Test Loss: 0.07510378211736679\n",
      "Epoch 5533, Train Loss: 0.025501806288957596, Test Loss: 0.07511110603809357\n",
      "Epoch 5534, Train Loss: 0.025496341288089752, Test Loss: 0.07510840147733688\n",
      "Epoch 5535, Train Loss: 0.025490915402770042, Test Loss: 0.07509652525186539\n",
      "Epoch 5536, Train Loss: 0.02548559568822384, Test Loss: 0.07509323954582214\n",
      "Epoch 5537, Train Loss: 0.025480013340711594, Test Loss: 0.07508716732263565\n",
      "Epoch 5538, Train Loss: 0.025474894791841507, Test Loss: 0.07508765161037445\n",
      "Epoch 5539, Train Loss: 0.025469457730650902, Test Loss: 0.07508090883493423\n",
      "Epoch 5540, Train Loss: 0.025464387610554695, Test Loss: 0.0750766173005104\n",
      "Epoch 5541, Train Loss: 0.02545957826077938, Test Loss: 0.07507683336734772\n",
      "Epoch 5542, Train Loss: 0.025453541427850723, Test Loss: 0.07507337629795074\n",
      "Epoch 5543, Train Loss: 0.02544821985065937, Test Loss: 0.07507330924272537\n",
      "Epoch 5544, Train Loss: 0.02544328011572361, Test Loss: 0.07507437467575073\n",
      "Epoch 5545, Train Loss: 0.025438081473112106, Test Loss: 0.07507247477769852\n",
      "Epoch 5546, Train Loss: 0.025432344526052475, Test Loss: 0.07506979256868362\n",
      "Epoch 5547, Train Loss: 0.025427114218473434, Test Loss: 0.0750749334692955\n",
      "Epoch 5548, Train Loss: 0.025421982631087303, Test Loss: 0.07507205009460449\n",
      "Epoch 5549, Train Loss: 0.025416338816285133, Test Loss: 0.0750715509057045\n",
      "Epoch 5550, Train Loss: 0.02541118860244751, Test Loss: 0.07507297396659851\n",
      "Epoch 5551, Train Loss: 0.025406207889318466, Test Loss: 0.0750620886683464\n",
      "Epoch 5552, Train Loss: 0.025400327518582344, Test Loss: 0.0750659853219986\n",
      "Epoch 5553, Train Loss: 0.025395451113581657, Test Loss: 0.07506733387708664\n",
      "Epoch 5554, Train Loss: 0.025389602407813072, Test Loss: 0.07506146281957626\n",
      "Epoch 5555, Train Loss: 0.025384560227394104, Test Loss: 0.07506789267063141\n",
      "Epoch 5556, Train Loss: 0.02537907473742962, Test Loss: 0.07505986839532852\n",
      "Epoch 5557, Train Loss: 0.025373835116624832, Test Loss: 0.07505881041288376\n",
      "Epoch 5558, Train Loss: 0.025368643924593925, Test Loss: 0.07504986971616745\n",
      "Epoch 5559, Train Loss: 0.0253631379455328, Test Loss: 0.07503953576087952\n",
      "Epoch 5560, Train Loss: 0.025358472019433975, Test Loss: 0.07503999024629593\n",
      "Epoch 5561, Train Loss: 0.02535277232527733, Test Loss: 0.07504117488861084\n",
      "Epoch 5562, Train Loss: 0.025347383692860603, Test Loss: 0.07503729313611984\n",
      "Epoch 5563, Train Loss: 0.025341978296637535, Test Loss: 0.07502759248018265\n",
      "Epoch 5564, Train Loss: 0.02533678151667118, Test Loss: 0.07501846551895142\n",
      "Epoch 5565, Train Loss: 0.02533135935664177, Test Loss: 0.07501944154500961\n",
      "Epoch 5566, Train Loss: 0.025325901806354523, Test Loss: 0.07502242177724838\n",
      "Epoch 5567, Train Loss: 0.025320466607809067, Test Loss: 0.07502361387014389\n",
      "Epoch 5568, Train Loss: 0.02531542256474495, Test Loss: 0.0750221461057663\n",
      "Epoch 5569, Train Loss: 0.025309784337878227, Test Loss: 0.07502041757106781\n",
      "Epoch 5570, Train Loss: 0.025304537266492844, Test Loss: 0.07501690089702606\n",
      "Epoch 5571, Train Loss: 0.02529948204755783, Test Loss: 0.07501915097236633\n",
      "Epoch 5572, Train Loss: 0.025294404476881027, Test Loss: 0.07501428574323654\n",
      "Epoch 5573, Train Loss: 0.025288956239819527, Test Loss: 0.07500636577606201\n",
      "Epoch 5574, Train Loss: 0.02528388425707817, Test Loss: 0.0750114917755127\n",
      "Epoch 5575, Train Loss: 0.025278905406594276, Test Loss: 0.07499803602695465\n",
      "Epoch 5576, Train Loss: 0.02527393028140068, Test Loss: 0.07498903572559357\n",
      "Epoch 5577, Train Loss: 0.025268666446208954, Test Loss: 0.07500405609607697\n",
      "Epoch 5578, Train Loss: 0.025262802839279175, Test Loss: 0.07499930262565613\n",
      "Epoch 5579, Train Loss: 0.025258490815758705, Test Loss: 0.07498715817928314\n",
      "Epoch 5580, Train Loss: 0.025252997875213623, Test Loss: 0.07498335838317871\n",
      "Epoch 5581, Train Loss: 0.025247445330023766, Test Loss: 0.07499387115240097\n",
      "Epoch 5582, Train Loss: 0.02524193748831749, Test Loss: 0.07498794794082642\n",
      "Epoch 5583, Train Loss: 0.0252369437366724, Test Loss: 0.0749773159623146\n",
      "Epoch 5584, Train Loss: 0.025231655687093735, Test Loss: 0.074979268014431\n",
      "Epoch 5585, Train Loss: 0.025226492434740067, Test Loss: 0.07497826218605042\n",
      "Epoch 5586, Train Loss: 0.02522125653922558, Test Loss: 0.07497826218605042\n",
      "Epoch 5587, Train Loss: 0.02521587163209915, Test Loss: 0.07498861849308014\n",
      "Epoch 5588, Train Loss: 0.025211183354258537, Test Loss: 0.07498825341463089\n",
      "Epoch 5589, Train Loss: 0.02520585060119629, Test Loss: 0.0749763622879982\n",
      "Epoch 5590, Train Loss: 0.025200488045811653, Test Loss: 0.07496915757656097\n",
      "Epoch 5591, Train Loss: 0.025195229798555374, Test Loss: 0.07496699690818787\n",
      "Epoch 5592, Train Loss: 0.02519133873283863, Test Loss: 0.07497725635766983\n",
      "Epoch 5593, Train Loss: 0.025184985250234604, Test Loss: 0.07497171312570572\n",
      "Epoch 5594, Train Loss: 0.02517964504659176, Test Loss: 0.0749722272157669\n",
      "Epoch 5595, Train Loss: 0.025174636393785477, Test Loss: 0.07496637105941772\n",
      "Epoch 5596, Train Loss: 0.0251693706959486, Test Loss: 0.07496621459722519\n",
      "Epoch 5597, Train Loss: 0.025164149701595306, Test Loss: 0.07495836168527603\n",
      "Epoch 5598, Train Loss: 0.025158988311886787, Test Loss: 0.07495264708995819\n",
      "Epoch 5599, Train Loss: 0.025154193863272667, Test Loss: 0.07495694607496262\n",
      "Epoch 5600, Train Loss: 0.025149183347821236, Test Loss: 0.07493781298398972\n",
      "Epoch 5601, Train Loss: 0.025143463164567947, Test Loss: 0.07495632022619247\n",
      "Epoch 5602, Train Loss: 0.025138068944215775, Test Loss: 0.07494595646858215\n",
      "Epoch 5603, Train Loss: 0.025133153423666954, Test Loss: 0.0749393031001091\n",
      "Epoch 5604, Train Loss: 0.025127818807959557, Test Loss: 0.07494667172431946\n",
      "Epoch 5605, Train Loss: 0.025122834369540215, Test Loss: 0.07492569088935852\n",
      "Epoch 5606, Train Loss: 0.025117291137576103, Test Loss: 0.07493077218532562\n",
      "Epoch 5607, Train Loss: 0.02511250041425228, Test Loss: 0.07492250949144363\n",
      "Epoch 5608, Train Loss: 0.02510693110525608, Test Loss: 0.07493207603693008\n",
      "Epoch 5609, Train Loss: 0.02510225959122181, Test Loss: 0.07492442429065704\n",
      "Epoch 5610, Train Loss: 0.02509671077132225, Test Loss: 0.07492021471261978\n",
      "Epoch 5611, Train Loss: 0.025091882795095444, Test Loss: 0.07490483671426773\n",
      "Epoch 5612, Train Loss: 0.025088045746088028, Test Loss: 0.07489537447690964\n",
      "Epoch 5613, Train Loss: 0.02508121356368065, Test Loss: 0.07490777969360352\n",
      "Epoch 5614, Train Loss: 0.025076137855648994, Test Loss: 0.07491408288478851\n",
      "Epoch 5615, Train Loss: 0.0250709131360054, Test Loss: 0.07490510493516922\n",
      "Epoch 5616, Train Loss: 0.025066403672099113, Test Loss: 0.0748947411775589\n",
      "Epoch 5617, Train Loss: 0.025061115622520447, Test Loss: 0.07488872855901718\n",
      "Epoch 5618, Train Loss: 0.0250557754188776, Test Loss: 0.07487869262695312\n",
      "Epoch 5619, Train Loss: 0.025050725787878036, Test Loss: 0.07487814873456955\n",
      "Epoch 5620, Train Loss: 0.025045150890946388, Test Loss: 0.07489220052957535\n",
      "Epoch 5621, Train Loss: 0.025040505453944206, Test Loss: 0.0748894065618515\n",
      "Epoch 5622, Train Loss: 0.025035157799720764, Test Loss: 0.07488811016082764\n",
      "Epoch 5623, Train Loss: 0.02503015100955963, Test Loss: 0.07488156855106354\n",
      "Epoch 5624, Train Loss: 0.02502509020268917, Test Loss: 0.07488879561424255\n",
      "Epoch 5625, Train Loss: 0.025019505992531776, Test Loss: 0.07488879561424255\n",
      "Epoch 5626, Train Loss: 0.025014756247401237, Test Loss: 0.07488004118204117\n",
      "Epoch 5627, Train Loss: 0.025009630247950554, Test Loss: 0.07487944513559341\n",
      "Epoch 5628, Train Loss: 0.025004655122756958, Test Loss: 0.07487373799085617\n",
      "Epoch 5629, Train Loss: 0.024998970329761505, Test Loss: 0.0748855248093605\n",
      "Epoch 5630, Train Loss: 0.02499406971037388, Test Loss: 0.07487748563289642\n",
      "Epoch 5631, Train Loss: 0.02498907782137394, Test Loss: 0.07486642897129059\n",
      "Epoch 5632, Train Loss: 0.02498454786837101, Test Loss: 0.07486839592456818\n",
      "Epoch 5633, Train Loss: 0.02497861720621586, Test Loss: 0.07486537843942642\n",
      "Epoch 5634, Train Loss: 0.02497345767915249, Test Loss: 0.07486650347709656\n",
      "Epoch 5635, Train Loss: 0.0249688271433115, Test Loss: 0.07486152648925781\n",
      "Epoch 5636, Train Loss: 0.024963119998574257, Test Loss: 0.07487326115369797\n",
      "Epoch 5637, Train Loss: 0.024958321824669838, Test Loss: 0.07486937940120697\n",
      "Epoch 5638, Train Loss: 0.024952920153737068, Test Loss: 0.07485785335302353\n",
      "Epoch 5639, Train Loss: 0.024948187172412872, Test Loss: 0.07485724985599518\n",
      "Epoch 5640, Train Loss: 0.024942917749285698, Test Loss: 0.07487110793590546\n",
      "Epoch 5641, Train Loss: 0.024939078837633133, Test Loss: 0.07486670464277267\n",
      "Epoch 5642, Train Loss: 0.024933166801929474, Test Loss: 0.07484836876392365\n",
      "Epoch 5643, Train Loss: 0.024927977472543716, Test Loss: 0.07483575493097305\n",
      "Epoch 5644, Train Loss: 0.024922573938965797, Test Loss: 0.07483714818954468\n",
      "Epoch 5645, Train Loss: 0.024917230010032654, Test Loss: 0.07484167069196701\n",
      "Epoch 5646, Train Loss: 0.024912375956773758, Test Loss: 0.07483719289302826\n",
      "Epoch 5647, Train Loss: 0.024907052516937256, Test Loss: 0.07483568787574768\n",
      "Epoch 5648, Train Loss: 0.02490239590406418, Test Loss: 0.07483380287885666\n",
      "Epoch 5649, Train Loss: 0.024897640570998192, Test Loss: 0.07483087480068207\n",
      "Epoch 5650, Train Loss: 0.024892345070838928, Test Loss: 0.07481910288333893\n",
      "Epoch 5651, Train Loss: 0.024886958301067352, Test Loss: 0.07482285797595978\n",
      "Epoch 5652, Train Loss: 0.024881623685359955, Test Loss: 0.0748334676027298\n",
      "Epoch 5653, Train Loss: 0.02487696148455143, Test Loss: 0.07481487840414047\n",
      "Epoch 5654, Train Loss: 0.02487214468419552, Test Loss: 0.0748121365904808\n",
      "Epoch 5655, Train Loss: 0.024866405874490738, Test Loss: 0.07481161504983902\n",
      "Epoch 5656, Train Loss: 0.024862485006451607, Test Loss: 0.07481508702039719\n",
      "Epoch 5657, Train Loss: 0.02485623210668564, Test Loss: 0.07481341063976288\n",
      "Epoch 5658, Train Loss: 0.024851076304912567, Test Loss: 0.07481260597705841\n",
      "Epoch 5659, Train Loss: 0.024845995008945465, Test Loss: 0.07481440901756287\n",
      "Epoch 5660, Train Loss: 0.02484130673110485, Test Loss: 0.07480116933584213\n",
      "Epoch 5661, Train Loss: 0.024836240336298943, Test Loss: 0.074797123670578\n",
      "Epoch 5662, Train Loss: 0.024831676855683327, Test Loss: 0.07481548190116882\n",
      "Epoch 5663, Train Loss: 0.024825673550367355, Test Loss: 0.07480454444885254\n",
      "Epoch 5664, Train Loss: 0.02482094056904316, Test Loss: 0.0747997909784317\n",
      "Epoch 5665, Train Loss: 0.024817103520035744, Test Loss: 0.07480832189321518\n",
      "Epoch 5666, Train Loss: 0.02481134608387947, Test Loss: 0.07479093968868256\n",
      "Epoch 5667, Train Loss: 0.024805501103401184, Test Loss: 0.07480007410049438\n",
      "Epoch 5668, Train Loss: 0.02480066381394863, Test Loss: 0.07478807866573334\n",
      "Epoch 5669, Train Loss: 0.024796228855848312, Test Loss: 0.07478507608175278\n",
      "Epoch 5670, Train Loss: 0.024790579453110695, Test Loss: 0.074781633913517\n",
      "Epoch 5671, Train Loss: 0.024785742163658142, Test Loss: 0.07477951049804688\n",
      "Epoch 5672, Train Loss: 0.02478094771504402, Test Loss: 0.07478196918964386\n",
      "Epoch 5673, Train Loss: 0.024776212871074677, Test Loss: 0.07477295398712158\n",
      "Epoch 5674, Train Loss: 0.024770865216851234, Test Loss: 0.07476631551980972\n",
      "Epoch 5675, Train Loss: 0.02476530894637108, Test Loss: 0.07477232813835144\n",
      "Epoch 5676, Train Loss: 0.024761216714978218, Test Loss: 0.0747574046254158\n",
      "Epoch 5677, Train Loss: 0.02475559711456299, Test Loss: 0.07476282864809036\n",
      "Epoch 5678, Train Loss: 0.024750659242272377, Test Loss: 0.07475972175598145\n",
      "Epoch 5679, Train Loss: 0.024746272712945938, Test Loss: 0.07475820928812027\n",
      "Epoch 5680, Train Loss: 0.024740582332015038, Test Loss: 0.07475293427705765\n",
      "Epoch 5681, Train Loss: 0.0247354656457901, Test Loss: 0.07475894689559937\n",
      "Epoch 5682, Train Loss: 0.024731170386075974, Test Loss: 0.07476269453763962\n",
      "Epoch 5683, Train Loss: 0.02472572587430477, Test Loss: 0.07475835084915161\n",
      "Epoch 5684, Train Loss: 0.024720532819628716, Test Loss: 0.07475630193948746\n",
      "Epoch 5685, Train Loss: 0.024716002866625786, Test Loss: 0.07473704218864441\n",
      "Epoch 5686, Train Loss: 0.024710040539503098, Test Loss: 0.0747421458363533\n",
      "Epoch 5687, Train Loss: 0.02470511756837368, Test Loss: 0.07474155724048615\n",
      "Epoch 5688, Train Loss: 0.024700254201889038, Test Loss: 0.07473748922348022\n",
      "Epoch 5689, Train Loss: 0.024695130065083504, Test Loss: 0.07473323494195938\n",
      "Epoch 5690, Train Loss: 0.02468991093337536, Test Loss: 0.07474453747272491\n",
      "Epoch 5691, Train Loss: 0.024684812873601913, Test Loss: 0.07474059611558914\n",
      "Epoch 5692, Train Loss: 0.024679850786924362, Test Loss: 0.07473352551460266\n",
      "Epoch 5693, Train Loss: 0.02467542700469494, Test Loss: 0.07472873479127884\n",
      "Epoch 5694, Train Loss: 0.024670233950018883, Test Loss: 0.07472927123308182\n",
      "Epoch 5695, Train Loss: 0.024664925411343575, Test Loss: 0.07472392171621323\n",
      "Epoch 5696, Train Loss: 0.02466006577014923, Test Loss: 0.07472455501556396\n",
      "Epoch 5697, Train Loss: 0.024654850363731384, Test Loss: 0.07471979409456253\n",
      "Epoch 5698, Train Loss: 0.02464977838099003, Test Loss: 0.0747133418917656\n",
      "Epoch 5699, Train Loss: 0.024644747376441956, Test Loss: 0.0747218132019043\n",
      "Epoch 5700, Train Loss: 0.024639833718538284, Test Loss: 0.07471153885126114\n",
      "Epoch 5701, Train Loss: 0.02463528886437416, Test Loss: 0.07470595836639404\n",
      "Epoch 5702, Train Loss: 0.024630501866340637, Test Loss: 0.07470409572124481\n",
      "Epoch 5703, Train Loss: 0.024625038728117943, Test Loss: 0.07470279932022095\n",
      "Epoch 5704, Train Loss: 0.024620812386274338, Test Loss: 0.0747055932879448\n",
      "Epoch 5705, Train Loss: 0.02461499534547329, Test Loss: 0.07470575720071793\n",
      "Epoch 5706, Train Loss: 0.024609848856925964, Test Loss: 0.07471010833978653\n",
      "Epoch 5707, Train Loss: 0.024605557322502136, Test Loss: 0.0746980682015419\n",
      "Epoch 5708, Train Loss: 0.024599801748991013, Test Loss: 0.07469437271356583\n",
      "Epoch 5709, Train Loss: 0.02459513209760189, Test Loss: 0.07469270378351212\n",
      "Epoch 5710, Train Loss: 0.02458980493247509, Test Loss: 0.0747005045413971\n",
      "Epoch 5711, Train Loss: 0.024585122242569923, Test Loss: 0.07469157874584198\n",
      "Epoch 5712, Train Loss: 0.024579888209700584, Test Loss: 0.07468321919441223\n",
      "Epoch 5713, Train Loss: 0.024575231596827507, Test Loss: 0.07468380779027939\n",
      "Epoch 5714, Train Loss: 0.024569954723119736, Test Loss: 0.07468277215957642\n",
      "Epoch 5715, Train Loss: 0.02456490881741047, Test Loss: 0.0746825784444809\n",
      "Epoch 5716, Train Loss: 0.024559928104281425, Test Loss: 0.07468237727880478\n",
      "Epoch 5717, Train Loss: 0.02455536462366581, Test Loss: 0.0746731087565422\n",
      "Epoch 5718, Train Loss: 0.02455020695924759, Test Loss: 0.07467696070671082\n",
      "Epoch 5719, Train Loss: 0.024545038118958473, Test Loss: 0.07469015568494797\n",
      "Epoch 5720, Train Loss: 0.024540213868021965, Test Loss: 0.074683777987957\n",
      "Epoch 5721, Train Loss: 0.02453542873263359, Test Loss: 0.07467397302389145\n",
      "Epoch 5722, Train Loss: 0.024530019611120224, Test Loss: 0.07467830181121826\n",
      "Epoch 5723, Train Loss: 0.024525586515665054, Test Loss: 0.07465828210115433\n",
      "Epoch 5724, Train Loss: 0.024520115926861763, Test Loss: 0.07466161251068115\n",
      "Epoch 5725, Train Loss: 0.02451549656689167, Test Loss: 0.07466219365596771\n",
      "Epoch 5726, Train Loss: 0.02451043762266636, Test Loss: 0.07466829568147659\n",
      "Epoch 5727, Train Loss: 0.02450582943856716, Test Loss: 0.07466818392276764\n",
      "Epoch 5728, Train Loss: 0.024500468745827675, Test Loss: 0.07466278225183487\n",
      "Epoch 5729, Train Loss: 0.024495543912053108, Test Loss: 0.07466898113489151\n",
      "Epoch 5730, Train Loss: 0.024490337818861008, Test Loss: 0.07465831190347672\n",
      "Epoch 5731, Train Loss: 0.0244856346398592, Test Loss: 0.0746573656797409\n",
      "Epoch 5732, Train Loss: 0.02448035590350628, Test Loss: 0.07465547323226929\n",
      "Epoch 5733, Train Loss: 0.0244759488850832, Test Loss: 0.0746445506811142\n",
      "Epoch 5734, Train Loss: 0.024470550939440727, Test Loss: 0.07463887333869934\n",
      "Epoch 5735, Train Loss: 0.024466106668114662, Test Loss: 0.07464703917503357\n",
      "Epoch 5736, Train Loss: 0.02446124702692032, Test Loss: 0.07464078068733215\n",
      "Epoch 5737, Train Loss: 0.024456091225147247, Test Loss: 0.07463400810956955\n",
      "Epoch 5738, Train Loss: 0.024451004341244698, Test Loss: 0.07462696731090546\n",
      "Epoch 5739, Train Loss: 0.02444559708237648, Test Loss: 0.0746346190571785\n",
      "Epoch 5740, Train Loss: 0.02444085292518139, Test Loss: 0.07463358342647552\n",
      "Epoch 5741, Train Loss: 0.02443656511604786, Test Loss: 0.0746251717209816\n",
      "Epoch 5742, Train Loss: 0.02443140372633934, Test Loss: 0.07462400943040848\n",
      "Epoch 5743, Train Loss: 0.02442597784101963, Test Loss: 0.07463239133358002\n",
      "Epoch 5744, Train Loss: 0.02442130818963051, Test Loss: 0.07464125752449036\n",
      "Epoch 5745, Train Loss: 0.024416858330368996, Test Loss: 0.07464335113763809\n",
      "Epoch 5746, Train Loss: 0.024411950260400772, Test Loss: 0.0746312290430069\n",
      "Epoch 5747, Train Loss: 0.024407105520367622, Test Loss: 0.07462996244430542\n",
      "Epoch 5748, Train Loss: 0.024402279406785965, Test Loss: 0.07461394369602203\n",
      "Epoch 5749, Train Loss: 0.024396322667598724, Test Loss: 0.07461629062891006\n",
      "Epoch 5750, Train Loss: 0.02439175546169281, Test Loss: 0.07461092621088028\n",
      "Epoch 5751, Train Loss: 0.024386877194046974, Test Loss: 0.07461797446012497\n",
      "Epoch 5752, Train Loss: 0.024381523951888084, Test Loss: 0.07460936158895493\n",
      "Epoch 5753, Train Loss: 0.024376707151532173, Test Loss: 0.07461195439100266\n",
      "Epoch 5754, Train Loss: 0.024372059851884842, Test Loss: 0.07462161779403687\n",
      "Epoch 5755, Train Loss: 0.02436753921210766, Test Loss: 0.07459921389818192\n",
      "Epoch 5756, Train Loss: 0.02436293475329876, Test Loss: 0.07459933310747147\n",
      "Epoch 5757, Train Loss: 0.0243570227175951, Test Loss: 0.07460098713636398\n",
      "Epoch 5758, Train Loss: 0.0243535153567791, Test Loss: 0.07461244612932205\n",
      "Epoch 5759, Train Loss: 0.024347471073269844, Test Loss: 0.0746098980307579\n",
      "Epoch 5760, Train Loss: 0.02434304915368557, Test Loss: 0.07459264248609543\n",
      "Epoch 5761, Train Loss: 0.024338863790035248, Test Loss: 0.0745895728468895\n",
      "Epoch 5762, Train Loss: 0.024334348738193512, Test Loss: 0.07458139210939407\n",
      "Epoch 5763, Train Loss: 0.02432814985513687, Test Loss: 0.07458949834108353\n",
      "Epoch 5764, Train Loss: 0.024323029443621635, Test Loss: 0.07458162307739258\n",
      "Epoch 5765, Train Loss: 0.024317951872944832, Test Loss: 0.07458356022834778\n",
      "Epoch 5766, Train Loss: 0.02431315742433071, Test Loss: 0.07457428425550461\n",
      "Epoch 5767, Train Loss: 0.024308115243911743, Test Loss: 0.07458874583244324\n",
      "Epoch 5768, Train Loss: 0.024303380399942398, Test Loss: 0.07458066940307617\n",
      "Epoch 5769, Train Loss: 0.02429846115410328, Test Loss: 0.0745813399553299\n",
      "Epoch 5770, Train Loss: 0.02429342269897461, Test Loss: 0.07457457482814789\n",
      "Epoch 5771, Train Loss: 0.024289125576615334, Test Loss: 0.074565589427948\n",
      "Epoch 5772, Train Loss: 0.024283546954393387, Test Loss: 0.07456947863101959\n",
      "Epoch 5773, Train Loss: 0.0242789126932621, Test Loss: 0.07455448061227798\n",
      "Epoch 5774, Train Loss: 0.024273790419101715, Test Loss: 0.0745595321059227\n",
      "Epoch 5775, Train Loss: 0.02426883764564991, Test Loss: 0.07456057518720627\n",
      "Epoch 5776, Train Loss: 0.024263955652713776, Test Loss: 0.07456131279468536\n",
      "Epoch 5777, Train Loss: 0.024258987978100777, Test Loss: 0.07456787675619125\n",
      "Epoch 5778, Train Loss: 0.024254102259874344, Test Loss: 0.07456357777118683\n",
      "Epoch 5779, Train Loss: 0.02424939163029194, Test Loss: 0.07455530017614365\n",
      "Epoch 5780, Train Loss: 0.024245107546448708, Test Loss: 0.07456263154745102\n",
      "Epoch 5781, Train Loss: 0.024239536374807358, Test Loss: 0.07454823702573776\n",
      "Epoch 5782, Train Loss: 0.024235466495156288, Test Loss: 0.07455507665872574\n",
      "Epoch 5783, Train Loss: 0.024229668080806732, Test Loss: 0.07455708831548691\n",
      "Epoch 5784, Train Loss: 0.024224767461419106, Test Loss: 0.07453833520412445\n",
      "Epoch 5785, Train Loss: 0.02422008290886879, Test Loss: 0.07454686611890793\n",
      "Epoch 5786, Train Loss: 0.024214986711740494, Test Loss: 0.07455002516508102\n",
      "Epoch 5787, Train Loss: 0.02421034313738346, Test Loss: 0.07454317808151245\n",
      "Epoch 5788, Train Loss: 0.02420562133193016, Test Loss: 0.07452297955751419\n",
      "Epoch 5789, Train Loss: 0.02420136332511902, Test Loss: 0.07451380789279938\n",
      "Epoch 5790, Train Loss: 0.024195460602641106, Test Loss: 0.07452437281608582\n",
      "Epoch 5791, Train Loss: 0.024191008880734444, Test Loss: 0.07451745867729187\n",
      "Epoch 5792, Train Loss: 0.024185921996831894, Test Loss: 0.07451944053173065\n",
      "Epoch 5793, Train Loss: 0.024181339889764786, Test Loss: 0.0745156779885292\n",
      "Epoch 5794, Train Loss: 0.024176960811018944, Test Loss: 0.074509397149086\n",
      "Epoch 5795, Train Loss: 0.024171117693185806, Test Loss: 0.07451361417770386\n",
      "Epoch 5796, Train Loss: 0.024166462942957878, Test Loss: 0.07451533526182175\n",
      "Epoch 5797, Train Loss: 0.024161724373698235, Test Loss: 0.07451481372117996\n",
      "Epoch 5798, Train Loss: 0.024156644940376282, Test Loss: 0.0745009332895279\n",
      "Epoch 5799, Train Loss: 0.024151498451828957, Test Loss: 0.07450363039970398\n",
      "Epoch 5800, Train Loss: 0.02414746768772602, Test Loss: 0.07449525594711304\n",
      "Epoch 5801, Train Loss: 0.024141941219568253, Test Loss: 0.07449626177549362\n",
      "Epoch 5802, Train Loss: 0.024137122556567192, Test Loss: 0.07449326664209366\n",
      "Epoch 5803, Train Loss: 0.02413219027221203, Test Loss: 0.07449129223823547\n",
      "Epoch 5804, Train Loss: 0.02412732131779194, Test Loss: 0.07449688762426376\n",
      "Epoch 5805, Train Loss: 0.02412259578704834, Test Loss: 0.07450687140226364\n",
      "Epoch 5806, Train Loss: 0.024117937311530113, Test Loss: 0.07450411468744278\n",
      "Epoch 5807, Train Loss: 0.024112796410918236, Test Loss: 0.07449052482843399\n",
      "Epoch 5808, Train Loss: 0.02410813793540001, Test Loss: 0.07447858154773712\n",
      "Epoch 5809, Train Loss: 0.02410300448536873, Test Loss: 0.07447905838489532\n",
      "Epoch 5810, Train Loss: 0.024098370224237442, Test Loss: 0.07449004054069519\n",
      "Epoch 5811, Train Loss: 0.024093661457300186, Test Loss: 0.07448045164346695\n",
      "Epoch 5812, Train Loss: 0.02408871240913868, Test Loss: 0.0744776576757431\n",
      "Epoch 5813, Train Loss: 0.024083558470010757, Test Loss: 0.07447989284992218\n",
      "Epoch 5814, Train Loss: 0.02407902292907238, Test Loss: 0.0744786411523819\n",
      "Epoch 5815, Train Loss: 0.024074288085103035, Test Loss: 0.07447996735572815\n",
      "Epoch 5816, Train Loss: 0.024069296196103096, Test Loss: 0.07448236644268036\n",
      "Epoch 5817, Train Loss: 0.024064306169748306, Test Loss: 0.07447323948144913\n",
      "Epoch 5818, Train Loss: 0.02406017854809761, Test Loss: 0.07447264343500137\n",
      "Epoch 5819, Train Loss: 0.024054842069745064, Test Loss: 0.07447060197591782\n",
      "Epoch 5820, Train Loss: 0.02404964342713356, Test Loss: 0.07447075098752975\n",
      "Epoch 5821, Train Loss: 0.024046342819929123, Test Loss: 0.07447747141122818\n",
      "Epoch 5822, Train Loss: 0.02404022216796875, Test Loss: 0.07446213066577911\n",
      "Epoch 5823, Train Loss: 0.02403496764600277, Test Loss: 0.07446601986885071\n",
      "Epoch 5824, Train Loss: 0.024030443280935287, Test Loss: 0.07446030527353287\n",
      "Epoch 5825, Train Loss: 0.02402561716735363, Test Loss: 0.07445848733186722\n",
      "Epoch 5826, Train Loss: 0.02402118407189846, Test Loss: 0.07444129884243011\n",
      "Epoch 5827, Train Loss: 0.02401662990450859, Test Loss: 0.07443298399448395\n",
      "Epoch 5828, Train Loss: 0.024010896682739258, Test Loss: 0.07444767653942108\n",
      "Epoch 5829, Train Loss: 0.024006612598896027, Test Loss: 0.07443837821483612\n",
      "Epoch 5830, Train Loss: 0.02400161884725094, Test Loss: 0.07443784177303314\n",
      "Epoch 5831, Train Loss: 0.02399667538702488, Test Loss: 0.07444256544113159\n",
      "Epoch 5832, Train Loss: 0.023991521447896957, Test Loss: 0.07444583624601364\n",
      "Epoch 5833, Train Loss: 0.02398775890469551, Test Loss: 0.0744352787733078\n",
      "Epoch 5834, Train Loss: 0.023982273414731026, Test Loss: 0.07444361597299576\n",
      "Epoch 5835, Train Loss: 0.02397707663476467, Test Loss: 0.07443167269229889\n",
      "Epoch 5836, Train Loss: 0.023972513154149055, Test Loss: 0.07442367821931839\n",
      "Epoch 5837, Train Loss: 0.023968392983078957, Test Loss: 0.07442054897546768\n",
      "Epoch 5838, Train Loss: 0.023963144049048424, Test Loss: 0.07441878318786621\n",
      "Epoch 5839, Train Loss: 0.023959102109074593, Test Loss: 0.07441753894090652\n",
      "Epoch 5840, Train Loss: 0.023953517898917198, Test Loss: 0.07440423965454102\n",
      "Epoch 5841, Train Loss: 0.023948388174176216, Test Loss: 0.07440470159053802\n",
      "Epoch 5842, Train Loss: 0.023943206295371056, Test Loss: 0.07441854476928711\n",
      "Epoch 5843, Train Loss: 0.023938629776239395, Test Loss: 0.0744197741150856\n",
      "Epoch 5844, Train Loss: 0.023934002965688705, Test Loss: 0.07441376149654388\n",
      "Epoch 5845, Train Loss: 0.023929079994559288, Test Loss: 0.0744183138012886\n",
      "Epoch 5846, Train Loss: 0.023924553766846657, Test Loss: 0.07440023869276047\n",
      "Epoch 5847, Train Loss: 0.02391967363655567, Test Loss: 0.07441408187150955\n",
      "Epoch 5848, Train Loss: 0.023914644494652748, Test Loss: 0.07441127300262451\n",
      "Epoch 5849, Train Loss: 0.023909594863653183, Test Loss: 0.0744078978896141\n",
      "Epoch 5850, Train Loss: 0.023904724046587944, Test Loss: 0.07441035658121109\n",
      "Epoch 5851, Train Loss: 0.023899909108877182, Test Loss: 0.07440636307001114\n",
      "Epoch 5852, Train Loss: 0.02389533258974552, Test Loss: 0.07439739257097244\n",
      "Epoch 5853, Train Loss: 0.023890359327197075, Test Loss: 0.07440614700317383\n",
      "Epoch 5854, Train Loss: 0.023885754868388176, Test Loss: 0.07440150529146194\n",
      "Epoch 5855, Train Loss: 0.02388126030564308, Test Loss: 0.07439161837100983\n",
      "Epoch 5856, Train Loss: 0.023876158520579338, Test Loss: 0.07438407093286514\n",
      "Epoch 5857, Train Loss: 0.023871317505836487, Test Loss: 0.07437938451766968\n",
      "Epoch 5858, Train Loss: 0.023867204785346985, Test Loss: 0.07436884194612503\n",
      "Epoch 5859, Train Loss: 0.023861665278673172, Test Loss: 0.07438378036022186\n",
      "Epoch 5860, Train Loss: 0.023857194930315018, Test Loss: 0.07439224421977997\n",
      "Epoch 5861, Train Loss: 0.023852551355957985, Test Loss: 0.07438386976718903\n",
      "Epoch 5862, Train Loss: 0.023847069591283798, Test Loss: 0.0743803158402443\n",
      "Epoch 5863, Train Loss: 0.02384248375892639, Test Loss: 0.07437381893396378\n",
      "Epoch 5864, Train Loss: 0.023837929591536522, Test Loss: 0.07438094168901443\n",
      "Epoch 5865, Train Loss: 0.023832840844988823, Test Loss: 0.07437155395746231\n",
      "Epoch 5866, Train Loss: 0.023828525096178055, Test Loss: 0.07437508553266525\n",
      "Epoch 5867, Train Loss: 0.023823602125048637, Test Loss: 0.07436132431030273\n",
      "Epoch 5868, Train Loss: 0.02381943166255951, Test Loss: 0.07436414062976837\n",
      "Epoch 5869, Train Loss: 0.023816509172320366, Test Loss: 0.07436445355415344\n",
      "Epoch 5870, Train Loss: 0.023810092359781265, Test Loss: 0.07437045872211456\n",
      "Epoch 5871, Train Loss: 0.02380533702671528, Test Loss: 0.07437664270401001\n",
      "Epoch 5872, Train Loss: 0.023799920454621315, Test Loss: 0.07436633110046387\n",
      "Epoch 5873, Train Loss: 0.02379448339343071, Test Loss: 0.07435961067676544\n",
      "Epoch 5874, Train Loss: 0.023790482431650162, Test Loss: 0.0743456482887268\n",
      "Epoch 5875, Train Loss: 0.023785771802067757, Test Loss: 0.07435440272092819\n",
      "Epoch 5876, Train Loss: 0.02378070168197155, Test Loss: 0.07436494529247284\n",
      "Epoch 5877, Train Loss: 0.0237753763794899, Test Loss: 0.07435458898544312\n",
      "Epoch 5878, Train Loss: 0.02377118170261383, Test Loss: 0.07435501366853714\n",
      "Epoch 5879, Train Loss: 0.023766659200191498, Test Loss: 0.07435937970876694\n",
      "Epoch 5880, Train Loss: 0.02376122772693634, Test Loss: 0.07434946298599243\n",
      "Epoch 5881, Train Loss: 0.02375650778412819, Test Loss: 0.07434968650341034\n",
      "Epoch 5882, Train Loss: 0.02375168539583683, Test Loss: 0.0743594542145729\n",
      "Epoch 5883, Train Loss: 0.0237466748803854, Test Loss: 0.07434440404176712\n",
      "Epoch 5884, Train Loss: 0.023742493242025375, Test Loss: 0.07433027029037476\n",
      "Epoch 5885, Train Loss: 0.023736925795674324, Test Loss: 0.07434020191431046\n",
      "Epoch 5886, Train Loss: 0.02373235858976841, Test Loss: 0.07433391362428665\n",
      "Epoch 5887, Train Loss: 0.02372761256992817, Test Loss: 0.07433883845806122\n",
      "Epoch 5888, Train Loss: 0.023723343387246132, Test Loss: 0.07432959973812103\n",
      "Epoch 5889, Train Loss: 0.023719053715467453, Test Loss: 0.07432791590690613\n",
      "Epoch 5890, Train Loss: 0.023713186383247375, Test Loss: 0.0743304193019867\n",
      "Epoch 5891, Train Loss: 0.023708529770374298, Test Loss: 0.0743173286318779\n",
      "Epoch 5892, Train Loss: 0.023704135790467262, Test Loss: 0.07432179898023605\n",
      "Epoch 5893, Train Loss: 0.023698652163147926, Test Loss: 0.07432837039232254\n",
      "Epoch 5894, Train Loss: 0.023694004863500595, Test Loss: 0.0743192806839943\n",
      "Epoch 5895, Train Loss: 0.023689402267336845, Test Loss: 0.07431906461715698\n",
      "Epoch 5896, Train Loss: 0.023685267195105553, Test Loss: 0.0743139386177063\n",
      "Epoch 5897, Train Loss: 0.023679738864302635, Test Loss: 0.07430810481309891\n",
      "Epoch 5898, Train Loss: 0.023675326257944107, Test Loss: 0.07431384176015854\n",
      "Epoch 5899, Train Loss: 0.023670384660363197, Test Loss: 0.0743199959397316\n",
      "Epoch 5900, Train Loss: 0.02366553619503975, Test Loss: 0.07431468367576599\n",
      "Epoch 5901, Train Loss: 0.023661058396100998, Test Loss: 0.07429555803537369\n",
      "Epoch 5902, Train Loss: 0.02365618571639061, Test Loss: 0.07429414242506027\n",
      "Epoch 5903, Train Loss: 0.02365175448358059, Test Loss: 0.07429199665784836\n",
      "Epoch 5904, Train Loss: 0.023646339774131775, Test Loss: 0.0743015855550766\n",
      "Epoch 5905, Train Loss: 0.023641614243388176, Test Loss: 0.07430963218212128\n",
      "Epoch 5906, Train Loss: 0.023636745288968086, Test Loss: 0.07429322600364685\n",
      "Epoch 5907, Train Loss: 0.023632284253835678, Test Loss: 0.07428766041994095\n",
      "Epoch 5908, Train Loss: 0.02362767979502678, Test Loss: 0.07427965104579926\n",
      "Epoch 5909, Train Loss: 0.023622967302799225, Test Loss: 0.07427234202623367\n",
      "Epoch 5910, Train Loss: 0.023618074133992195, Test Loss: 0.07427002489566803\n",
      "Epoch 5911, Train Loss: 0.023613419383764267, Test Loss: 0.07427939772605896\n",
      "Epoch 5912, Train Loss: 0.023608585819602013, Test Loss: 0.07428622990846634\n",
      "Epoch 5913, Train Loss: 0.02360575832426548, Test Loss: 0.07428839802742004\n",
      "Epoch 5914, Train Loss: 0.023598643019795418, Test Loss: 0.07428625971078873\n",
      "Epoch 5915, Train Loss: 0.023594103753566742, Test Loss: 0.07428652048110962\n",
      "Epoch 5916, Train Loss: 0.023589380085468292, Test Loss: 0.07427990436553955\n",
      "Epoch 5917, Train Loss: 0.023585297167301178, Test Loss: 0.07426755130290985\n",
      "Epoch 5918, Train Loss: 0.023580098524689674, Test Loss: 0.07426679134368896\n",
      "Epoch 5919, Train Loss: 0.0235750712454319, Test Loss: 0.07426521927118301\n",
      "Epoch 5920, Train Loss: 0.02357024885714054, Test Loss: 0.07427091151475906\n",
      "Epoch 5921, Train Loss: 0.0235657449811697, Test Loss: 0.07426155358552933\n",
      "Epoch 5922, Train Loss: 0.023560985922813416, Test Loss: 0.0742640271782875\n",
      "Epoch 5923, Train Loss: 0.02355639822781086, Test Loss: 0.07426024228334427\n",
      "Epoch 5924, Train Loss: 0.023551909253001213, Test Loss: 0.07425078749656677\n",
      "Epoch 5925, Train Loss: 0.023546861484646797, Test Loss: 0.07424987852573395\n",
      "Epoch 5926, Train Loss: 0.023542489856481552, Test Loss: 0.0742473378777504\n",
      "Epoch 5927, Train Loss: 0.023537442088127136, Test Loss: 0.07425059378147125\n",
      "Epoch 5928, Train Loss: 0.023533085361123085, Test Loss: 0.07424657046794891\n",
      "Epoch 5929, Train Loss: 0.02352779544889927, Test Loss: 0.0742427408695221\n",
      "Epoch 5930, Train Loss: 0.023523632436990738, Test Loss: 0.0742255225777626\n",
      "Epoch 5931, Train Loss: 0.02351829782128334, Test Loss: 0.07424310594797134\n",
      "Epoch 5932, Train Loss: 0.023513972759246826, Test Loss: 0.07423237711191177\n",
      "Epoch 5933, Train Loss: 0.023508915677666664, Test Loss: 0.07423426955938339\n",
      "Epoch 5934, Train Loss: 0.023504365235567093, Test Loss: 0.07424510270357132\n",
      "Epoch 5935, Train Loss: 0.023499928414821625, Test Loss: 0.07424753159284592\n",
      "Epoch 5936, Train Loss: 0.023494891822338104, Test Loss: 0.07424281537532806\n",
      "Epoch 5937, Train Loss: 0.023490004241466522, Test Loss: 0.07423299551010132\n",
      "Epoch 5938, Train Loss: 0.023486049845814705, Test Loss: 0.07423073798418045\n",
      "Epoch 5939, Train Loss: 0.023480698466300964, Test Loss: 0.07423300296068192\n",
      "Epoch 5940, Train Loss: 0.02347620762884617, Test Loss: 0.07422222942113876\n",
      "Epoch 5941, Train Loss: 0.023471128195524216, Test Loss: 0.07422256469726562\n",
      "Epoch 5942, Train Loss: 0.023466406390070915, Test Loss: 0.07423054426908493\n",
      "Epoch 5943, Train Loss: 0.023461682721972466, Test Loss: 0.0742223858833313\n",
      "Epoch 5944, Train Loss: 0.023457229137420654, Test Loss: 0.07421809434890747\n",
      "Epoch 5945, Train Loss: 0.023452721536159515, Test Loss: 0.07421232759952545\n",
      "Epoch 5946, Train Loss: 0.023448418825864792, Test Loss: 0.07421793043613434\n",
      "Epoch 5947, Train Loss: 0.023443112149834633, Test Loss: 0.0742085725069046\n",
      "Epoch 5948, Train Loss: 0.023438185453414917, Test Loss: 0.07421960681676865\n",
      "Epoch 5949, Train Loss: 0.023433592170476913, Test Loss: 0.07422066479921341\n",
      "Epoch 5950, Train Loss: 0.02342931553721428, Test Loss: 0.07420731335878372\n",
      "Epoch 5951, Train Loss: 0.02342432551085949, Test Loss: 0.07420574128627777\n",
      "Epoch 5952, Train Loss: 0.023419702425599098, Test Loss: 0.07420605421066284\n",
      "Epoch 5953, Train Loss: 0.023414667695760727, Test Loss: 0.07420678436756134\n",
      "Epoch 5954, Train Loss: 0.023410595953464508, Test Loss: 0.07419093698263168\n",
      "Epoch 5955, Train Loss: 0.02340538799762726, Test Loss: 0.07419368624687195\n",
      "Epoch 5956, Train Loss: 0.02340131439268589, Test Loss: 0.07420546561479568\n",
      "Epoch 5957, Train Loss: 0.023396536707878113, Test Loss: 0.07419975847005844\n",
      "Epoch 5958, Train Loss: 0.02339138835668564, Test Loss: 0.07418627291917801\n",
      "Epoch 5959, Train Loss: 0.02338670752942562, Test Loss: 0.07419009506702423\n",
      "Epoch 5960, Train Loss: 0.02338268607854843, Test Loss: 0.07418463379144669\n",
      "Epoch 5961, Train Loss: 0.0233773123472929, Test Loss: 0.07418802380561829\n",
      "Epoch 5962, Train Loss: 0.02337251976132393, Test Loss: 0.0741955116391182\n",
      "Epoch 5963, Train Loss: 0.023367902263998985, Test Loss: 0.07418142259120941\n",
      "Epoch 5964, Train Loss: 0.02336372807621956, Test Loss: 0.07419606298208237\n",
      "Epoch 5965, Train Loss: 0.023358585312962532, Test Loss: 0.07418838888406754\n",
      "Epoch 5966, Train Loss: 0.02335401624441147, Test Loss: 0.07417148351669312\n",
      "Epoch 5967, Train Loss: 0.023349009454250336, Test Loss: 0.07417766749858856\n",
      "Epoch 5968, Train Loss: 0.023344548419117928, Test Loss: 0.07417092472314835\n",
      "Epoch 5969, Train Loss: 0.023339733481407166, Test Loss: 0.07418275624513626\n",
      "Epoch 5970, Train Loss: 0.023335110396146774, Test Loss: 0.07417505234479904\n",
      "Epoch 5971, Train Loss: 0.02333040162920952, Test Loss: 0.07416504621505737\n",
      "Epoch 5972, Train Loss: 0.023325452581048012, Test Loss: 0.07417377829551697\n",
      "Epoch 5973, Train Loss: 0.023321187123656273, Test Loss: 0.07415546476840973\n",
      "Epoch 5974, Train Loss: 0.02331646718084812, Test Loss: 0.07416298240423203\n",
      "Epoch 5975, Train Loss: 0.023311814293265343, Test Loss: 0.07417061179876328\n",
      "Epoch 5976, Train Loss: 0.023306801915168762, Test Loss: 0.07416350394487381\n",
      "Epoch 5977, Train Loss: 0.02330280840396881, Test Loss: 0.07414824515581131\n",
      "Epoch 5978, Train Loss: 0.023297665640711784, Test Loss: 0.07415717095136642\n",
      "Epoch 5979, Train Loss: 0.023292843252420425, Test Loss: 0.07417311519384384\n",
      "Epoch 5980, Train Loss: 0.023288484662771225, Test Loss: 0.07416990399360657\n",
      "Epoch 5981, Train Loss: 0.02328363060951233, Test Loss: 0.07416921854019165\n",
      "Epoch 5982, Train Loss: 0.02327915094792843, Test Loss: 0.07415970414876938\n",
      "Epoch 5983, Train Loss: 0.02327430434525013, Test Loss: 0.07415264844894409\n",
      "Epoch 5984, Train Loss: 0.02326950617134571, Test Loss: 0.07414288818836212\n",
      "Epoch 5985, Train Loss: 0.023265261203050613, Test Loss: 0.07414296269416809\n",
      "Epoch 5986, Train Loss: 0.023260686546564102, Test Loss: 0.07414229214191437\n",
      "Epoch 5987, Train Loss: 0.023255838081240654, Test Loss: 0.07413068413734436\n",
      "Epoch 5988, Train Loss: 0.02325071580708027, Test Loss: 0.07413510978221893\n",
      "Epoch 5989, Train Loss: 0.02324633114039898, Test Loss: 0.0741315707564354\n",
      "Epoch 5990, Train Loss: 0.023241864517331123, Test Loss: 0.07412779331207275\n",
      "Epoch 5991, Train Loss: 0.0232367143034935, Test Loss: 0.07413368672132492\n",
      "Epoch 5992, Train Loss: 0.023232508450746536, Test Loss: 0.07413474470376968\n",
      "Epoch 5993, Train Loss: 0.023227833211421967, Test Loss: 0.07413174957036972\n",
      "Epoch 5994, Train Loss: 0.023222943767905235, Test Loss: 0.07413429766893387\n",
      "Epoch 5995, Train Loss: 0.02321867272257805, Test Loss: 0.07411468774080276\n",
      "Epoch 5996, Train Loss: 0.023213902488350868, Test Loss: 0.07411553710699081\n",
      "Epoch 5997, Train Loss: 0.023209210485219955, Test Loss: 0.07411445677280426\n",
      "Epoch 5998, Train Loss: 0.023204544559121132, Test Loss: 0.07412271201610565\n",
      "Epoch 5999, Train Loss: 0.023200208321213722, Test Loss: 0.07413309812545776\n",
      "Epoch 6000, Train Loss: 0.023195352405309677, Test Loss: 0.07412607222795486\n",
      "Epoch 6001, Train Loss: 0.02319079264998436, Test Loss: 0.074116550385952\n",
      "Epoch 6002, Train Loss: 0.02318601869046688, Test Loss: 0.07410281896591187\n",
      "Epoch 6003, Train Loss: 0.02318170852959156, Test Loss: 0.0740971714258194\n",
      "Epoch 6004, Train Loss: 0.023176800459623337, Test Loss: 0.07410890609025955\n",
      "Epoch 6005, Train Loss: 0.023172764107584953, Test Loss: 0.0740964487195015\n",
      "Epoch 6006, Train Loss: 0.02316739223897457, Test Loss: 0.07411786168813705\n",
      "Epoch 6007, Train Loss: 0.02316269837319851, Test Loss: 0.07410786300897598\n",
      "Epoch 6008, Train Loss: 0.02315804362297058, Test Loss: 0.07411200553178787\n",
      "Epoch 6009, Train Loss: 0.023153671994805336, Test Loss: 0.07410403341054916\n",
      "Epoch 6010, Train Loss: 0.02314925566315651, Test Loss: 0.07409801334142685\n",
      "Epoch 6011, Train Loss: 0.02314441092312336, Test Loss: 0.07409622520208359\n",
      "Epoch 6012, Train Loss: 0.02313948981463909, Test Loss: 0.07410044223070145\n",
      "Epoch 6013, Train Loss: 0.02313566766679287, Test Loss: 0.07408150285482407\n",
      "Epoch 6014, Train Loss: 0.023130318149924278, Test Loss: 0.07409155368804932\n",
      "Epoch 6015, Train Loss: 0.02312607876956463, Test Loss: 0.07409346848726273\n",
      "Epoch 6016, Train Loss: 0.023121971637010574, Test Loss: 0.07409105449914932\n",
      "Epoch 6017, Train Loss: 0.023116597905755043, Test Loss: 0.07408967614173889\n",
      "Epoch 6018, Train Loss: 0.023112526163458824, Test Loss: 0.07407578825950623\n",
      "Epoch 6019, Train Loss: 0.02310788258910179, Test Loss: 0.07408484071493149\n",
      "Epoch 6020, Train Loss: 0.023102829232811928, Test Loss: 0.07408434897661209\n",
      "Epoch 6021, Train Loss: 0.023097993806004524, Test Loss: 0.07408381998538971\n",
      "Epoch 6022, Train Loss: 0.023093659430742264, Test Loss: 0.0740756019949913\n",
      "Epoch 6023, Train Loss: 0.02308906614780426, Test Loss: 0.07407040148973465\n",
      "Epoch 6024, Train Loss: 0.02308443933725357, Test Loss: 0.07407251000404358\n",
      "Epoch 6025, Train Loss: 0.023079896345734596, Test Loss: 0.07406264543533325\n",
      "Epoch 6026, Train Loss: 0.02307516150176525, Test Loss: 0.07405813783407211\n",
      "Epoch 6027, Train Loss: 0.023070229217410088, Test Loss: 0.07407049834728241\n",
      "Epoch 6028, Train Loss: 0.023065712302923203, Test Loss: 0.07406432181596756\n",
      "Epoch 6029, Train Loss: 0.023061158135533333, Test Loss: 0.07406491786241531\n",
      "Epoch 6030, Train Loss: 0.023056520149111748, Test Loss: 0.07405819743871689\n",
      "Epoch 6031, Train Loss: 0.023052062839269638, Test Loss: 0.07405417412519455\n",
      "Epoch 6032, Train Loss: 0.02304757386445999, Test Loss: 0.07405894994735718\n",
      "Epoch 6033, Train Loss: 0.023042699322104454, Test Loss: 0.07406382262706757\n",
      "Epoch 6034, Train Loss: 0.023038526996970177, Test Loss: 0.0740431472659111\n",
      "Epoch 6035, Train Loss: 0.02303413115441799, Test Loss: 0.07405400276184082\n",
      "Epoch 6036, Train Loss: 0.023029308766126633, Test Loss: 0.07404975593090057\n",
      "Epoch 6037, Train Loss: 0.023024603724479675, Test Loss: 0.0740428939461708\n",
      "Epoch 6038, Train Loss: 0.02302010916173458, Test Loss: 0.07404671609401703\n",
      "Epoch 6039, Train Loss: 0.02301521599292755, Test Loss: 0.07403977960348129\n",
      "Epoch 6040, Train Loss: 0.023011188954114914, Test Loss: 0.07403329014778137\n",
      "Epoch 6041, Train Loss: 0.023006262257695198, Test Loss: 0.07404118776321411\n",
      "Epoch 6042, Train Loss: 0.023001786321401596, Test Loss: 0.07404430210590363\n",
      "Epoch 6043, Train Loss: 0.022997286170721054, Test Loss: 0.07403112202882767\n",
      "Epoch 6044, Train Loss: 0.022992778569459915, Test Loss: 0.0740264430642128\n",
      "Epoch 6045, Train Loss: 0.022987863048911095, Test Loss: 0.07403025776147842\n",
      "Epoch 6046, Train Loss: 0.022983277216553688, Test Loss: 0.07403139024972916\n",
      "Epoch 6047, Train Loss: 0.022978942841291428, Test Loss: 0.07403167337179184\n",
      "Epoch 6048, Train Loss: 0.022974660620093346, Test Loss: 0.07402588427066803\n",
      "Epoch 6049, Train Loss: 0.022969400510191917, Test Loss: 0.07403367757797241\n",
      "Epoch 6050, Train Loss: 0.02296503446996212, Test Loss: 0.07401951402425766\n",
      "Epoch 6051, Train Loss: 0.022960413247346878, Test Loss: 0.07402823120355606\n",
      "Epoch 6052, Train Loss: 0.022955674678087234, Test Loss: 0.07403190433979034\n",
      "Epoch 6053, Train Loss: 0.02295118197798729, Test Loss: 0.07402567565441132\n",
      "Epoch 6054, Train Loss: 0.022946469485759735, Test Loss: 0.07402687519788742\n",
      "Epoch 6055, Train Loss: 0.02294289320707321, Test Loss: 0.0740165188908577\n",
      "Epoch 6056, Train Loss: 0.022937843576073647, Test Loss: 0.07401089370250702\n",
      "Epoch 6057, Train Loss: 0.02293294295668602, Test Loss: 0.07401289790868759\n",
      "Epoch 6058, Train Loss: 0.022928576916456223, Test Loss: 0.07400257885456085\n",
      "Epoch 6059, Train Loss: 0.02292422577738762, Test Loss: 0.07400162518024445\n",
      "Epoch 6060, Train Loss: 0.02291925996541977, Test Loss: 0.07400102913379669\n",
      "Epoch 6061, Train Loss: 0.022914554923772812, Test Loss: 0.07400790601968765\n",
      "Epoch 6062, Train Loss: 0.022910382598638535, Test Loss: 0.07400158792734146\n",
      "Epoch 6063, Train Loss: 0.02290554903447628, Test Loss: 0.07400607317686081\n",
      "Epoch 6064, Train Loss: 0.02290104702115059, Test Loss: 0.07399262487888336\n",
      "Epoch 6065, Train Loss: 0.022896599024534225, Test Loss: 0.07399864494800568\n",
      "Epoch 6066, Train Loss: 0.022892098873853683, Test Loss: 0.07399456202983856\n",
      "Epoch 6067, Train Loss: 0.022888749837875366, Test Loss: 0.0739787220954895\n",
      "Epoch 6068, Train Loss: 0.0228828564286232, Test Loss: 0.07398326694965363\n",
      "Epoch 6069, Train Loss: 0.022878190502524376, Test Loss: 0.07398759573698044\n",
      "Epoch 6070, Train Loss: 0.02287396974861622, Test Loss: 0.07398977875709534\n",
      "Epoch 6071, Train Loss: 0.022869564592838287, Test Loss: 0.07398619502782822\n",
      "Epoch 6072, Train Loss: 0.022865034639835358, Test Loss: 0.07397978752851486\n",
      "Epoch 6073, Train Loss: 0.022860249504446983, Test Loss: 0.0739830955862999\n",
      "Epoch 6074, Train Loss: 0.022855710238218307, Test Loss: 0.07398758828639984\n",
      "Epoch 6075, Train Loss: 0.022852372378110886, Test Loss: 0.07397884130477905\n",
      "Epoch 6076, Train Loss: 0.022848280146718025, Test Loss: 0.07397183775901794\n",
      "Epoch 6077, Train Loss: 0.02284270152449608, Test Loss: 0.07396368682384491\n",
      "Epoch 6078, Train Loss: 0.022838570177555084, Test Loss: 0.0739743560552597\n",
      "Epoch 6079, Train Loss: 0.022834565490484238, Test Loss: 0.07395191490650177\n",
      "Epoch 6080, Train Loss: 0.022828485816717148, Test Loss: 0.07396143674850464\n",
      "Epoch 6081, Train Loss: 0.022824203595519066, Test Loss: 0.07396282255649567\n",
      "Epoch 6082, Train Loss: 0.022819435223937035, Test Loss: 0.073966845870018\n",
      "Epoch 6083, Train Loss: 0.02281499281525612, Test Loss: 0.07395541667938232\n",
      "Epoch 6084, Train Loss: 0.02281024120748043, Test Loss: 0.07395400106906891\n",
      "Epoch 6085, Train Loss: 0.02280595526099205, Test Loss: 0.07395750284194946\n",
      "Epoch 6086, Train Loss: 0.022801462560892105, Test Loss: 0.0739436000585556\n",
      "Epoch 6087, Train Loss: 0.02279697172343731, Test Loss: 0.07394334673881531\n",
      "Epoch 6088, Train Loss: 0.022792037576436996, Test Loss: 0.07395469397306442\n",
      "Epoch 6089, Train Loss: 0.02278853952884674, Test Loss: 0.07394687086343765\n",
      "Epoch 6090, Train Loss: 0.02278328873217106, Test Loss: 0.07395690679550171\n",
      "Epoch 6091, Train Loss: 0.022778905928134918, Test Loss: 0.07394588738679886\n",
      "Epoch 6092, Train Loss: 0.022775715216994286, Test Loss: 0.0739375576376915\n",
      "Epoch 6093, Train Loss: 0.02277049422264099, Test Loss: 0.07393303513526917\n",
      "Epoch 6094, Train Loss: 0.022765763103961945, Test Loss: 0.07393477112054825\n",
      "Epoch 6095, Train Loss: 0.02276068553328514, Test Loss: 0.07394549250602722\n",
      "Epoch 6096, Train Loss: 0.022756144404411316, Test Loss: 0.07394809275865555\n",
      "Epoch 6097, Train Loss: 0.022751852869987488, Test Loss: 0.07395639270544052\n",
      "Epoch 6098, Train Loss: 0.022747304290533066, Test Loss: 0.07394636422395706\n",
      "Epoch 6099, Train Loss: 0.022742897272109985, Test Loss: 0.07393953204154968\n",
      "Epoch 6100, Train Loss: 0.022738298401236534, Test Loss: 0.07395259290933609\n",
      "Epoch 6101, Train Loss: 0.02273520454764366, Test Loss: 0.07393380999565125\n",
      "Epoch 6102, Train Loss: 0.02272919937968254, Test Loss: 0.07394382357597351\n",
      "Epoch 6103, Train Loss: 0.02272467501461506, Test Loss: 0.07392904907464981\n",
      "Epoch 6104, Train Loss: 0.022721178829669952, Test Loss: 0.07392024248838425\n",
      "Epoch 6105, Train Loss: 0.022716108709573746, Test Loss: 0.0739133208990097\n",
      "Epoch 6106, Train Loss: 0.022711465135216713, Test Loss: 0.07392305135726929\n",
      "Epoch 6107, Train Loss: 0.022707050666213036, Test Loss: 0.0739225521683693\n",
      "Epoch 6108, Train Loss: 0.022702354937791824, Test Loss: 0.07391584664583206\n",
      "Epoch 6109, Train Loss: 0.022697633132338524, Test Loss: 0.07392551749944687\n",
      "Epoch 6110, Train Loss: 0.022693447768688202, Test Loss: 0.07391439378261566\n",
      "Epoch 6111, Train Loss: 0.02268880233168602, Test Loss: 0.07391801476478577\n",
      "Epoch 6112, Train Loss: 0.022684594616293907, Test Loss: 0.07391989231109619\n",
      "Epoch 6113, Train Loss: 0.022679783403873444, Test Loss: 0.07391055673360825\n",
      "Epoch 6114, Train Loss: 0.02267557755112648, Test Loss: 0.07391568273305893\n",
      "Epoch 6115, Train Loss: 0.022670622915029526, Test Loss: 0.07390527427196503\n",
      "Epoch 6116, Train Loss: 0.022666143253445625, Test Loss: 0.0739072784781456\n",
      "Epoch 6117, Train Loss: 0.02266169898211956, Test Loss: 0.07391062378883362\n",
      "Epoch 6118, Train Loss: 0.022657353430986404, Test Loss: 0.07390493899583817\n",
      "Epoch 6119, Train Loss: 0.022652912884950638, Test Loss: 0.07389318197965622\n",
      "Epoch 6120, Train Loss: 0.022648220881819725, Test Loss: 0.07389986515045166\n",
      "Epoch 6121, Train Loss: 0.02264401502907276, Test Loss: 0.07388892024755478\n",
      "Epoch 6122, Train Loss: 0.022639350965619087, Test Loss: 0.07389707118272781\n",
      "Epoch 6123, Train Loss: 0.02263513207435608, Test Loss: 0.07388857007026672\n",
      "Epoch 6124, Train Loss: 0.02263037860393524, Test Loss: 0.07388362288475037\n",
      "Epoch 6125, Train Loss: 0.022626129910349846, Test Loss: 0.0738898515701294\n",
      "Epoch 6126, Train Loss: 0.022621503099799156, Test Loss: 0.07388363778591156\n",
      "Epoch 6127, Train Loss: 0.02261759527027607, Test Loss: 0.07387598603963852\n",
      "Epoch 6128, Train Loss: 0.02261320687830448, Test Loss: 0.07388380169868469\n",
      "Epoch 6129, Train Loss: 0.02260889858007431, Test Loss: 0.07387036085128784\n",
      "Epoch 6130, Train Loss: 0.022603699937462807, Test Loss: 0.07387543469667435\n",
      "Epoch 6131, Train Loss: 0.02259894460439682, Test Loss: 0.07388237118721008\n",
      "Epoch 6132, Train Loss: 0.022594980895519257, Test Loss: 0.07386871427297592\n",
      "Epoch 6133, Train Loss: 0.022590484470129013, Test Loss: 0.07387275993824005\n",
      "Epoch 6134, Train Loss: 0.02258589304983616, Test Loss: 0.07387550175189972\n",
      "Epoch 6135, Train Loss: 0.022581934928894043, Test Loss: 0.07386980205774307\n",
      "Epoch 6136, Train Loss: 0.022576959803700447, Test Loss: 0.07386409491300583\n",
      "Epoch 6137, Train Loss: 0.022572405636310577, Test Loss: 0.07387025654315948\n",
      "Epoch 6138, Train Loss: 0.022567974403500557, Test Loss: 0.07386546581983566\n",
      "Epoch 6139, Train Loss: 0.022563684731721878, Test Loss: 0.0738593339920044\n",
      "Epoch 6140, Train Loss: 0.022559314966201782, Test Loss: 0.07387431710958481\n",
      "Epoch 6141, Train Loss: 0.022554807364940643, Test Loss: 0.07386080920696259\n",
      "Epoch 6142, Train Loss: 0.022550107911229134, Test Loss: 0.07386941462755203\n",
      "Epoch 6143, Train Loss: 0.022545529529452324, Test Loss: 0.07385710626840591\n",
      "Epoch 6144, Train Loss: 0.022541271522641182, Test Loss: 0.07384997606277466\n",
      "Epoch 6145, Train Loss: 0.02253727801144123, Test Loss: 0.07384651899337769\n",
      "Epoch 6146, Train Loss: 0.022532761096954346, Test Loss: 0.07384675741195679\n",
      "Epoch 6147, Train Loss: 0.022527994588017464, Test Loss: 0.07384590804576874\n",
      "Epoch 6148, Train Loss: 0.02252446487545967, Test Loss: 0.07384313642978668\n",
      "Epoch 6149, Train Loss: 0.022519012913107872, Test Loss: 0.07385063171386719\n",
      "Epoch 6150, Train Loss: 0.02251504175364971, Test Loss: 0.07384607195854187\n",
      "Epoch 6151, Train Loss: 0.02251022309064865, Test Loss: 0.07384849339723587\n",
      "Epoch 6152, Train Loss: 0.022505933418869972, Test Loss: 0.07385239750146866\n",
      "Epoch 6153, Train Loss: 0.022501472383737564, Test Loss: 0.07385852932929993\n",
      "Epoch 6154, Train Loss: 0.02249744161963463, Test Loss: 0.07384158670902252\n",
      "Epoch 6155, Train Loss: 0.022492578253149986, Test Loss: 0.07384111732244492\n",
      "Epoch 6156, Train Loss: 0.022488338872790337, Test Loss: 0.07383525371551514\n",
      "Epoch 6157, Train Loss: 0.022483963519334793, Test Loss: 0.07382864505052567\n",
      "Epoch 6158, Train Loss: 0.02247929945588112, Test Loss: 0.07383514940738678\n",
      "Epoch 6159, Train Loss: 0.02247489243745804, Test Loss: 0.07383260130882263\n",
      "Epoch 6160, Train Loss: 0.022470684722065926, Test Loss: 0.07383059710264206\n",
      "Epoch 6161, Train Loss: 0.02246653288602829, Test Loss: 0.07383864372968674\n",
      "Epoch 6162, Train Loss: 0.02246212773025036, Test Loss: 0.0738246962428093\n",
      "Epoch 6163, Train Loss: 0.022458359599113464, Test Loss: 0.07381652295589447\n",
      "Epoch 6164, Train Loss: 0.022453123703598976, Test Loss: 0.07381468266248703\n",
      "Epoch 6165, Train Loss: 0.022448673844337463, Test Loss: 0.07382602989673615\n",
      "Epoch 6166, Train Loss: 0.022444216534495354, Test Loss: 0.07383177429437637\n",
      "Epoch 6167, Train Loss: 0.022440144792199135, Test Loss: 0.0738137811422348\n",
      "Epoch 6168, Train Loss: 0.02243565022945404, Test Loss: 0.07381262630224228\n",
      "Epoch 6169, Train Loss: 0.022430874407291412, Test Loss: 0.07382670789957047\n",
      "Epoch 6170, Train Loss: 0.022426575422286987, Test Loss: 0.07382448762655258\n",
      "Epoch 6171, Train Loss: 0.02242228202521801, Test Loss: 0.07382075488567352\n",
      "Epoch 6172, Train Loss: 0.022417711094021797, Test Loss: 0.07382771372795105\n",
      "Epoch 6173, Train Loss: 0.02241375483572483, Test Loss: 0.07382123917341232\n",
      "Epoch 6174, Train Loss: 0.02240956947207451, Test Loss: 0.07381947338581085\n",
      "Epoch 6175, Train Loss: 0.02240489237010479, Test Loss: 0.07382449507713318\n",
      "Epoch 6176, Train Loss: 0.022400574758648872, Test Loss: 0.07380881160497665\n",
      "Epoch 6177, Train Loss: 0.022396143525838852, Test Loss: 0.07381177693605423\n",
      "Epoch 6178, Train Loss: 0.0223920289427042, Test Loss: 0.07380049675703049\n",
      "Epoch 6179, Train Loss: 0.022387845441699028, Test Loss: 0.07380121201276779\n",
      "Epoch 6180, Train Loss: 0.022382749244570732, Test Loss: 0.07380954921245575\n",
      "Epoch 6181, Train Loss: 0.022378746420145035, Test Loss: 0.07380426675081253\n",
      "Epoch 6182, Train Loss: 0.02237432636320591, Test Loss: 0.07380222529172897\n",
      "Epoch 6183, Train Loss: 0.022369498386979103, Test Loss: 0.07379890233278275\n",
      "Epoch 6184, Train Loss: 0.02236614003777504, Test Loss: 0.07379291206598282\n",
      "Epoch 6185, Train Loss: 0.022361652925610542, Test Loss: 0.0737866684794426\n",
      "Epoch 6186, Train Loss: 0.022357594221830368, Test Loss: 0.07378735393285751\n",
      "Epoch 6187, Train Loss: 0.022352708503603935, Test Loss: 0.0737871453166008\n",
      "Epoch 6188, Train Loss: 0.02234836295247078, Test Loss: 0.0737869068980217\n",
      "Epoch 6189, Train Loss: 0.02234409563243389, Test Loss: 0.07378671318292618\n",
      "Epoch 6190, Train Loss: 0.022339846938848495, Test Loss: 0.07377927005290985\n",
      "Epoch 6191, Train Loss: 0.022334855049848557, Test Loss: 0.07377675175666809\n",
      "Epoch 6192, Train Loss: 0.022330570966005325, Test Loss: 0.07377571612596512\n",
      "Epoch 6193, Train Loss: 0.02232593670487404, Test Loss: 0.07378103584051132\n",
      "Epoch 6194, Train Loss: 0.02232179045677185, Test Loss: 0.07377935200929642\n",
      "Epoch 6195, Train Loss: 0.022317079827189445, Test Loss: 0.07378385961055756\n",
      "Epoch 6196, Train Loss: 0.02231309376657009, Test Loss: 0.07377354800701141\n",
      "Epoch 6197, Train Loss: 0.02230839431285858, Test Loss: 0.0737704187631607\n",
      "Epoch 6198, Train Loss: 0.0223048347979784, Test Loss: 0.0737585797905922\n",
      "Epoch 6199, Train Loss: 0.02230061963200569, Test Loss: 0.07375553995370865\n",
      "Epoch 6200, Train Loss: 0.022295797243714333, Test Loss: 0.07374989241361618\n",
      "Epoch 6201, Train Loss: 0.022291161119937897, Test Loss: 0.07375482469797134\n",
      "Epoch 6202, Train Loss: 0.022286992520093918, Test Loss: 0.07375939190387726\n",
      "Epoch 6203, Train Loss: 0.02228204905986786, Test Loss: 0.07376552373170853\n",
      "Epoch 6204, Train Loss: 0.02227817289531231, Test Loss: 0.07377287745475769\n",
      "Epoch 6205, Train Loss: 0.02227371372282505, Test Loss: 0.07377900183200836\n",
      "Epoch 6206, Train Loss: 0.022269297391176224, Test Loss: 0.07376792281866074\n",
      "Epoch 6207, Train Loss: 0.02226489968597889, Test Loss: 0.07375287264585495\n",
      "Epoch 6208, Train Loss: 0.02226053737103939, Test Loss: 0.07376527786254883\n",
      "Epoch 6209, Train Loss: 0.02225593477487564, Test Loss: 0.07375705242156982\n",
      "Epoch 6210, Train Loss: 0.02225170098245144, Test Loss: 0.07375948131084442\n",
      "Epoch 6211, Train Loss: 0.022247539833188057, Test Loss: 0.07376734912395477\n",
      "Epoch 6212, Train Loss: 0.022243903949856758, Test Loss: 0.07376707345247269\n",
      "Epoch 6213, Train Loss: 0.022238604724407196, Test Loss: 0.07375333458185196\n",
      "Epoch 6214, Train Loss: 0.0222342386841774, Test Loss: 0.07375328242778778\n",
      "Epoch 6215, Train Loss: 0.022229965776205063, Test Loss: 0.07375537604093552\n",
      "Epoch 6216, Train Loss: 0.022226106375455856, Test Loss: 0.07373827695846558\n",
      "Epoch 6217, Train Loss: 0.022221677005290985, Test Loss: 0.07373792678117752\n",
      "Epoch 6218, Train Loss: 0.022217385470867157, Test Loss: 0.07373030483722687\n",
      "Epoch 6219, Train Loss: 0.022212525829672813, Test Loss: 0.07373181730508804\n",
      "Epoch 6220, Train Loss: 0.02220839448273182, Test Loss: 0.07373078167438507\n",
      "Epoch 6221, Train Loss: 0.02220427617430687, Test Loss: 0.07372203469276428\n",
      "Epoch 6222, Train Loss: 0.022199898958206177, Test Loss: 0.07371503114700317\n",
      "Epoch 6223, Train Loss: 0.022195691242814064, Test Loss: 0.07371654361486435\n",
      "Epoch 6224, Train Loss: 0.022190770134329796, Test Loss: 0.07372788339853287\n",
      "Epoch 6225, Train Loss: 0.022186247631907463, Test Loss: 0.07373468577861786\n",
      "Epoch 6226, Train Loss: 0.02218233235180378, Test Loss: 0.07372482120990753\n",
      "Epoch 6227, Train Loss: 0.022177578881382942, Test Loss: 0.0737200677394867\n",
      "Epoch 6228, Train Loss: 0.02217349410057068, Test Loss: 0.07371800392866135\n",
      "Epoch 6229, Train Loss: 0.022169701755046844, Test Loss: 0.07371512055397034\n",
      "Epoch 6230, Train Loss: 0.02216474711894989, Test Loss: 0.07371441274881363\n",
      "Epoch 6231, Train Loss: 0.02216072753071785, Test Loss: 0.07371590286493301\n",
      "Epoch 6232, Train Loss: 0.02215588465332985, Test Loss: 0.07371821999549866\n",
      "Epoch 6233, Train Loss: 0.022151408717036247, Test Loss: 0.07371586561203003\n",
      "Epoch 6234, Train Loss: 0.022147368639707565, Test Loss: 0.07372181862592697\n",
      "Epoch 6235, Train Loss: 0.022142810747027397, Test Loss: 0.07371880859136581\n",
      "Epoch 6236, Train Loss: 0.022138603031635284, Test Loss: 0.07372283190488815\n",
      "Epoch 6237, Train Loss: 0.022134102880954742, Test Loss: 0.07371751964092255\n",
      "Epoch 6238, Train Loss: 0.022130301222205162, Test Loss: 0.07371663302183151\n",
      "Epoch 6239, Train Loss: 0.022125622257590294, Test Loss: 0.07370388507843018\n",
      "Epoch 6240, Train Loss: 0.02212216705083847, Test Loss: 0.07369406521320343\n",
      "Epoch 6241, Train Loss: 0.022117365151643753, Test Loss: 0.07369129359722137\n",
      "Epoch 6242, Train Loss: 0.022112714126706123, Test Loss: 0.07369809597730637\n",
      "Epoch 6243, Train Loss: 0.022108513861894608, Test Loss: 0.07370561361312866\n",
      "Epoch 6244, Train Loss: 0.02210361137986183, Test Loss: 0.07369989156723022\n",
      "Epoch 6245, Train Loss: 0.02209956757724285, Test Loss: 0.07369790226221085\n",
      "Epoch 6246, Train Loss: 0.02209518849849701, Test Loss: 0.07369364798069\n",
      "Epoch 6247, Train Loss: 0.022091059014201164, Test Loss: 0.07369371503591537\n",
      "Epoch 6248, Train Loss: 0.02208654209971428, Test Loss: 0.07368215173482895\n",
      "Epoch 6249, Train Loss: 0.022082550451159477, Test Loss: 0.07368039339780807\n",
      "Epoch 6250, Train Loss: 0.022078737616539, Test Loss: 0.07368314266204834\n",
      "Epoch 6251, Train Loss: 0.022073516622185707, Test Loss: 0.07369104772806168\n",
      "Epoch 6252, Train Loss: 0.02206922136247158, Test Loss: 0.07367680966854095\n",
      "Epoch 6253, Train Loss: 0.022064950317144394, Test Loss: 0.07367924600839615\n",
      "Epoch 6254, Train Loss: 0.022061021998524666, Test Loss: 0.07368646562099457\n",
      "Epoch 6255, Train Loss: 0.02205617167055607, Test Loss: 0.07368605583906174\n",
      "Epoch 6256, Train Loss: 0.022052530199289322, Test Loss: 0.07367630302906036\n",
      "Epoch 6257, Train Loss: 0.022048141807317734, Test Loss: 0.07366681098937988\n",
      "Epoch 6258, Train Loss: 0.022043660283088684, Test Loss: 0.0736709013581276\n",
      "Epoch 6259, Train Loss: 0.022038746625185013, Test Loss: 0.07366809993982315\n",
      "Epoch 6260, Train Loss: 0.022035373374819756, Test Loss: 0.07365568727254868\n",
      "Epoch 6261, Train Loss: 0.022030910477042198, Test Loss: 0.07366646081209183\n",
      "Epoch 6262, Train Loss: 0.022025994956493378, Test Loss: 0.0736607238650322\n",
      "Epoch 6263, Train Loss: 0.022022247314453125, Test Loss: 0.07365971803665161\n",
      "Epoch 6264, Train Loss: 0.022017426788806915, Test Loss: 0.0736653059720993\n",
      "Epoch 6265, Train Loss: 0.0220132227987051, Test Loss: 0.07366436719894409\n",
      "Epoch 6266, Train Loss: 0.022009337320923805, Test Loss: 0.0736466720700264\n",
      "Epoch 6267, Train Loss: 0.022004248574376106, Test Loss: 0.07365509867668152\n",
      "Epoch 6268, Train Loss: 0.022000577300786972, Test Loss: 0.07364339381456375\n",
      "Epoch 6269, Train Loss: 0.021996984258294106, Test Loss: 0.07363864779472351\n",
      "Epoch 6270, Train Loss: 0.02199266105890274, Test Loss: 0.07364299148321152\n",
      "Epoch 6271, Train Loss: 0.02198740281164646, Test Loss: 0.07364244014024734\n",
      "Epoch 6272, Train Loss: 0.02198289893567562, Test Loss: 0.07365109771490097\n",
      "Epoch 6273, Train Loss: 0.02197890914976597, Test Loss: 0.07364661246538162\n",
      "Epoch 6274, Train Loss: 0.02197405695915222, Test Loss: 0.07364945113658905\n",
      "Epoch 6275, Train Loss: 0.021970387548208237, Test Loss: 0.0736386775970459\n",
      "Epoch 6276, Train Loss: 0.021965626627206802, Test Loss: 0.07363652437925339\n",
      "Epoch 6277, Train Loss: 0.021961167454719543, Test Loss: 0.07363782078027725\n",
      "Epoch 6278, Train Loss: 0.02195698954164982, Test Loss: 0.07364172488451004\n",
      "Epoch 6279, Train Loss: 0.021952802315354347, Test Loss: 0.0736386626958847\n",
      "Epoch 6280, Train Loss: 0.02194872312247753, Test Loss: 0.07362625747919083\n",
      "Epoch 6281, Train Loss: 0.021944241598248482, Test Loss: 0.07361812144517899\n",
      "Epoch 6282, Train Loss: 0.021940136328339577, Test Loss: 0.07362613081932068\n",
      "Epoch 6283, Train Loss: 0.02193531207740307, Test Loss: 0.07362976670265198\n",
      "Epoch 6284, Train Loss: 0.021931437775492668, Test Loss: 0.07362522184848785\n",
      "Epoch 6285, Train Loss: 0.02192717045545578, Test Loss: 0.07361821085214615\n",
      "Epoch 6286, Train Loss: 0.021922454237937927, Test Loss: 0.07361967116594315\n",
      "Epoch 6287, Train Loss: 0.021918803453445435, Test Loss: 0.07360832393169403\n",
      "Epoch 6288, Train Loss: 0.021914472803473473, Test Loss: 0.07360019534826279\n",
      "Epoch 6289, Train Loss: 0.021909883245825768, Test Loss: 0.07360447198152542\n",
      "Epoch 6290, Train Loss: 0.02190564200282097, Test Loss: 0.07360111176967621\n",
      "Epoch 6291, Train Loss: 0.021901234984397888, Test Loss: 0.07360444962978363\n",
      "Epoch 6292, Train Loss: 0.021897202357649803, Test Loss: 0.07360449433326721\n",
      "Epoch 6293, Train Loss: 0.021893097087740898, Test Loss: 0.07359746098518372\n",
      "Epoch 6294, Train Loss: 0.02188820205628872, Test Loss: 0.07360910624265671\n",
      "Epoch 6295, Train Loss: 0.02188464067876339, Test Loss: 0.07360518723726273\n",
      "Epoch 6296, Train Loss: 0.02187972702085972, Test Loss: 0.07359740138053894\n",
      "Epoch 6297, Train Loss: 0.021875858306884766, Test Loss: 0.07358971238136292\n",
      "Epoch 6298, Train Loss: 0.02187136560678482, Test Loss: 0.07360490411520004\n",
      "Epoch 6299, Train Loss: 0.02186717838048935, Test Loss: 0.07361960411071777\n",
      "Epoch 6300, Train Loss: 0.021863000467419624, Test Loss: 0.07360910624265671\n",
      "Epoch 6301, Train Loss: 0.021858612075448036, Test Loss: 0.07359890639781952\n",
      "Epoch 6302, Train Loss: 0.021854273974895477, Test Loss: 0.07360291481018066\n",
      "Epoch 6303, Train Loss: 0.021850276738405228, Test Loss: 0.07361099869012833\n",
      "Epoch 6304, Train Loss: 0.021845582872629166, Test Loss: 0.0735895112156868\n",
      "Epoch 6305, Train Loss: 0.02184128761291504, Test Loss: 0.07359351217746735\n",
      "Epoch 6306, Train Loss: 0.021837016567587852, Test Loss: 0.07358518242835999\n",
      "Epoch 6307, Train Loss: 0.021833103150129318, Test Loss: 0.07358595728874207\n",
      "Epoch 6308, Train Loss: 0.02182869054377079, Test Loss: 0.07357325404882431\n",
      "Epoch 6309, Train Loss: 0.021824078634381294, Test Loss: 0.0735698789358139\n",
      "Epoch 6310, Train Loss: 0.02181979827582836, Test Loss: 0.07356772571802139\n",
      "Epoch 6311, Train Loss: 0.02181568183004856, Test Loss: 0.07357031106948853\n",
      "Epoch 6312, Train Loss: 0.021811101585626602, Test Loss: 0.07357221841812134\n",
      "Epoch 6313, Train Loss: 0.021807199344038963, Test Loss: 0.07357508689165115\n",
      "Epoch 6314, Train Loss: 0.021803639829158783, Test Loss: 0.07357765734195709\n",
      "Epoch 6315, Train Loss: 0.021798547357320786, Test Loss: 0.07356183975934982\n",
      "Epoch 6316, Train Loss: 0.02179446816444397, Test Loss: 0.07357374578714371\n",
      "Epoch 6317, Train Loss: 0.021790416911244392, Test Loss: 0.07357565313577652\n",
      "Epoch 6318, Train Loss: 0.021786360070109367, Test Loss: 0.0735701397061348\n",
      "Epoch 6319, Train Loss: 0.021781526505947113, Test Loss: 0.07357151806354523\n",
      "Epoch 6320, Train Loss: 0.021777920424938202, Test Loss: 0.07357114553451538\n",
      "Epoch 6321, Train Loss: 0.021773099899291992, Test Loss: 0.07355772703886032\n",
      "Epoch 6322, Train Loss: 0.021769309416413307, Test Loss: 0.07356026023626328\n",
      "Epoch 6323, Train Loss: 0.02176506258547306, Test Loss: 0.07354545593261719\n",
      "Epoch 6324, Train Loss: 0.021760368719697, Test Loss: 0.07355526834726334\n",
      "Epoch 6325, Train Loss: 0.021755754947662354, Test Loss: 0.07355283200740814\n",
      "Epoch 6326, Train Loss: 0.02175227366387844, Test Loss: 0.07354747503995895\n",
      "Epoch 6327, Train Loss: 0.021748628467321396, Test Loss: 0.07354095578193665\n",
      "Epoch 6328, Train Loss: 0.02174314111471176, Test Loss: 0.0735475942492485\n",
      "Epoch 6329, Train Loss: 0.02173919416964054, Test Loss: 0.07354993373155594\n",
      "Epoch 6330, Train Loss: 0.02173469215631485, Test Loss: 0.07354772835969925\n",
      "Epoch 6331, Train Loss: 0.0217310581356287, Test Loss: 0.0735519602894783\n",
      "Epoch 6332, Train Loss: 0.021726250648498535, Test Loss: 0.07354773581027985\n",
      "Epoch 6333, Train Loss: 0.021722959354519844, Test Loss: 0.07352297753095627\n",
      "Epoch 6334, Train Loss: 0.021718835458159447, Test Loss: 0.0735287219285965\n",
      "Epoch 6335, Train Loss: 0.021713590249419212, Test Loss: 0.07353205978870392\n",
      "Epoch 6336, Train Loss: 0.021709783002734184, Test Loss: 0.07352468371391296\n",
      "Epoch 6337, Train Loss: 0.02170478366315365, Test Loss: 0.07353223115205765\n",
      "Epoch 6338, Train Loss: 0.02170083299279213, Test Loss: 0.07353866845369339\n",
      "Epoch 6339, Train Loss: 0.0216964203864336, Test Loss: 0.07352899760007858\n",
      "Epoch 6340, Train Loss: 0.02169240266084671, Test Loss: 0.07352527230978012\n",
      "Epoch 6341, Train Loss: 0.021687790751457214, Test Loss: 0.07352731376886368\n",
      "Epoch 6342, Train Loss: 0.021683987230062485, Test Loss: 0.07351777702569962\n",
      "Epoch 6343, Train Loss: 0.021679630503058434, Test Loss: 0.07351721823215485\n",
      "Epoch 6344, Train Loss: 0.021675679832696915, Test Loss: 0.07350977510213852\n",
      "Epoch 6345, Train Loss: 0.021670788526535034, Test Loss: 0.07351355999708176\n",
      "Epoch 6346, Train Loss: 0.02166675217449665, Test Loss: 0.07351259142160416\n",
      "Epoch 6347, Train Loss: 0.02166307531297207, Test Loss: 0.07350566238164902\n",
      "Epoch 6348, Train Loss: 0.02165832929313183, Test Loss: 0.0735158771276474\n",
      "Epoch 6349, Train Loss: 0.02165459655225277, Test Loss: 0.0735003873705864\n",
      "Epoch 6350, Train Loss: 0.021649884060025215, Test Loss: 0.07350731641054153\n",
      "Epoch 6351, Train Loss: 0.021645652130246162, Test Loss: 0.07351565361022949\n",
      "Epoch 6352, Train Loss: 0.021642090752720833, Test Loss: 0.07350369542837143\n",
      "Epoch 6353, Train Loss: 0.021637221798300743, Test Loss: 0.07350625842809677\n",
      "Epoch 6354, Train Loss: 0.021632740274071693, Test Loss: 0.07350435107946396\n",
      "Epoch 6355, Train Loss: 0.02162853442132473, Test Loss: 0.0734965056180954\n",
      "Epoch 6356, Train Loss: 0.021624401211738586, Test Loss: 0.07349986582994461\n",
      "Epoch 6357, Train Loss: 0.02162032388150692, Test Loss: 0.07350567728281021\n",
      "Epoch 6358, Train Loss: 0.02161644771695137, Test Loss: 0.07349205017089844\n",
      "Epoch 6359, Train Loss: 0.02161191962659359, Test Loss: 0.07349217683076859\n",
      "Epoch 6360, Train Loss: 0.02160750702023506, Test Loss: 0.07350166887044907\n",
      "Epoch 6361, Train Loss: 0.021603496745228767, Test Loss: 0.07350624352693558\n",
      "Epoch 6362, Train Loss: 0.021599186584353447, Test Loss: 0.07348857820034027\n",
      "Epoch 6363, Train Loss: 0.021595047786831856, Test Loss: 0.07348179817199707\n",
      "Epoch 6364, Train Loss: 0.021590538322925568, Test Loss: 0.07348401099443436\n",
      "Epoch 6365, Train Loss: 0.021586356684565544, Test Loss: 0.0734851136803627\n",
      "Epoch 6366, Train Loss: 0.021582383662462234, Test Loss: 0.07349210232496262\n",
      "Epoch 6367, Train Loss: 0.021578161045908928, Test Loss: 0.0734851062297821\n",
      "Epoch 6368, Train Loss: 0.0215743500739336, Test Loss: 0.07349167764186859\n",
      "Epoch 6369, Train Loss: 0.021569661796092987, Test Loss: 0.07348690927028656\n",
      "Epoch 6370, Train Loss: 0.02156558819115162, Test Loss: 0.07348146289587021\n",
      "Epoch 6371, Train Loss: 0.021561356261372566, Test Loss: 0.07348299771547318\n",
      "Epoch 6372, Train Loss: 0.021556803956627846, Test Loss: 0.0734725072979927\n",
      "Epoch 6373, Train Loss: 0.021553125232458115, Test Loss: 0.07347477972507477\n",
      "Epoch 6374, Train Loss: 0.021548621356487274, Test Loss: 0.07346286624670029\n",
      "Epoch 6375, Train Loss: 0.021544242277741432, Test Loss: 0.07346566021442413\n",
      "Epoch 6376, Train Loss: 0.02154037542641163, Test Loss: 0.07346993684768677\n",
      "Epoch 6377, Train Loss: 0.021535834297537804, Test Loss: 0.07346010208129883\n",
      "Epoch 6378, Train Loss: 0.021531907841563225, Test Loss: 0.07346365600824356\n",
      "Epoch 6379, Train Loss: 0.021527480334043503, Test Loss: 0.07345552742481232\n",
      "Epoch 6380, Train Loss: 0.021523579955101013, Test Loss: 0.07344972342252731\n",
      "Epoch 6381, Train Loss: 0.02151912823319435, Test Loss: 0.07345892488956451\n",
      "Epoch 6382, Train Loss: 0.021515099331736565, Test Loss: 0.07345812022686005\n",
      "Epoch 6383, Train Loss: 0.02151091769337654, Test Loss: 0.0734545961022377\n",
      "Epoch 6384, Train Loss: 0.0215064138174057, Test Loss: 0.07345561683177948\n",
      "Epoch 6385, Train Loss: 0.02150261215865612, Test Loss: 0.07345087081193924\n",
      "Epoch 6386, Train Loss: 0.021498093381524086, Test Loss: 0.07344840466976166\n",
      "Epoch 6387, Train Loss: 0.02149413712322712, Test Loss: 0.07344003021717072\n",
      "Epoch 6388, Train Loss: 0.021489979699254036, Test Loss: 0.07344888150691986\n",
      "Epoch 6389, Train Loss: 0.021485598757863045, Test Loss: 0.07343808561563492\n",
      "Epoch 6390, Train Loss: 0.021481215953826904, Test Loss: 0.07343411445617676\n",
      "Epoch 6391, Train Loss: 0.021477749571204185, Test Loss: 0.07344213873147964\n",
      "Epoch 6392, Train Loss: 0.021473029628396034, Test Loss: 0.07344333827495575\n",
      "Epoch 6393, Train Loss: 0.0214686281979084, Test Loss: 0.0734298899769783\n",
      "Epoch 6394, Train Loss: 0.021465463563799858, Test Loss: 0.0734211802482605\n",
      "Epoch 6395, Train Loss: 0.021461453288793564, Test Loss: 0.0734143778681755\n",
      "Epoch 6396, Train Loss: 0.021456366404891014, Test Loss: 0.0734231173992157\n",
      "Epoch 6397, Train Loss: 0.021452268585562706, Test Loss: 0.07342719286680222\n",
      "Epoch 6398, Train Loss: 0.021447695791721344, Test Loss: 0.0734260082244873\n",
      "Epoch 6399, Train Loss: 0.02144358493387699, Test Loss: 0.07342125475406647\n",
      "Epoch 6400, Train Loss: 0.021439287811517715, Test Loss: 0.07342562824487686\n",
      "Epoch 6401, Train Loss: 0.021435532718896866, Test Loss: 0.07341249287128448\n",
      "Epoch 6402, Train Loss: 0.021431000903248787, Test Loss: 0.07342375814914703\n",
      "Epoch 6403, Train Loss: 0.021426906809210777, Test Loss: 0.07341550290584564\n",
      "Epoch 6404, Train Loss: 0.02142276056110859, Test Loss: 0.07340861856937408\n",
      "Epoch 6405, Train Loss: 0.02141929604113102, Test Loss: 0.07340139150619507\n",
      "Epoch 6406, Train Loss: 0.02141456864774227, Test Loss: 0.0734100341796875\n",
      "Epoch 6407, Train Loss: 0.021410241723060608, Test Loss: 0.07341210544109344\n",
      "Epoch 6408, Train Loss: 0.0214071162045002, Test Loss: 0.07341207563877106\n",
      "Epoch 6409, Train Loss: 0.02140181139111519, Test Loss: 0.07340625673532486\n",
      "Epoch 6410, Train Loss: 0.021397961303591728, Test Loss: 0.07340448349714279\n",
      "Epoch 6411, Train Loss: 0.02139347791671753, Test Loss: 0.07340875267982483\n",
      "Epoch 6412, Train Loss: 0.021390274167060852, Test Loss: 0.07340572029352188\n",
      "Epoch 6413, Train Loss: 0.02138540707528591, Test Loss: 0.07340631633996964\n",
      "Epoch 6414, Train Loss: 0.02138134464621544, Test Loss: 0.07339227944612503\n",
      "Epoch 6415, Train Loss: 0.02137753926217556, Test Loss: 0.07339346408843994\n",
      "Epoch 6416, Train Loss: 0.021373340860009193, Test Loss: 0.07339842617511749\n",
      "Epoch 6417, Train Loss: 0.021368753165006638, Test Loss: 0.07339587062597275\n",
      "Epoch 6418, Train Loss: 0.021364547312259674, Test Loss: 0.07339058071374893\n",
      "Epoch 6419, Train Loss: 0.021360673010349274, Test Loss: 0.07338827103376389\n",
      "Epoch 6420, Train Loss: 0.021356798708438873, Test Loss: 0.07337913662195206\n",
      "Epoch 6421, Train Loss: 0.021352579817175865, Test Loss: 0.07337459176778793\n",
      "Epoch 6422, Train Loss: 0.0213483739644289, Test Loss: 0.07337550073862076\n",
      "Epoch 6423, Train Loss: 0.021344030275940895, Test Loss: 0.0733729675412178\n",
      "Epoch 6424, Train Loss: 0.021339790895581245, Test Loss: 0.07337895035743713\n",
      "Epoch 6425, Train Loss: 0.021335545927286148, Test Loss: 0.07337670028209686\n",
      "Epoch 6426, Train Loss: 0.021331490948796272, Test Loss: 0.07338007539510727\n",
      "Epoch 6427, Train Loss: 0.02132798358798027, Test Loss: 0.07336648553609848\n",
      "Epoch 6428, Train Loss: 0.02132371813058853, Test Loss: 0.0733681172132492\n",
      "Epoch 6429, Train Loss: 0.021319547668099403, Test Loss: 0.07337839156389236\n",
      "Epoch 6430, Train Loss: 0.021315136924386024, Test Loss: 0.07337523251771927\n",
      "Epoch 6431, Train Loss: 0.02131098136305809, Test Loss: 0.0733737125992775\n",
      "Epoch 6432, Train Loss: 0.02130703441798687, Test Loss: 0.07337900251150131\n",
      "Epoch 6433, Train Loss: 0.021303223446011543, Test Loss: 0.07336477935314178\n",
      "Epoch 6434, Train Loss: 0.021298499777913094, Test Loss: 0.07337231189012527\n",
      "Epoch 6435, Train Loss: 0.02129487693309784, Test Loss: 0.07335863262414932\n",
      "Epoch 6436, Train Loss: 0.021290147677063942, Test Loss: 0.07336458563804626\n",
      "Epoch 6437, Train Loss: 0.02128644287586212, Test Loss: 0.0733666718006134\n",
      "Epoch 6438, Train Loss: 0.021282047033309937, Test Loss: 0.07336065173149109\n",
      "Epoch 6439, Train Loss: 0.02127791754901409, Test Loss: 0.07335658371448517\n",
      "Epoch 6440, Train Loss: 0.021274132654070854, Test Loss: 0.07335256785154343\n",
      "Epoch 6441, Train Loss: 0.021270141005516052, Test Loss: 0.07335811108350754\n",
      "Epoch 6442, Train Loss: 0.021265564486384392, Test Loss: 0.07336408644914627\n",
      "Epoch 6443, Train Loss: 0.021261436864733696, Test Loss: 0.07335595786571503\n",
      "Epoch 6444, Train Loss: 0.021257443353533745, Test Loss: 0.07335414737462997\n",
      "Epoch 6445, Train Loss: 0.021253643557429314, Test Loss: 0.0733572393655777\n",
      "Epoch 6446, Train Loss: 0.02124962769448757, Test Loss: 0.07335411757230759\n",
      "Epoch 6447, Train Loss: 0.02124512754380703, Test Loss: 0.07335066050291061\n",
      "Epoch 6448, Train Loss: 0.02124115824699402, Test Loss: 0.07333754003047943\n",
      "Epoch 6449, Train Loss: 0.02123687043786049, Test Loss: 0.07334297895431519\n",
      "Epoch 6450, Train Loss: 0.02123326249420643, Test Loss: 0.07334613054990768\n",
      "Epoch 6451, Train Loss: 0.02122916467487812, Test Loss: 0.07335001975297928\n",
      "Epoch 6452, Train Loss: 0.021224625408649445, Test Loss: 0.07333488762378693\n",
      "Epoch 6453, Train Loss: 0.02122042514383793, Test Loss: 0.07332916557788849\n",
      "Epoch 6454, Train Loss: 0.02121671661734581, Test Loss: 0.07333395630121231\n",
      "Epoch 6455, Train Loss: 0.021212445572018623, Test Loss: 0.07332149147987366\n",
      "Epoch 6456, Train Loss: 0.021208470687270164, Test Loss: 0.07331745326519012\n",
      "Epoch 6457, Train Loss: 0.021204335615038872, Test Loss: 0.07332318276166916\n",
      "Epoch 6458, Train Loss: 0.021200183779001236, Test Loss: 0.07334072887897491\n",
      "Epoch 6459, Train Loss: 0.021195977926254272, Test Loss: 0.07333213835954666\n",
      "Epoch 6460, Train Loss: 0.021191835403442383, Test Loss: 0.07332854717969894\n",
      "Epoch 6461, Train Loss: 0.021187737584114075, Test Loss: 0.073317751288414\n",
      "Epoch 6462, Train Loss: 0.021184192970395088, Test Loss: 0.07331427186727524\n",
      "Epoch 6463, Train Loss: 0.021179741248488426, Test Loss: 0.07331973314285278\n",
      "Epoch 6464, Train Loss: 0.02117580734193325, Test Loss: 0.07331406325101852\n",
      "Epoch 6465, Train Loss: 0.02117151953279972, Test Loss: 0.07331640273332596\n",
      "Epoch 6466, Train Loss: 0.02116740308701992, Test Loss: 0.07331035286188126\n",
      "Epoch 6467, Train Loss: 0.02116398513317108, Test Loss: 0.07331999391317368\n",
      "Epoch 6468, Train Loss: 0.0211595818400383, Test Loss: 0.07331227511167526\n",
      "Epoch 6469, Train Loss: 0.021155746653676033, Test Loss: 0.07330559194087982\n",
      "Epoch 6470, Train Loss: 0.02115102857351303, Test Loss: 0.07331537455320358\n",
      "Epoch 6471, Train Loss: 0.02114778384566307, Test Loss: 0.07329750806093216\n",
      "Epoch 6472, Train Loss: 0.021143045276403427, Test Loss: 0.07329913973808289\n",
      "Epoch 6473, Train Loss: 0.02113869972527027, Test Loss: 0.0733085498213768\n",
      "Epoch 6474, Train Loss: 0.021134627982974052, Test Loss: 0.07330603897571564\n",
      "Epoch 6475, Train Loss: 0.021131092682480812, Test Loss: 0.07329777628183365\n",
      "Epoch 6476, Train Loss: 0.021126553416252136, Test Loss: 0.07329796254634857\n",
      "Epoch 6477, Train Loss: 0.021122390404343605, Test Loss: 0.07330381125211716\n",
      "Epoch 6478, Train Loss: 0.02111848257482052, Test Loss: 0.07330165803432465\n",
      "Epoch 6479, Train Loss: 0.02111455239355564, Test Loss: 0.07330022007226944\n",
      "Epoch 6480, Train Loss: 0.02111036889255047, Test Loss: 0.0732865110039711\n",
      "Epoch 6481, Train Loss: 0.021106315776705742, Test Loss: 0.07329189777374268\n",
      "Epoch 6482, Train Loss: 0.021102173253893852, Test Loss: 0.07328757643699646\n",
      "Epoch 6483, Train Loss: 0.021098557859659195, Test Loss: 0.07329097390174866\n",
      "Epoch 6484, Train Loss: 0.021094083786010742, Test Loss: 0.07328139245510101\n",
      "Epoch 6485, Train Loss: 0.021090004593133926, Test Loss: 0.07328502833843231\n",
      "Epoch 6486, Train Loss: 0.021086128428578377, Test Loss: 0.07328028976917267\n",
      "Epoch 6487, Train Loss: 0.021081864833831787, Test Loss: 0.07328411936759949\n",
      "Epoch 6488, Train Loss: 0.021078163757920265, Test Loss: 0.07327441871166229\n",
      "Epoch 6489, Train Loss: 0.02107382006943226, Test Loss: 0.0732710063457489\n",
      "Epoch 6490, Train Loss: 0.021069660782814026, Test Loss: 0.0732884630560875\n",
      "Epoch 6491, Train Loss: 0.021066518500447273, Test Loss: 0.07329128682613373\n",
      "Epoch 6492, Train Loss: 0.02106163091957569, Test Loss: 0.07327596098184586\n",
      "Epoch 6493, Train Loss: 0.021058164536952972, Test Loss: 0.073268361389637\n",
      "Epoch 6494, Train Loss: 0.021054385229945183, Test Loss: 0.07325151562690735\n",
      "Epoch 6495, Train Loss: 0.021049359813332558, Test Loss: 0.07326970249414444\n",
      "Epoch 6496, Train Loss: 0.021045297384262085, Test Loss: 0.07327380776405334\n",
      "Epoch 6497, Train Loss: 0.021042151376605034, Test Loss: 0.07325819879770279\n",
      "Epoch 6498, Train Loss: 0.021037355065345764, Test Loss: 0.07325474172830582\n",
      "Epoch 6499, Train Loss: 0.021033454686403275, Test Loss: 0.07325372844934464\n",
      "Epoch 6500, Train Loss: 0.021030012518167496, Test Loss: 0.07324200868606567\n",
      "Epoch 6501, Train Loss: 0.021025478839874268, Test Loss: 0.0732470452785492\n",
      "Epoch 6502, Train Loss: 0.021021496504545212, Test Loss: 0.07324342429637909\n",
      "Epoch 6503, Train Loss: 0.021018071100115776, Test Loss: 0.07323794811964035\n",
      "Epoch 6504, Train Loss: 0.021014150232076645, Test Loss: 0.07323305308818817\n",
      "Epoch 6505, Train Loss: 0.02101002261042595, Test Loss: 0.07323410362005234\n",
      "Epoch 6506, Train Loss: 0.021005189046263695, Test Loss: 0.07323778420686722\n",
      "Epoch 6507, Train Loss: 0.02100113406777382, Test Loss: 0.07323930412530899\n",
      "Epoch 6508, Train Loss: 0.020997539162635803, Test Loss: 0.07323945313692093\n",
      "Epoch 6509, Train Loss: 0.020992787554860115, Test Loss: 0.07323276251554489\n",
      "Epoch 6510, Train Loss: 0.020988648757338524, Test Loss: 0.07323464006185532\n",
      "Epoch 6511, Train Loss: 0.020984791219234467, Test Loss: 0.07323761284351349\n",
      "Epoch 6512, Train Loss: 0.0209819246083498, Test Loss: 0.07322721183300018\n",
      "Epoch 6513, Train Loss: 0.02097654528915882, Test Loss: 0.07323312759399414\n",
      "Epoch 6514, Train Loss: 0.02097254991531372, Test Loss: 0.07323204725980759\n",
      "Epoch 6515, Train Loss: 0.020968560129404068, Test Loss: 0.07323542982339859\n",
      "Epoch 6516, Train Loss: 0.020964723080396652, Test Loss: 0.07322697341442108\n",
      "Epoch 6517, Train Loss: 0.020960992202162743, Test Loss: 0.07322833687067032\n",
      "Epoch 6518, Train Loss: 0.020956847816705704, Test Loss: 0.07322283834218979\n",
      "Epoch 6519, Train Loss: 0.020952744409441948, Test Loss: 0.07321412116289139\n",
      "Epoch 6520, Train Loss: 0.020948434248566628, Test Loss: 0.07321881502866745\n",
      "Epoch 6521, Train Loss: 0.020944101735949516, Test Loss: 0.07322409749031067\n",
      "Epoch 6522, Train Loss: 0.020940538495779037, Test Loss: 0.07322274893522263\n",
      "Epoch 6523, Train Loss: 0.020936330780386925, Test Loss: 0.07321533560752869\n",
      "Epoch 6524, Train Loss: 0.020932229235768318, Test Loss: 0.07321219146251678\n",
      "Epoch 6525, Train Loss: 0.020928392186760902, Test Loss: 0.07321082800626755\n",
      "Epoch 6526, Train Loss: 0.020924290642142296, Test Loss: 0.07320625334978104\n",
      "Epoch 6527, Train Loss: 0.020920801907777786, Test Loss: 0.07320026308298111\n",
      "Epoch 6528, Train Loss: 0.020916827023029327, Test Loss: 0.07319484651088715\n",
      "Epoch 6529, Train Loss: 0.020912107080221176, Test Loss: 0.0732099637389183\n",
      "Epoch 6530, Train Loss: 0.020908348262310028, Test Loss: 0.07319966703653336\n",
      "Epoch 6531, Train Loss: 0.020904207602143288, Test Loss: 0.07320351153612137\n",
      "Epoch 6532, Train Loss: 0.020900655537843704, Test Loss: 0.0731893926858902\n",
      "Epoch 6533, Train Loss: 0.020896805450320244, Test Loss: 0.07318196445703506\n",
      "Epoch 6534, Train Loss: 0.020892063155770302, Test Loss: 0.07318945974111557\n",
      "Epoch 6535, Train Loss: 0.020888112485408783, Test Loss: 0.07319355010986328\n",
      "Epoch 6536, Train Loss: 0.02088398113846779, Test Loss: 0.07319939136505127\n",
      "Epoch 6537, Train Loss: 0.02088044211268425, Test Loss: 0.0731971338391304\n",
      "Epoch 6538, Train Loss: 0.020876072347164154, Test Loss: 0.07318654656410217\n",
      "Epoch 6539, Train Loss: 0.020872170105576515, Test Loss: 0.07318176329135895\n",
      "Epoch 6540, Train Loss: 0.02086867392063141, Test Loss: 0.07317186146974564\n",
      "Epoch 6541, Train Loss: 0.020863745361566544, Test Loss: 0.07319330424070358\n",
      "Epoch 6542, Train Loss: 0.020859887823462486, Test Loss: 0.07318977266550064\n",
      "Epoch 6543, Train Loss: 0.020855838432908058, Test Loss: 0.07317706942558289\n",
      "Epoch 6544, Train Loss: 0.020852236077189445, Test Loss: 0.07317125052213669\n",
      "Epoch 6545, Train Loss: 0.02084789238870144, Test Loss: 0.07317609339952469\n",
      "Epoch 6546, Train Loss: 0.020843911916017532, Test Loss: 0.0731920525431633\n",
      "Epoch 6547, Train Loss: 0.020839573815464973, Test Loss: 0.07318495213985443\n",
      "Epoch 6548, Train Loss: 0.02083556540310383, Test Loss: 0.07318336516618729\n",
      "Epoch 6549, Train Loss: 0.020831773057579994, Test Loss: 0.07317269593477249\n",
      "Epoch 6550, Train Loss: 0.02082890458405018, Test Loss: 0.07315715402364731\n",
      "Epoch 6551, Train Loss: 0.02082405984401703, Test Loss: 0.07315693795681\n",
      "Epoch 6552, Train Loss: 0.02081972174346447, Test Loss: 0.07316529750823975\n",
      "Epoch 6553, Train Loss: 0.020815707743167877, Test Loss: 0.0731581598520279\n",
      "Epoch 6554, Train Loss: 0.020812293514609337, Test Loss: 0.07315260916948318\n",
      "Epoch 6555, Train Loss: 0.020807748660445213, Test Loss: 0.07315429300069809\n",
      "Epoch 6556, Train Loss: 0.02080363780260086, Test Loss: 0.0731532871723175\n",
      "Epoch 6557, Train Loss: 0.020800888538360596, Test Loss: 0.07315461337566376\n",
      "Epoch 6558, Train Loss: 0.02079588919878006, Test Loss: 0.07316077500581741\n",
      "Epoch 6559, Train Loss: 0.020792268216609955, Test Loss: 0.0731390118598938\n",
      "Epoch 6560, Train Loss: 0.02078770287334919, Test Loss: 0.07313405722379684\n",
      "Epoch 6561, Train Loss: 0.020784171298146248, Test Loss: 0.07313331216573715\n",
      "Epoch 6562, Train Loss: 0.020780052989721298, Test Loss: 0.07313375920057297\n",
      "Epoch 6563, Train Loss: 0.02077624946832657, Test Loss: 0.07313356548547745\n",
      "Epoch 6564, Train Loss: 0.02077193185687065, Test Loss: 0.07313963770866394\n",
      "Epoch 6565, Train Loss: 0.020767806097865105, Test Loss: 0.07313299924135208\n",
      "Epoch 6566, Train Loss: 0.020764214918017387, Test Loss: 0.07312799245119095\n",
      "Epoch 6567, Train Loss: 0.02075989358127117, Test Loss: 0.07312872260808945\n",
      "Epoch 6568, Train Loss: 0.020757034420967102, Test Loss: 0.07313211262226105\n",
      "Epoch 6569, Train Loss: 0.02075222134590149, Test Loss: 0.07314124703407288\n",
      "Epoch 6570, Train Loss: 0.020747806876897812, Test Loss: 0.0731368437409401\n",
      "Epoch 6571, Train Loss: 0.02074429951608181, Test Loss: 0.07312384247779846\n",
      "Epoch 6572, Train Loss: 0.020739641040563583, Test Loss: 0.07312794774770737\n",
      "Epoch 6573, Train Loss: 0.02073577605187893, Test Loss: 0.073126882314682\n",
      "Epoch 6574, Train Loss: 0.020731795579195023, Test Loss: 0.07312432676553726\n",
      "Epoch 6575, Train Loss: 0.02072790637612343, Test Loss: 0.07312561571598053\n",
      "Epoch 6576, Train Loss: 0.02072404883801937, Test Loss: 0.07312359660863876\n",
      "Epoch 6577, Train Loss: 0.020719971507787704, Test Loss: 0.07311969250440598\n",
      "Epoch 6578, Train Loss: 0.020716369152069092, Test Loss: 0.07310748100280762\n",
      "Epoch 6579, Train Loss: 0.02071223221719265, Test Loss: 0.0731077641248703\n",
      "Epoch 6580, Train Loss: 0.020708035677671432, Test Loss: 0.07310561090707779\n",
      "Epoch 6581, Train Loss: 0.02070404775440693, Test Loss: 0.07311038672924042\n",
      "Epoch 6582, Train Loss: 0.020701535046100616, Test Loss: 0.07310660928487778\n",
      "Epoch 6583, Train Loss: 0.020696386694908142, Test Loss: 0.0731043741106987\n",
      "Epoch 6584, Train Loss: 0.020691776648163795, Test Loss: 0.07310865074396133\n",
      "Epoch 6585, Train Loss: 0.020688330754637718, Test Loss: 0.0730954185128212\n",
      "Epoch 6586, Train Loss: 0.020684299990534782, Test Loss: 0.0731000304222107\n",
      "Epoch 6587, Train Loss: 0.020680096000432968, Test Loss: 0.07310536503791809\n",
      "Epoch 6588, Train Loss: 0.020676378160715103, Test Loss: 0.07310066372156143\n",
      "Epoch 6589, Train Loss: 0.02067207731306553, Test Loss: 0.07310014963150024\n",
      "Epoch 6590, Train Loss: 0.020667878910899162, Test Loss: 0.07309616357088089\n",
      "Epoch 6591, Train Loss: 0.020664041861891747, Test Loss: 0.07309321314096451\n",
      "Epoch 6592, Train Loss: 0.020660221576690674, Test Loss: 0.07310155034065247\n",
      "Epoch 6593, Train Loss: 0.02065608650445938, Test Loss: 0.07309551537036896\n",
      "Epoch 6594, Train Loss: 0.020652787759900093, Test Loss: 0.07309030741453171\n",
      "Epoch 6595, Train Loss: 0.020648831501603127, Test Loss: 0.07309611886739731\n",
      "Epoch 6596, Train Loss: 0.020644688978791237, Test Loss: 0.07308061420917511\n",
      "Epoch 6597, Train Loss: 0.02064049243927002, Test Loss: 0.07308372855186462\n",
      "Epoch 6598, Train Loss: 0.020636552944779396, Test Loss: 0.07308116555213928\n",
      "Epoch 6599, Train Loss: 0.020632103085517883, Test Loss: 0.07307964563369751\n",
      "Epoch 6600, Train Loss: 0.020628128200769424, Test Loss: 0.07308027893304825\n",
      "Epoch 6601, Train Loss: 0.020625367760658264, Test Loss: 0.07306937873363495\n",
      "Epoch 6602, Train Loss: 0.020620957016944885, Test Loss: 0.07307447493076324\n",
      "Epoch 6603, Train Loss: 0.020617356523871422, Test Loss: 0.07307020574808121\n",
      "Epoch 6604, Train Loss: 0.02061302587389946, Test Loss: 0.07306405156850815\n",
      "Epoch 6605, Train Loss: 0.020609911531209946, Test Loss: 0.07304900884628296\n",
      "Epoch 6606, Train Loss: 0.02060551568865776, Test Loss: 0.07304973155260086\n",
      "Epoch 6607, Train Loss: 0.020600849762558937, Test Loss: 0.07307286560535431\n",
      "Epoch 6608, Train Loss: 0.02059641107916832, Test Loss: 0.07306740432977676\n",
      "Epoch 6609, Train Loss: 0.020592564716935158, Test Loss: 0.07306712865829468\n",
      "Epoch 6610, Train Loss: 0.020588340237736702, Test Loss: 0.07306893914937973\n",
      "Epoch 6611, Train Loss: 0.020584680140018463, Test Loss: 0.07306570559740067\n",
      "Epoch 6612, Train Loss: 0.020580682903528214, Test Loss: 0.07305897772312164\n",
      "Epoch 6613, Train Loss: 0.02057657390832901, Test Loss: 0.07305999845266342\n",
      "Epoch 6614, Train Loss: 0.020572654902935028, Test Loss: 0.07304756343364716\n",
      "Epoch 6615, Train Loss: 0.02056879550218582, Test Loss: 0.07303961366415024\n",
      "Epoch 6616, Train Loss: 0.020564598962664604, Test Loss: 0.07304872572422028\n",
      "Epoch 6617, Train Loss: 0.020560944452881813, Test Loss: 0.0730394721031189\n",
      "Epoch 6618, Train Loss: 0.020556876435875893, Test Loss: 0.07303817570209503\n",
      "Epoch 6619, Train Loss: 0.02055315114557743, Test Loss: 0.0730476900935173\n",
      "Epoch 6620, Train Loss: 0.02054920792579651, Test Loss: 0.07304048538208008\n",
      "Epoch 6621, Train Loss: 0.020544754341244698, Test Loss: 0.073041170835495\n",
      "Epoch 6622, Train Loss: 0.020541641861200333, Test Loss: 0.07302483916282654\n",
      "Epoch 6623, Train Loss: 0.020536918193101883, Test Loss: 0.07303828001022339\n",
      "Epoch 6624, Train Loss: 0.02053295634686947, Test Loss: 0.0730368047952652\n",
      "Epoch 6625, Train Loss: 0.02053043060004711, Test Loss: 0.07303465157747269\n",
      "Epoch 6626, Train Loss: 0.02052491344511509, Test Loss: 0.07303345203399658\n",
      "Epoch 6627, Train Loss: 0.020521104335784912, Test Loss: 0.07303545624017715\n",
      "Epoch 6628, Train Loss: 0.020517399534583092, Test Loss: 0.07304246723651886\n",
      "Epoch 6629, Train Loss: 0.020513346418738365, Test Loss: 0.07302907854318619\n",
      "Epoch 6630, Train Loss: 0.020510032773017883, Test Loss: 0.07301744073629379\n",
      "Epoch 6631, Train Loss: 0.02050534076988697, Test Loss: 0.07301769405603409\n",
      "Epoch 6632, Train Loss: 0.02050132118165493, Test Loss: 0.07301367819309235\n",
      "Epoch 6633, Train Loss: 0.020497139543294907, Test Loss: 0.07301605492830276\n",
      "Epoch 6634, Train Loss: 0.0204938817769289, Test Loss: 0.07300771027803421\n",
      "Epoch 6635, Train Loss: 0.020489539951086044, Test Loss: 0.07303111255168915\n",
      "Epoch 6636, Train Loss: 0.020485715940594673, Test Loss: 0.07301541417837143\n",
      "Epoch 6637, Train Loss: 0.020481662824749947, Test Loss: 0.07301272451877594\n",
      "Epoch 6638, Train Loss: 0.02047777734696865, Test Loss: 0.07300795614719391\n",
      "Epoch 6639, Train Loss: 0.020473722368478775, Test Loss: 0.07300706207752228\n",
      "Epoch 6640, Train Loss: 0.020469600334763527, Test Loss: 0.07300581783056259\n",
      "Epoch 6641, Train Loss: 0.020465580746531487, Test Loss: 0.07300905883312225\n",
      "Epoch 6642, Train Loss: 0.02046210877597332, Test Loss: 0.07300025224685669\n",
      "Epoch 6643, Train Loss: 0.02045794017612934, Test Loss: 0.07300049066543579\n",
      "Epoch 6644, Train Loss: 0.02045372873544693, Test Loss: 0.07299212366342545\n",
      "Epoch 6645, Train Loss: 0.020450009033083916, Test Loss: 0.07300025224685669\n",
      "Epoch 6646, Train Loss: 0.020446015521883965, Test Loss: 0.07298750430345535\n",
      "Epoch 6647, Train Loss: 0.02044222503900528, Test Loss: 0.07297787070274353\n",
      "Epoch 6648, Train Loss: 0.02043805457651615, Test Loss: 0.0729849711060524\n",
      "Epoch 6649, Train Loss: 0.02043398842215538, Test Loss: 0.07298535853624344\n",
      "Epoch 6650, Train Loss: 0.02043016068637371, Test Loss: 0.07298685610294342\n",
      "Epoch 6651, Train Loss: 0.02042672410607338, Test Loss: 0.07297981530427933\n",
      "Epoch 6652, Train Loss: 0.020422974601387978, Test Loss: 0.07296957820653915\n",
      "Epoch 6653, Train Loss: 0.020418163388967514, Test Loss: 0.07297912985086441\n",
      "Epoch 6654, Train Loss: 0.0204149279743433, Test Loss: 0.07297186553478241\n",
      "Epoch 6655, Train Loss: 0.0204106867313385, Test Loss: 0.07296813279390335\n",
      "Epoch 6656, Train Loss: 0.020406892523169518, Test Loss: 0.07296808809041977\n",
      "Epoch 6657, Train Loss: 0.020403144881129265, Test Loss: 0.07298090308904648\n",
      "Epoch 6658, Train Loss: 0.020398922264575958, Test Loss: 0.07296885550022125\n",
      "Epoch 6659, Train Loss: 0.020394669845700264, Test Loss: 0.07297050952911377\n",
      "Epoch 6660, Train Loss: 0.020391764119267464, Test Loss: 0.07295174151659012\n",
      "Epoch 6661, Train Loss: 0.020387955009937286, Test Loss: 0.07295793294906616\n",
      "Epoch 6662, Train Loss: 0.020383121445775032, Test Loss: 0.07295218110084534\n",
      "Epoch 6663, Train Loss: 0.020379044115543365, Test Loss: 0.07295852154493332\n",
      "Epoch 6664, Train Loss: 0.02037517912685871, Test Loss: 0.07296914607286453\n",
      "Epoch 6665, Train Loss: 0.02037145383656025, Test Loss: 0.07296399772167206\n",
      "Epoch 6666, Train Loss: 0.02036707103252411, Test Loss: 0.07296005636453629\n",
      "Epoch 6667, Train Loss: 0.02036326751112938, Test Loss: 0.07295002788305283\n",
      "Epoch 6668, Train Loss: 0.020359987393021584, Test Loss: 0.07293464988470078\n",
      "Epoch 6669, Train Loss: 0.020355721935629845, Test Loss: 0.07294083386659622\n",
      "Epoch 6670, Train Loss: 0.02035173587501049, Test Loss: 0.07293909043073654\n",
      "Epoch 6671, Train Loss: 0.02034776844084263, Test Loss: 0.07293324172496796\n",
      "Epoch 6672, Train Loss: 0.020343465730547905, Test Loss: 0.07294151186943054\n",
      "Epoch 6673, Train Loss: 0.02034013159573078, Test Loss: 0.07294220477342606\n",
      "Epoch 6674, Train Loss: 0.0203365720808506, Test Loss: 0.07293486595153809\n",
      "Epoch 6675, Train Loss: 0.020331991836428642, Test Loss: 0.07293565571308136\n",
      "Epoch 6676, Train Loss: 0.020328497514128685, Test Loss: 0.07293043285608292\n",
      "Epoch 6677, Train Loss: 0.020324181765317917, Test Loss: 0.07293092459440231\n",
      "Epoch 6678, Train Loss: 0.02032064087688923, Test Loss: 0.07292763143777847\n",
      "Epoch 6679, Train Loss: 0.02031647227704525, Test Loss: 0.0729205533862114\n",
      "Epoch 6680, Train Loss: 0.02031262218952179, Test Loss: 0.07293055951595306\n",
      "Epoch 6681, Train Loss: 0.02030830644071102, Test Loss: 0.07293467968702316\n",
      "Epoch 6682, Train Loss: 0.02030426263809204, Test Loss: 0.07292395085096359\n",
      "Epoch 6683, Train Loss: 0.020301155745983124, Test Loss: 0.07292471826076508\n",
      "Epoch 6684, Train Loss: 0.020296579226851463, Test Loss: 0.07292977720499039\n",
      "Epoch 6685, Train Loss: 0.02029247023165226, Test Loss: 0.07292138040065765\n",
      "Epoch 6686, Train Loss: 0.020288607105612755, Test Loss: 0.07291345298290253\n",
      "Epoch 6687, Train Loss: 0.02028481848537922, Test Loss: 0.07290530949831009\n",
      "Epoch 6688, Train Loss: 0.020281098783016205, Test Loss: 0.07290442287921906\n",
      "Epoch 6689, Train Loss: 0.020277298986911774, Test Loss: 0.07291143387556076\n",
      "Epoch 6690, Train Loss: 0.02027345821261406, Test Loss: 0.07290442287921906\n",
      "Epoch 6691, Train Loss: 0.02026887610554695, Test Loss: 0.0729169026017189\n",
      "Epoch 6692, Train Loss: 0.020265236496925354, Test Loss: 0.0729074701666832\n",
      "Epoch 6693, Train Loss: 0.020261328667402267, Test Loss: 0.0729130282998085\n",
      "Epoch 6694, Train Loss: 0.020258136093616486, Test Loss: 0.07291470468044281\n",
      "Epoch 6695, Train Loss: 0.02025405690073967, Test Loss: 0.07291664183139801\n",
      "Epoch 6696, Train Loss: 0.020249664783477783, Test Loss: 0.07290281355381012\n",
      "Epoch 6697, Train Loss: 0.020245999097824097, Test Loss: 0.0728951096534729\n",
      "Epoch 6698, Train Loss: 0.020242318511009216, Test Loss: 0.07290512323379517\n",
      "Epoch 6699, Train Loss: 0.02023766189813614, Test Loss: 0.0728956088423729\n",
      "Epoch 6700, Train Loss: 0.020234234631061554, Test Loss: 0.072880819439888\n",
      "Epoch 6701, Train Loss: 0.02022980898618698, Test Loss: 0.07288975268602371\n",
      "Epoch 6702, Train Loss: 0.020226789638400078, Test Loss: 0.07289767265319824\n",
      "Epoch 6703, Train Loss: 0.02022205851972103, Test Loss: 0.0728825107216835\n",
      "Epoch 6704, Train Loss: 0.02021840773522854, Test Loss: 0.0728917345404625\n",
      "Epoch 6705, Train Loss: 0.02021452784538269, Test Loss: 0.07288746535778046\n",
      "Epoch 6706, Train Loss: 0.0202103890478611, Test Loss: 0.07288485020399094\n",
      "Epoch 6707, Train Loss: 0.020206646993756294, Test Loss: 0.0728803426027298\n",
      "Epoch 6708, Train Loss: 0.02020256593823433, Test Loss: 0.07287491112947464\n",
      "Epoch 6709, Train Loss: 0.020198820158839226, Test Loss: 0.0728798359632492\n",
      "Epoch 6710, Train Loss: 0.0201947670429945, Test Loss: 0.07287567853927612\n",
      "Epoch 6711, Train Loss: 0.020191188901662827, Test Loss: 0.07287415117025375\n",
      "Epoch 6712, Train Loss: 0.020187199115753174, Test Loss: 0.07288415729999542\n",
      "Epoch 6713, Train Loss: 0.020183740183711052, Test Loss: 0.07286343723535538\n",
      "Epoch 6714, Train Loss: 0.020179282873868942, Test Loss: 0.07287443429231644\n",
      "Epoch 6715, Train Loss: 0.02017550729215145, Test Loss: 0.07286989688873291\n",
      "Epoch 6716, Train Loss: 0.02017151564359665, Test Loss: 0.0728837102651596\n",
      "Epoch 6717, Train Loss: 0.020168263465166092, Test Loss: 0.07286569476127625\n",
      "Epoch 6718, Train Loss: 0.02016385644674301, Test Loss: 0.07285487651824951\n",
      "Epoch 6719, Train Loss: 0.020159829407930374, Test Loss: 0.0728544071316719\n",
      "Epoch 6720, Train Loss: 0.020156651735305786, Test Loss: 0.07285042107105255\n",
      "Epoch 6721, Train Loss: 0.02015216462314129, Test Loss: 0.07284747809171677\n",
      "Epoch 6722, Train Loss: 0.02014823630452156, Test Loss: 0.07284072786569595\n",
      "Epoch 6723, Train Loss: 0.020144153386354446, Test Loss: 0.07284897565841675\n",
      "Epoch 6724, Train Loss: 0.020140239968895912, Test Loss: 0.07284630089998245\n",
      "Epoch 6725, Train Loss: 0.020136646926403046, Test Loss: 0.07283949106931686\n",
      "Epoch 6726, Train Loss: 0.020132435485720634, Test Loss: 0.07285360991954803\n",
      "Epoch 6727, Train Loss: 0.020128652453422546, Test Loss: 0.07285382598638535\n",
      "Epoch 6728, Train Loss: 0.020124711096286774, Test Loss: 0.07284513860940933\n",
      "Epoch 6729, Train Loss: 0.020120728760957718, Test Loss: 0.07284914702177048\n",
      "Epoch 6730, Train Loss: 0.02011890336871147, Test Loss: 0.07284083962440491\n",
      "Epoch 6731, Train Loss: 0.020113278180360794, Test Loss: 0.07283485680818558\n",
      "Epoch 6732, Train Loss: 0.020109152421355247, Test Loss: 0.07283782958984375\n",
      "Epoch 6733, Train Loss: 0.020105531439185143, Test Loss: 0.07283621281385422\n",
      "Epoch 6734, Train Loss: 0.02010158821940422, Test Loss: 0.07282507419586182\n",
      "Epoch 6735, Train Loss: 0.020098041743040085, Test Loss: 0.0728166252374649\n",
      "Epoch 6736, Train Loss: 0.020094009116292, Test Loss: 0.07283095270395279\n",
      "Epoch 6737, Train Loss: 0.020089896395802498, Test Loss: 0.07282011955976486\n",
      "Epoch 6738, Train Loss: 0.020086392760276794, Test Loss: 0.07282332330942154\n",
      "Epoch 6739, Train Loss: 0.020083043724298477, Test Loss: 0.07280603051185608\n",
      "Epoch 6740, Train Loss: 0.02007862739264965, Test Loss: 0.07280907034873962\n",
      "Epoch 6741, Train Loss: 0.02007473260164261, Test Loss: 0.07280943542718887\n",
      "Epoch 6742, Train Loss: 0.020070545375347137, Test Loss: 0.07282086461782455\n",
      "Epoch 6743, Train Loss: 0.020066894590854645, Test Loss: 0.0728277638554573\n",
      "Epoch 6744, Train Loss: 0.02006300538778305, Test Loss: 0.0728183165192604\n",
      "Epoch 6745, Train Loss: 0.020059196278452873, Test Loss: 0.07281886786222458\n",
      "Epoch 6746, Train Loss: 0.0200548954308033, Test Loss: 0.0728270411491394\n",
      "Epoch 6747, Train Loss: 0.0200511384755373, Test Loss: 0.07281799614429474\n",
      "Epoch 6748, Train Loss: 0.020047232508659363, Test Loss: 0.0728188008069992\n",
      "Epoch 6749, Train Loss: 0.02004324086010456, Test Loss: 0.07281755656003952\n",
      "Epoch 6750, Train Loss: 0.020039383322000504, Test Loss: 0.07281243056058884\n",
      "Epoch 6751, Train Loss: 0.020035572350025177, Test Loss: 0.07280157506465912\n",
      "Epoch 6752, Train Loss: 0.020031647756695747, Test Loss: 0.07279236614704132\n",
      "Epoch 6753, Train Loss: 0.02002788335084915, Test Loss: 0.07279190421104431\n",
      "Epoch 6754, Train Loss: 0.020024098455905914, Test Loss: 0.07279416918754578\n",
      "Epoch 6755, Train Loss: 0.020019948482513428, Test Loss: 0.07279075682163239\n",
      "Epoch 6756, Train Loss: 0.020016103982925415, Test Loss: 0.07280285656452179\n",
      "Epoch 6757, Train Loss: 0.020012596622109413, Test Loss: 0.0728033185005188\n",
      "Epoch 6758, Train Loss: 0.020008958876132965, Test Loss: 0.07278500497341156\n",
      "Epoch 6759, Train Loss: 0.02000490017235279, Test Loss: 0.07278119772672653\n",
      "Epoch 6760, Train Loss: 0.020002026110887527, Test Loss: 0.07277937233448029\n",
      "Epoch 6761, Train Loss: 0.019996648654341698, Test Loss: 0.07278129458427429\n",
      "Epoch 6762, Train Loss: 0.019992820918560028, Test Loss: 0.07277464121580124\n",
      "Epoch 6763, Train Loss: 0.019989091902971268, Test Loss: 0.07276979088783264\n",
      "Epoch 6764, Train Loss: 0.019984930753707886, Test Loss: 0.07277416437864304\n",
      "Epoch 6765, Train Loss: 0.019980989396572113, Test Loss: 0.07277620583772659\n",
      "Epoch 6766, Train Loss: 0.01997750625014305, Test Loss: 0.0727720707654953\n",
      "Epoch 6767, Train Loss: 0.019973568618297577, Test Loss: 0.0727815181016922\n",
      "Epoch 6768, Train Loss: 0.01996992528438568, Test Loss: 0.07278019189834595\n",
      "Epoch 6769, Train Loss: 0.019965671002864838, Test Loss: 0.07277003675699234\n",
      "Epoch 6770, Train Loss: 0.019962139427661896, Test Loss: 0.07278195768594742\n",
      "Epoch 6771, Train Loss: 0.019958022981882095, Test Loss: 0.07276593148708344\n",
      "Epoch 6772, Train Loss: 0.01995457522571087, Test Loss: 0.07276145368814468\n",
      "Epoch 6773, Train Loss: 0.019950291141867638, Test Loss: 0.07275951653718948\n",
      "Epoch 6774, Train Loss: 0.019946323707699776, Test Loss: 0.07275179773569107\n",
      "Epoch 6775, Train Loss: 0.01994249038398266, Test Loss: 0.0727459192276001\n",
      "Epoch 6776, Train Loss: 0.019939076155424118, Test Loss: 0.07273558527231216\n",
      "Epoch 6777, Train Loss: 0.019934730604290962, Test Loss: 0.07275248318910599\n",
      "Epoch 6778, Train Loss: 0.019930748268961906, Test Loss: 0.07275959849357605\n",
      "Epoch 6779, Train Loss: 0.01992766559123993, Test Loss: 0.0727425143122673\n",
      "Epoch 6780, Train Loss: 0.019923921674489975, Test Loss: 0.07274165004491806\n",
      "Epoch 6781, Train Loss: 0.019919278100132942, Test Loss: 0.07274733483791351\n",
      "Epoch 6782, Train Loss: 0.019915597513318062, Test Loss: 0.07275211811065674\n",
      "Epoch 6783, Train Loss: 0.01991160586476326, Test Loss: 0.07274892926216125\n",
      "Epoch 6784, Train Loss: 0.01990758813917637, Test Loss: 0.07274056226015091\n",
      "Epoch 6785, Train Loss: 0.01990393176674843, Test Loss: 0.07273957133293152\n",
      "Epoch 6786, Train Loss: 0.019900117069482803, Test Loss: 0.07273320108652115\n",
      "Epoch 6787, Train Loss: 0.019896604120731354, Test Loss: 0.07273133844137192\n",
      "Epoch 6788, Train Loss: 0.019892940297722816, Test Loss: 0.07273495942354202\n",
      "Epoch 6789, Train Loss: 0.019888775423169136, Test Loss: 0.07272262871265411\n",
      "Epoch 6790, Train Loss: 0.01988445781171322, Test Loss: 0.0727306455373764\n",
      "Epoch 6791, Train Loss: 0.019880972802639008, Test Loss: 0.07272235304117203\n",
      "Epoch 6792, Train Loss: 0.01987728662788868, Test Loss: 0.07271170616149902\n",
      "Epoch 6793, Train Loss: 0.019873008131980896, Test Loss: 0.07271240651607513\n",
      "Epoch 6794, Train Loss: 0.019869046285748482, Test Loss: 0.07272064685821533\n",
      "Epoch 6795, Train Loss: 0.019865304231643677, Test Loss: 0.07272125035524368\n",
      "Epoch 6796, Train Loss: 0.019861450418829918, Test Loss: 0.07272570580244064\n",
      "Epoch 6797, Train Loss: 0.019857315346598625, Test Loss: 0.07271945476531982\n",
      "Epoch 6798, Train Loss: 0.01985424943268299, Test Loss: 0.07272747904062271\n",
      "Epoch 6799, Train Loss: 0.01985008642077446, Test Loss: 0.07272686064243317\n",
      "Epoch 6800, Train Loss: 0.019845835864543915, Test Loss: 0.07271907478570938\n",
      "Epoch 6801, Train Loss: 0.019842060282826424, Test Loss: 0.0727061852812767\n",
      "Epoch 6802, Train Loss: 0.01983838714659214, Test Loss: 0.07270698994398117\n",
      "Epoch 6803, Train Loss: 0.01983426697552204, Test Loss: 0.07270550727844238\n",
      "Epoch 6804, Train Loss: 0.01983058452606201, Test Loss: 0.0727049708366394\n",
      "Epoch 6805, Train Loss: 0.019826989620923996, Test Loss: 0.07269585132598877\n",
      "Epoch 6806, Train Loss: 0.019822988659143448, Test Loss: 0.072693832218647\n",
      "Epoch 6807, Train Loss: 0.019819019362330437, Test Loss: 0.07269710302352905\n",
      "Epoch 6808, Train Loss: 0.019815262407064438, Test Loss: 0.07271381467580795\n",
      "Epoch 6809, Train Loss: 0.019811203703284264, Test Loss: 0.07269424200057983\n",
      "Epoch 6810, Train Loss: 0.019807735458016396, Test Loss: 0.07269029319286346\n",
      "Epoch 6811, Train Loss: 0.019804706797003746, Test Loss: 0.07268663495779037\n",
      "Epoch 6812, Train Loss: 0.01980028674006462, Test Loss: 0.07267960160970688\n",
      "Epoch 6813, Train Loss: 0.019796224310994148, Test Loss: 0.0726880356669426\n",
      "Epoch 6814, Train Loss: 0.019792962819337845, Test Loss: 0.0726918876171112\n",
      "Epoch 6815, Train Loss: 0.019788414239883423, Test Loss: 0.07269643247127533\n",
      "Epoch 6816, Train Loss: 0.019784612581133842, Test Loss: 0.07269284129142761\n",
      "Epoch 6817, Train Loss: 0.019780809059739113, Test Loss: 0.07268466800451279\n",
      "Epoch 6818, Train Loss: 0.01977747492492199, Test Loss: 0.0726718157529831\n",
      "Epoch 6819, Train Loss: 0.01977289654314518, Test Loss: 0.0726756602525711\n",
      "Epoch 6820, Train Loss: 0.019770871847867966, Test Loss: 0.07269168645143509\n",
      "Epoch 6821, Train Loss: 0.019765367731451988, Test Loss: 0.07267212122678757\n",
      "Epoch 6822, Train Loss: 0.019761795178055763, Test Loss: 0.07266508042812347\n",
      "Epoch 6823, Train Loss: 0.019758326932787895, Test Loss: 0.0726579874753952\n",
      "Epoch 6824, Train Loss: 0.01975441537797451, Test Loss: 0.07265802472829819\n",
      "Epoch 6825, Train Loss: 0.01975014992058277, Test Loss: 0.07265820354223251\n",
      "Epoch 6826, Train Loss: 0.01974637806415558, Test Loss: 0.0726652443408966\n",
      "Epoch 6827, Train Loss: 0.019742505624890327, Test Loss: 0.07266642898321152\n",
      "Epoch 6828, Train Loss: 0.01973893865942955, Test Loss: 0.07266850769519806\n",
      "Epoch 6829, Train Loss: 0.01973513327538967, Test Loss: 0.0726696327328682\n",
      "Epoch 6830, Train Loss: 0.01973084919154644, Test Loss: 0.07266061007976532\n",
      "Epoch 6831, Train Loss: 0.01972791738808155, Test Loss: 0.07266228646039963\n",
      "Epoch 6832, Train Loss: 0.01972426474094391, Test Loss: 0.0726616233587265\n",
      "Epoch 6833, Train Loss: 0.019719542935490608, Test Loss: 0.07265878468751907\n",
      "Epoch 6834, Train Loss: 0.019716188311576843, Test Loss: 0.07267016917467117\n",
      "Epoch 6835, Train Loss: 0.01971219852566719, Test Loss: 0.07266097515821457\n",
      "Epoch 6836, Train Loss: 0.019708141684532166, Test Loss: 0.07264754176139832\n",
      "Epoch 6837, Train Loss: 0.019705140963196754, Test Loss: 0.07265239208936691\n",
      "Epoch 6838, Train Loss: 0.01970025710761547, Test Loss: 0.07264871895313263\n",
      "Epoch 6839, Train Loss: 0.019696587696671486, Test Loss: 0.07264550775289536\n",
      "Epoch 6840, Train Loss: 0.019693048670887947, Test Loss: 0.07264342159032822\n",
      "Epoch 6841, Train Loss: 0.019688988104462624, Test Loss: 0.07264480739831924\n",
      "Epoch 6842, Train Loss: 0.01968495361506939, Test Loss: 0.07264240831136703\n",
      "Epoch 6843, Train Loss: 0.019681133329868317, Test Loss: 0.07264544814825058\n",
      "Epoch 6844, Train Loss: 0.019677620381116867, Test Loss: 0.07263972610235214\n",
      "Epoch 6845, Train Loss: 0.019673865288496017, Test Loss: 0.07263599336147308\n",
      "Epoch 6846, Train Loss: 0.01967001147568226, Test Loss: 0.07262387871742249\n",
      "Epoch 6847, Train Loss: 0.019666597247123718, Test Loss: 0.07264535874128342\n",
      "Epoch 6848, Train Loss: 0.019662125036120415, Test Loss: 0.07263772934675217\n",
      "Epoch 6849, Train Loss: 0.01965886726975441, Test Loss: 0.0726439505815506\n",
      "Epoch 6850, Train Loss: 0.0196546483784914, Test Loss: 0.0726289451122284\n",
      "Epoch 6851, Train Loss: 0.019650928676128387, Test Loss: 0.07262919843196869\n",
      "Epoch 6852, Train Loss: 0.019647229462862015, Test Loss: 0.07261662930250168\n",
      "Epoch 6853, Train Loss: 0.019643200561404228, Test Loss: 0.07262483239173889\n",
      "Epoch 6854, Train Loss: 0.019639240577816963, Test Loss: 0.07261922210454941\n",
      "Epoch 6855, Train Loss: 0.019635288044810295, Test Loss: 0.07261961698532104\n",
      "Epoch 6856, Train Loss: 0.019631555303931236, Test Loss: 0.07261957228183746\n",
      "Epoch 6857, Train Loss: 0.019627973437309265, Test Loss: 0.07260750234127045\n",
      "Epoch 6858, Train Loss: 0.019623922184109688, Test Loss: 0.07261502742767334\n",
      "Epoch 6859, Train Loss: 0.019620152190327644, Test Loss: 0.07261427491903305\n",
      "Epoch 6860, Train Loss: 0.019616441801190376, Test Loss: 0.07261960208415985\n",
      "Epoch 6861, Train Loss: 0.01961314119398594, Test Loss: 0.07260879129171371\n",
      "Epoch 6862, Train Loss: 0.019609447568655014, Test Loss: 0.07259912043809891\n",
      "Epoch 6863, Train Loss: 0.01960582472383976, Test Loss: 0.07260223478078842\n",
      "Epoch 6864, Train Loss: 0.019601257517933846, Test Loss: 0.07259968668222427\n",
      "Epoch 6865, Train Loss: 0.019598057493567467, Test Loss: 0.07260683178901672\n",
      "Epoch 6866, Train Loss: 0.019593602046370506, Test Loss: 0.07259783148765564\n",
      "Epoch 6867, Train Loss: 0.01959020271897316, Test Loss: 0.0726018100976944\n",
      "Epoch 6868, Train Loss: 0.019586190581321716, Test Loss: 0.07259844988584518\n",
      "Epoch 6869, Train Loss: 0.01958227902650833, Test Loss: 0.07259304821491241\n",
      "Epoch 6870, Train Loss: 0.019579149782657623, Test Loss: 0.07259199768304825\n",
      "Epoch 6871, Train Loss: 0.01957465149462223, Test Loss: 0.07259517163038254\n",
      "Epoch 6872, Train Loss: 0.01957053132355213, Test Loss: 0.07259491831064224\n",
      "Epoch 6873, Train Loss: 0.019566891714930534, Test Loss: 0.07260297238826752\n",
      "Epoch 6874, Train Loss: 0.019562875851988792, Test Loss: 0.07259970158338547\n",
      "Epoch 6875, Train Loss: 0.019559625536203384, Test Loss: 0.0725894495844841\n",
      "Epoch 6876, Train Loss: 0.019555581733584404, Test Loss: 0.07258469611406326\n",
      "Epoch 6877, Train Loss: 0.019551418721675873, Test Loss: 0.07258781790733337\n",
      "Epoch 6878, Train Loss: 0.019547700881958008, Test Loss: 0.07259093225002289\n",
      "Epoch 6879, Train Loss: 0.019543861970305443, Test Loss: 0.07258951663970947\n",
      "Epoch 6880, Train Loss: 0.01954040676355362, Test Loss: 0.07257343083620071\n",
      "Epoch 6881, Train Loss: 0.01953626424074173, Test Loss: 0.07257683575153351\n",
      "Epoch 6882, Train Loss: 0.01953231915831566, Test Loss: 0.07258389890193939\n",
      "Epoch 6883, Train Loss: 0.0195288248360157, Test Loss: 0.07257437705993652\n",
      "Epoch 6884, Train Loss: 0.01952480338513851, Test Loss: 0.0725766122341156\n",
      "Epoch 6885, Train Loss: 0.01952122151851654, Test Loss: 0.07256891578435898\n",
      "Epoch 6886, Train Loss: 0.019517410546541214, Test Loss: 0.07257755100727081\n",
      "Epoch 6887, Train Loss: 0.019513510167598724, Test Loss: 0.07257907092571259\n",
      "Epoch 6888, Train Loss: 0.0195100549608469, Test Loss: 0.07257373631000519\n",
      "Epoch 6889, Train Loss: 0.01950584538280964, Test Loss: 0.07257474213838577\n",
      "Epoch 6890, Train Loss: 0.019502440467476845, Test Loss: 0.07255837321281433\n",
      "Epoch 6891, Train Loss: 0.01949865184724331, Test Loss: 0.07255574315786362\n",
      "Epoch 6892, Train Loss: 0.01949465088546276, Test Loss: 0.07257255911827087\n",
      "Epoch 6893, Train Loss: 0.01949063502252102, Test Loss: 0.07256209850311279\n",
      "Epoch 6894, Train Loss: 0.019487157464027405, Test Loss: 0.0725601390004158\n",
      "Epoch 6895, Train Loss: 0.019483106210827827, Test Loss: 0.0725538432598114\n",
      "Epoch 6896, Train Loss: 0.019479241222143173, Test Loss: 0.07255850732326508\n",
      "Epoch 6897, Train Loss: 0.019476110115647316, Test Loss: 0.07255463302135468\n",
      "Epoch 6898, Train Loss: 0.01947181485593319, Test Loss: 0.07254695147275925\n",
      "Epoch 6899, Train Loss: 0.01946806162595749, Test Loss: 0.07253561913967133\n",
      "Epoch 6900, Train Loss: 0.01946478709578514, Test Loss: 0.07254122197628021\n",
      "Epoch 6901, Train Loss: 0.019460350275039673, Test Loss: 0.0725439116358757\n",
      "Epoch 6902, Train Loss: 0.01945636235177517, Test Loss: 0.07254306226968765\n",
      "Epoch 6903, Train Loss: 0.019452905282378197, Test Loss: 0.07254361361265182\n",
      "Epoch 6904, Train Loss: 0.019448919221758842, Test Loss: 0.07254493981599808\n",
      "Epoch 6905, Train Loss: 0.01944485306739807, Test Loss: 0.07254921644926071\n",
      "Epoch 6906, Train Loss: 0.01944141276180744, Test Loss: 0.07253310084342957\n",
      "Epoch 6907, Train Loss: 0.019437618553638458, Test Loss: 0.07253531366586685\n",
      "Epoch 6908, Train Loss: 0.01943376660346985, Test Loss: 0.07252796739339828\n",
      "Epoch 6909, Train Loss: 0.019431082531809807, Test Loss: 0.07251767069101334\n",
      "Epoch 6910, Train Loss: 0.019425885751843452, Test Loss: 0.07252520322799683\n",
      "Epoch 6911, Train Loss: 0.019422920420765877, Test Loss: 0.0725281834602356\n",
      "Epoch 6912, Train Loss: 0.019418561831116676, Test Loss: 0.07252251356840134\n",
      "Epoch 6913, Train Loss: 0.019414760172367096, Test Loss: 0.07251962274312973\n",
      "Epoch 6914, Train Loss: 0.019410917535424232, Test Loss: 0.07251641154289246\n",
      "Epoch 6915, Train Loss: 0.019407503306865692, Test Loss: 0.07251608371734619\n",
      "Epoch 6916, Train Loss: 0.01940385065972805, Test Loss: 0.07251996546983719\n",
      "Epoch 6917, Train Loss: 0.019400212913751602, Test Loss: 0.07250584661960602\n",
      "Epoch 6918, Train Loss: 0.019397087395191193, Test Loss: 0.0725061371922493\n",
      "Epoch 6919, Train Loss: 0.01939268223941326, Test Loss: 0.07251157611608505\n",
      "Epoch 6920, Train Loss: 0.019388647750020027, Test Loss: 0.0725029781460762\n",
      "Epoch 6921, Train Loss: 0.019384492188692093, Test Loss: 0.0725000724196434\n",
      "Epoch 6922, Train Loss: 0.01938038133084774, Test Loss: 0.072507344186306\n",
      "Epoch 6923, Train Loss: 0.019376862794160843, Test Loss: 0.07250461727380753\n",
      "Epoch 6924, Train Loss: 0.019373200833797455, Test Loss: 0.07250407338142395\n",
      "Epoch 6925, Train Loss: 0.019369754940271378, Test Loss: 0.07249516993761063\n",
      "Epoch 6926, Train Loss: 0.019365575164556503, Test Loss: 0.0725061222910881\n",
      "Epoch 6927, Train Loss: 0.019362274557352066, Test Loss: 0.07248842716217041\n",
      "Epoch 6928, Train Loss: 0.019358014687895775, Test Loss: 0.07250344008207321\n",
      "Epoch 6929, Train Loss: 0.01935485377907753, Test Loss: 0.07248463481664658\n",
      "Epoch 6930, Train Loss: 0.019350335001945496, Test Loss: 0.07249085605144501\n",
      "Epoch 6931, Train Loss: 0.019346406683325768, Test Loss: 0.07249130308628082\n",
      "Epoch 6932, Train Loss: 0.019342700019478798, Test Loss: 0.07249893248081207\n",
      "Epoch 6933, Train Loss: 0.01933923363685608, Test Loss: 0.07250135391950607\n",
      "Epoch 6934, Train Loss: 0.019336717203259468, Test Loss: 0.07249629497528076\n",
      "Epoch 6935, Train Loss: 0.019331907853484154, Test Loss: 0.0724857971072197\n",
      "Epoch 6936, Train Loss: 0.019327450543642044, Test Loss: 0.07248406112194061\n",
      "Epoch 6937, Train Loss: 0.019323967397212982, Test Loss: 0.07247897237539291\n",
      "Epoch 6938, Train Loss: 0.019320152699947357, Test Loss: 0.07248522341251373\n",
      "Epoch 6939, Train Loss: 0.019316624850034714, Test Loss: 0.07248073071241379\n",
      "Epoch 6940, Train Loss: 0.01931276172399521, Test Loss: 0.07247496396303177\n",
      "Epoch 6941, Train Loss: 0.019308675080537796, Test Loss: 0.07246887683868408\n",
      "Epoch 6942, Train Loss: 0.019305337220430374, Test Loss: 0.07246561348438263\n",
      "Epoch 6943, Train Loss: 0.01930168829858303, Test Loss: 0.07246046513319016\n",
      "Epoch 6944, Train Loss: 0.019297435879707336, Test Loss: 0.07246316969394684\n",
      "Epoch 6945, Train Loss: 0.019293511286377907, Test Loss: 0.07246653735637665\n",
      "Epoch 6946, Train Loss: 0.01929069310426712, Test Loss: 0.07246672362089157\n",
      "Epoch 6947, Train Loss: 0.019286256283521652, Test Loss: 0.07247138768434525\n",
      "Epoch 6948, Train Loss: 0.019282445311546326, Test Loss: 0.07246693968772888\n",
      "Epoch 6949, Train Loss: 0.019279690459370613, Test Loss: 0.07244332134723663\n",
      "Epoch 6950, Train Loss: 0.019274843856692314, Test Loss: 0.07245080918073654\n",
      "Epoch 6951, Train Loss: 0.019271427765488625, Test Loss: 0.0724501684308052\n",
      "Epoch 6952, Train Loss: 0.019268788397312164, Test Loss: 0.07243853062391281\n",
      "Epoch 6953, Train Loss: 0.019264237955212593, Test Loss: 0.0724339634180069\n",
      "Epoch 6954, Train Loss: 0.019260931760072708, Test Loss: 0.07243970036506653\n",
      "Epoch 6955, Train Loss: 0.01925632916390896, Test Loss: 0.0724431499838829\n",
      "Epoch 6956, Train Loss: 0.01925276219844818, Test Loss: 0.07244188338518143\n",
      "Epoch 6957, Train Loss: 0.019248714670538902, Test Loss: 0.07245069742202759\n",
      "Epoch 6958, Train Loss: 0.019244974479079247, Test Loss: 0.07244624942541122\n",
      "Epoch 6959, Train Loss: 0.019241614267230034, Test Loss: 0.07244009524583817\n",
      "Epoch 6960, Train Loss: 0.019237404689192772, Test Loss: 0.07244034856557846\n",
      "Epoch 6961, Train Loss: 0.019233666360378265, Test Loss: 0.07244595140218735\n",
      "Epoch 6962, Train Loss: 0.019230186939239502, Test Loss: 0.07244236767292023\n",
      "Epoch 6963, Train Loss: 0.019226716831326485, Test Loss: 0.0724288746714592\n",
      "Epoch 6964, Train Loss: 0.019222592934966087, Test Loss: 0.07243765890598297\n",
      "Epoch 6965, Train Loss: 0.01921854354441166, Test Loss: 0.07244183868169785\n",
      "Epoch 6966, Train Loss: 0.019215218722820282, Test Loss: 0.07243712246417999\n",
      "Epoch 6967, Train Loss: 0.01921110600233078, Test Loss: 0.0724269449710846\n",
      "Epoch 6968, Train Loss: 0.019207743927836418, Test Loss: 0.07242804765701294\n",
      "Epoch 6969, Train Loss: 0.01920345053076744, Test Loss: 0.07242706418037415\n",
      "Epoch 6970, Train Loss: 0.019199814647436142, Test Loss: 0.0724271908402443\n",
      "Epoch 6971, Train Loss: 0.0191961620002985, Test Loss: 0.07243156433105469\n",
      "Epoch 6972, Train Loss: 0.019192472100257874, Test Loss: 0.07242628186941147\n",
      "Epoch 6973, Train Loss: 0.01918875053524971, Test Loss: 0.07242485880851746\n",
      "Epoch 6974, Train Loss: 0.01918479986488819, Test Loss: 0.07240898907184601\n",
      "Epoch 6975, Train Loss: 0.019181130453944206, Test Loss: 0.0724141001701355\n",
      "Epoch 6976, Train Loss: 0.019177770242094994, Test Loss: 0.07241854071617126\n",
      "Epoch 6977, Train Loss: 0.01917407289147377, Test Loss: 0.07241126149892807\n",
      "Epoch 6978, Train Loss: 0.01916959322988987, Test Loss: 0.07240697741508484\n",
      "Epoch 6979, Train Loss: 0.01916605606675148, Test Loss: 0.0724034309387207\n",
      "Epoch 6980, Train Loss: 0.01916239969432354, Test Loss: 0.07239934056997299\n",
      "Epoch 6981, Train Loss: 0.019158756360411644, Test Loss: 0.07239165902137756\n",
      "Epoch 6982, Train Loss: 0.019155578687787056, Test Loss: 0.0723925530910492\n",
      "Epoch 6983, Train Loss: 0.019150832667946815, Test Loss: 0.07239993661642075\n",
      "Epoch 6984, Train Loss: 0.01914716511964798, Test Loss: 0.07240262627601624\n",
      "Epoch 6985, Train Loss: 0.01914350688457489, Test Loss: 0.07238788902759552\n",
      "Epoch 6986, Train Loss: 0.019139880314469337, Test Loss: 0.07238705456256866\n",
      "Epoch 6987, Train Loss: 0.019136451184749603, Test Loss: 0.07237889617681503\n",
      "Epoch 6988, Train Loss: 0.01913192681968212, Test Loss: 0.07238589972257614\n",
      "Epoch 6989, Train Loss: 0.01912839710712433, Test Loss: 0.07239580899477005\n",
      "Epoch 6990, Train Loss: 0.019124314188957214, Test Loss: 0.07239516079425812\n",
      "Epoch 6991, Train Loss: 0.019120842218399048, Test Loss: 0.07238077372312546\n",
      "Epoch 6992, Train Loss: 0.019117578864097595, Test Loss: 0.07238199561834335\n",
      "Epoch 6993, Train Loss: 0.01911478489637375, Test Loss: 0.0723884180188179\n",
      "Epoch 6994, Train Loss: 0.019109249114990234, Test Loss: 0.07238535583019257\n",
      "Epoch 6995, Train Loss: 0.019105881452560425, Test Loss: 0.07238344103097916\n",
      "Epoch 6996, Train Loss: 0.01910247839987278, Test Loss: 0.0723772644996643\n",
      "Epoch 6997, Train Loss: 0.019098199903964996, Test Loss: 0.07236818224191666\n",
      "Epoch 6998, Train Loss: 0.019094295799732208, Test Loss: 0.07237288355827332\n",
      "Epoch 6999, Train Loss: 0.019090648740530014, Test Loss: 0.07236602902412415\n",
      "Epoch 7000, Train Loss: 0.019087394699454308, Test Loss: 0.07236304879188538\n",
      "Epoch 7001, Train Loss: 0.019083211198449135, Test Loss: 0.07235915958881378\n",
      "Epoch 7002, Train Loss: 0.01907986029982567, Test Loss: 0.07236523926258087\n",
      "Epoch 7003, Train Loss: 0.019075995311141014, Test Loss: 0.07235646992921829\n",
      "Epoch 7004, Train Loss: 0.019072014838457108, Test Loss: 0.07235723733901978\n",
      "Epoch 7005, Train Loss: 0.019067993387579918, Test Loss: 0.07236319780349731\n",
      "Epoch 7006, Train Loss: 0.01906445622444153, Test Loss: 0.07235471159219742\n",
      "Epoch 7007, Train Loss: 0.019060712307691574, Test Loss: 0.0723583772778511\n",
      "Epoch 7008, Train Loss: 0.019057493656873703, Test Loss: 0.07235137373209\n",
      "Epoch 7009, Train Loss: 0.019053990021348, Test Loss: 0.07235288619995117\n",
      "Epoch 7010, Train Loss: 0.019049637019634247, Test Loss: 0.07235586643218994\n",
      "Epoch 7011, Train Loss: 0.019046131521463394, Test Loss: 0.07234379649162292\n",
      "Epoch 7012, Train Loss: 0.019042203202843666, Test Loss: 0.07234267145395279\n",
      "Epoch 7013, Train Loss: 0.019038697704672813, Test Loss: 0.07234689593315125\n",
      "Epoch 7014, Train Loss: 0.01903442107141018, Test Loss: 0.07235027849674225\n",
      "Epoch 7015, Train Loss: 0.019031334668397903, Test Loss: 0.0723419040441513\n",
      "Epoch 7016, Train Loss: 0.01902681775391102, Test Loss: 0.07233401387929916\n",
      "Epoch 7017, Train Loss: 0.019023286178708076, Test Loss: 0.07233596593141556\n",
      "Epoch 7018, Train Loss: 0.01901964284479618, Test Loss: 0.07234513014554977\n",
      "Epoch 7019, Train Loss: 0.019015973433852196, Test Loss: 0.07233121991157532\n",
      "Epoch 7020, Train Loss: 0.019012512639164925, Test Loss: 0.0723160058259964\n",
      "Epoch 7021, Train Loss: 0.019009068608283997, Test Loss: 0.07232988625764847\n",
      "Epoch 7022, Train Loss: 0.019004717469215393, Test Loss: 0.07232598960399628\n",
      "Epoch 7023, Train Loss: 0.019001156091690063, Test Loss: 0.07232469320297241\n",
      "Epoch 7024, Train Loss: 0.018997790291905403, Test Loss: 0.07232460379600525\n",
      "Epoch 7025, Train Loss: 0.018993955105543137, Test Loss: 0.07231434434652328\n",
      "Epoch 7026, Train Loss: 0.01898997649550438, Test Loss: 0.0723189115524292\n",
      "Epoch 7027, Train Loss: 0.018986353650689125, Test Loss: 0.07231307774782181\n",
      "Epoch 7028, Train Loss: 0.0189825389534235, Test Loss: 0.0723206177353859\n",
      "Epoch 7029, Train Loss: 0.018978528678417206, Test Loss: 0.07232651114463806\n",
      "Epoch 7030, Train Loss: 0.018974952399730682, Test Loss: 0.07231905311346054\n",
      "Epoch 7031, Train Loss: 0.01897108554840088, Test Loss: 0.07231973856687546\n",
      "Epoch 7032, Train Loss: 0.018967652693390846, Test Loss: 0.07230954617261887\n",
      "Epoch 7033, Train Loss: 0.01896371692419052, Test Loss: 0.07229950278997421\n",
      "Epoch 7034, Train Loss: 0.018959805369377136, Test Loss: 0.0723118931055069\n",
      "Epoch 7035, Train Loss: 0.018956180661916733, Test Loss: 0.07230707257986069\n",
      "Epoch 7036, Train Loss: 0.018952514976263046, Test Loss: 0.07230428606271744\n",
      "Epoch 7037, Train Loss: 0.018948577344417572, Test Loss: 0.07230600714683533\n",
      "Epoch 7038, Train Loss: 0.018944818526506424, Test Loss: 0.07231245189905167\n",
      "Epoch 7039, Train Loss: 0.018941286951303482, Test Loss: 0.07230626046657562\n",
      "Epoch 7040, Train Loss: 0.018937742337584496, Test Loss: 0.07229576259851456\n",
      "Epoch 7041, Train Loss: 0.018933502957224846, Test Loss: 0.07230302691459656\n",
      "Epoch 7042, Train Loss: 0.018930090591311455, Test Loss: 0.07230473309755325\n",
      "Epoch 7043, Train Loss: 0.018926138058304787, Test Loss: 0.07230197638273239\n",
      "Epoch 7044, Train Loss: 0.018922341987490654, Test Loss: 0.07230652123689651\n",
      "Epoch 7045, Train Loss: 0.01891861855983734, Test Loss: 0.07229608297348022\n",
      "Epoch 7046, Train Loss: 0.01891619898378849, Test Loss: 0.07230371236801147\n",
      "Epoch 7047, Train Loss: 0.0189113337546587, Test Loss: 0.07229767739772797\n",
      "Epoch 7048, Train Loss: 0.018907776102423668, Test Loss: 0.07229401171207428\n",
      "Epoch 7049, Train Loss: 0.018904007971286774, Test Loss: 0.07229100167751312\n",
      "Epoch 7050, Train Loss: 0.01890048012137413, Test Loss: 0.0722932294011116\n",
      "Epoch 7051, Train Loss: 0.01889652945101261, Test Loss: 0.07227616012096405\n",
      "Epoch 7052, Train Loss: 0.01889309100806713, Test Loss: 0.07228459417819977\n",
      "Epoch 7053, Train Loss: 0.018888944759964943, Test Loss: 0.07228528708219528\n",
      "Epoch 7054, Train Loss: 0.018885301426053047, Test Loss: 0.07228060066699982\n",
      "Epoch 7055, Train Loss: 0.01888180337846279, Test Loss: 0.07226908206939697\n",
      "Epoch 7056, Train Loss: 0.018877752125263214, Test Loss: 0.07227808982133865\n",
      "Epoch 7057, Train Loss: 0.01887453906238079, Test Loss: 0.07226916402578354\n",
      "Epoch 7058, Train Loss: 0.018870273604989052, Test Loss: 0.07227667421102524\n",
      "Epoch 7059, Train Loss: 0.018866561353206635, Test Loss: 0.07226508110761642\n",
      "Epoch 7060, Train Loss: 0.018862809985876083, Test Loss: 0.07226327806711197\n",
      "Epoch 7061, Train Loss: 0.01885940693318844, Test Loss: 0.07227113842964172\n",
      "Epoch 7062, Train Loss: 0.018855949863791466, Test Loss: 0.07226009666919708\n",
      "Epoch 7063, Train Loss: 0.01885167695581913, Test Loss: 0.0722612515091896\n",
      "Epoch 7064, Train Loss: 0.01884847693145275, Test Loss: 0.0722687691450119\n",
      "Epoch 7065, Train Loss: 0.018844638019800186, Test Loss: 0.07226813584566116\n",
      "Epoch 7066, Train Loss: 0.018840454518795013, Test Loss: 0.07226339727640152\n",
      "Epoch 7067, Train Loss: 0.01883682608604431, Test Loss: 0.0722617506980896\n",
      "Epoch 7068, Train Loss: 0.018833668902516365, Test Loss: 0.0722653716802597\n",
      "Epoch 7069, Train Loss: 0.01882944256067276, Test Loss: 0.07225489616394043\n",
      "Epoch 7070, Train Loss: 0.01882568933069706, Test Loss: 0.07224739342927933\n",
      "Epoch 7071, Train Loss: 0.018822235986590385, Test Loss: 0.07224862277507782\n",
      "Epoch 7072, Train Loss: 0.01881827786564827, Test Loss: 0.07225102931261063\n",
      "Epoch 7073, Train Loss: 0.01881483942270279, Test Loss: 0.07225465029478073\n",
      "Epoch 7074, Train Loss: 0.018811017274856567, Test Loss: 0.07224494963884354\n",
      "Epoch 7075, Train Loss: 0.01880810596048832, Test Loss: 0.0722445398569107\n",
      "Epoch 7076, Train Loss: 0.018803417682647705, Test Loss: 0.07224128395318985\n",
      "Epoch 7077, Train Loss: 0.018799683079123497, Test Loss: 0.07224737107753754\n",
      "Epoch 7078, Train Loss: 0.018796253949403763, Test Loss: 0.07225096970796585\n",
      "Epoch 7079, Train Loss: 0.018792830407619476, Test Loss: 0.07225397229194641\n",
      "Epoch 7080, Train Loss: 0.018789958208799362, Test Loss: 0.07225140184164047\n",
      "Epoch 7081, Train Loss: 0.01878536492586136, Test Loss: 0.07224740087985992\n",
      "Epoch 7082, Train Loss: 0.018781548365950584, Test Loss: 0.072234608232975\n",
      "Epoch 7083, Train Loss: 0.018777955323457718, Test Loss: 0.07222753018140793\n",
      "Epoch 7084, Train Loss: 0.0187752116471529, Test Loss: 0.0722140222787857\n",
      "Epoch 7085, Train Loss: 0.018771382048726082, Test Loss: 0.07221280783414841\n",
      "Epoch 7086, Train Loss: 0.01876670867204666, Test Loss: 0.07221660763025284\n",
      "Epoch 7087, Train Loss: 0.018762877210974693, Test Loss: 0.07222175598144531\n",
      "Epoch 7088, Train Loss: 0.018759192898869514, Test Loss: 0.07221435010433197\n",
      "Epoch 7089, Train Loss: 0.018756017088890076, Test Loss: 0.07221157103776932\n",
      "Epoch 7090, Train Loss: 0.018751978874206543, Test Loss: 0.0722099095582962\n",
      "Epoch 7091, Train Loss: 0.018748050555586815, Test Loss: 0.07222146540880203\n",
      "Epoch 7092, Train Loss: 0.018744848668575287, Test Loss: 0.07222627848386765\n",
      "Epoch 7093, Train Loss: 0.01874115876853466, Test Loss: 0.07220711559057236\n",
      "Epoch 7094, Train Loss: 0.018737049773335457, Test Loss: 0.07220908254384995\n",
      "Epoch 7095, Train Loss: 0.018733983859419823, Test Loss: 0.07219990342855453\n",
      "Epoch 7096, Train Loss: 0.018730126321315765, Test Loss: 0.07219487428665161\n",
      "Epoch 7097, Train Loss: 0.018726063892245293, Test Loss: 0.07221616059541702\n",
      "Epoch 7098, Train Loss: 0.01872297003865242, Test Loss: 0.07219485193490982\n",
      "Epoch 7099, Train Loss: 0.01871890015900135, Test Loss: 0.0721956267952919\n",
      "Epoch 7100, Train Loss: 0.018714871257543564, Test Loss: 0.0722055658698082\n",
      "Epoch 7101, Train Loss: 0.018711762502789497, Test Loss: 0.07220257818698883\n",
      "Epoch 7102, Train Loss: 0.01870870031416416, Test Loss: 0.07219276577234268\n",
      "Epoch 7103, Train Loss: 0.018704507499933243, Test Loss: 0.07218471169471741\n",
      "Epoch 7104, Train Loss: 0.018700379878282547, Test Loss: 0.07218820601701736\n",
      "Epoch 7105, Train Loss: 0.01869686134159565, Test Loss: 0.07218413054943085\n",
      "Epoch 7106, Train Loss: 0.0186926256865263, Test Loss: 0.0721922442317009\n",
      "Epoch 7107, Train Loss: 0.018689461052417755, Test Loss: 0.07219122350215912\n",
      "Epoch 7108, Train Loss: 0.018686460331082344, Test Loss: 0.07218798249959946\n",
      "Epoch 7109, Train Loss: 0.018681854009628296, Test Loss: 0.07218324393033981\n",
      "Epoch 7110, Train Loss: 0.01867849752306938, Test Loss: 0.07217320799827576\n",
      "Epoch 7111, Train Loss: 0.018674449995160103, Test Loss: 0.07219189405441284\n",
      "Epoch 7112, Train Loss: 0.01867079548537731, Test Loss: 0.07218104600906372\n",
      "Epoch 7113, Train Loss: 0.01866704411804676, Test Loss: 0.07218367606401443\n",
      "Epoch 7114, Train Loss: 0.018664464354515076, Test Loss: 0.07217221707105637\n",
      "Epoch 7115, Train Loss: 0.01866045966744423, Test Loss: 0.0721772238612175\n",
      "Epoch 7116, Train Loss: 0.018656566739082336, Test Loss: 0.07216937839984894\n",
      "Epoch 7117, Train Loss: 0.01865309663116932, Test Loss: 0.07216517627239227\n",
      "Epoch 7118, Train Loss: 0.018648995086550713, Test Loss: 0.07217399775981903\n",
      "Epoch 7119, Train Loss: 0.018645301461219788, Test Loss: 0.072179414331913\n",
      "Epoch 7120, Train Loss: 0.018641332164406776, Test Loss: 0.07218135893344879\n",
      "Epoch 7121, Train Loss: 0.01863822340965271, Test Loss: 0.07218007743358612\n",
      "Epoch 7122, Train Loss: 0.018635060638189316, Test Loss: 0.07217621058225632\n",
      "Epoch 7123, Train Loss: 0.018630577251315117, Test Loss: 0.07216623425483704\n",
      "Epoch 7124, Train Loss: 0.018627610057592392, Test Loss: 0.07216281443834305\n",
      "Epoch 7125, Train Loss: 0.01862328127026558, Test Loss: 0.0721583440899849\n",
      "Epoch 7126, Train Loss: 0.01861969567835331, Test Loss: 0.07216278463602066\n",
      "Epoch 7127, Train Loss: 0.018615858629345894, Test Loss: 0.07216783612966537\n",
      "Epoch 7128, Train Loss: 0.018612530082464218, Test Loss: 0.072161465883255\n",
      "Epoch 7129, Train Loss: 0.018609199672937393, Test Loss: 0.07216482609510422\n",
      "Epoch 7130, Train Loss: 0.018605682998895645, Test Loss: 0.0721416100859642\n",
      "Epoch 7131, Train Loss: 0.018601365387439728, Test Loss: 0.07214480638504028\n",
      "Epoch 7132, Train Loss: 0.01859782449901104, Test Loss: 0.07214438915252686\n",
      "Epoch 7133, Train Loss: 0.0185942854732275, Test Loss: 0.07213908433914185\n",
      "Epoch 7134, Train Loss: 0.018590698018670082, Test Loss: 0.07213094085454941\n",
      "Epoch 7135, Train Loss: 0.018586937338113785, Test Loss: 0.07213820517063141\n",
      "Epoch 7136, Train Loss: 0.018583115190267563, Test Loss: 0.07214593887329102\n",
      "Epoch 7137, Train Loss: 0.018581174314022064, Test Loss: 0.07214181125164032\n",
      "Epoch 7138, Train Loss: 0.01857622154057026, Test Loss: 0.07213233411312103\n",
      "Epoch 7139, Train Loss: 0.01857193373143673, Test Loss: 0.07213835418224335\n",
      "Epoch 7140, Train Loss: 0.0185688603669405, Test Loss: 0.07212460041046143\n",
      "Epoch 7141, Train Loss: 0.01856468990445137, Test Loss: 0.07213284820318222\n",
      "Epoch 7142, Train Loss: 0.018560979515314102, Test Loss: 0.0721348449587822\n",
      "Epoch 7143, Train Loss: 0.01855759508907795, Test Loss: 0.07212352007627487\n",
      "Epoch 7144, Train Loss: 0.018553998321294785, Test Loss: 0.07211986929178238\n",
      "Epoch 7145, Train Loss: 0.018550174310803413, Test Loss: 0.07212536036968231\n",
      "Epoch 7146, Train Loss: 0.018546372652053833, Test Loss: 0.0721268504858017\n",
      "Epoch 7147, Train Loss: 0.018542923033237457, Test Loss: 0.0721317008137703\n",
      "Epoch 7148, Train Loss: 0.018539700657129288, Test Loss: 0.07212991267442703\n",
      "Epoch 7149, Train Loss: 0.018535835668444633, Test Loss: 0.07212002575397491\n",
      "Epoch 7150, Train Loss: 0.018532296642661095, Test Loss: 0.07211403548717499\n",
      "Epoch 7151, Train Loss: 0.018528448417782784, Test Loss: 0.07211233675479889\n",
      "Epoch 7152, Train Loss: 0.01852438598871231, Test Loss: 0.07211501151323318\n",
      "Epoch 7153, Train Loss: 0.018520956858992577, Test Loss: 0.07211669534444809\n",
      "Epoch 7154, Train Loss: 0.018518025055527687, Test Loss: 0.07211581617593765\n",
      "Epoch 7155, Train Loss: 0.018513837829232216, Test Loss: 0.07211258262395859\n",
      "Epoch 7156, Train Loss: 0.018510445952415466, Test Loss: 0.07212480157613754\n",
      "Epoch 7157, Train Loss: 0.018506772816181183, Test Loss: 0.07211049646139145\n",
      "Epoch 7158, Train Loss: 0.018503010272979736, Test Loss: 0.07211357355117798\n",
      "Epoch 7159, Train Loss: 0.018499664962291718, Test Loss: 0.07211549580097198\n",
      "Epoch 7160, Train Loss: 0.01849578693509102, Test Loss: 0.07211374491453171\n",
      "Epoch 7161, Train Loss: 0.018491996452212334, Test Loss: 0.07210403680801392\n",
      "Epoch 7162, Train Loss: 0.018488267436623573, Test Loss: 0.0721045583486557\n",
      "Epoch 7163, Train Loss: 0.01848454959690571, Test Loss: 0.07210046052932739\n",
      "Epoch 7164, Train Loss: 0.018481170758605003, Test Loss: 0.07209828495979309\n",
      "Epoch 7165, Train Loss: 0.01847716234624386, Test Loss: 0.0720921978354454\n",
      "Epoch 7166, Train Loss: 0.018473763018846512, Test Loss: 0.07209773361682892\n",
      "Epoch 7167, Train Loss: 0.018470123410224915, Test Loss: 0.07209755480289459\n",
      "Epoch 7168, Train Loss: 0.018466290086507797, Test Loss: 0.07209498435258865\n",
      "Epoch 7169, Train Loss: 0.018463052809238434, Test Loss: 0.07209266722202301\n",
      "Epoch 7170, Train Loss: 0.01845918409526348, Test Loss: 0.07208267599344254\n",
      "Epoch 7171, Train Loss: 0.018455544486641884, Test Loss: 0.07209837436676025\n",
      "Epoch 7172, Train Loss: 0.018452061340212822, Test Loss: 0.07209067791700363\n",
      "Epoch 7173, Train Loss: 0.01844875141978264, Test Loss: 0.0720742866396904\n",
      "Epoch 7174, Train Loss: 0.01844448782503605, Test Loss: 0.07209009677171707\n",
      "Epoch 7175, Train Loss: 0.018440820276737213, Test Loss: 0.07208550721406937\n",
      "Epoch 7176, Train Loss: 0.0184384286403656, Test Loss: 0.07207556068897247\n",
      "Epoch 7177, Train Loss: 0.018434740602970123, Test Loss: 0.07207116484642029\n",
      "Epoch 7178, Train Loss: 0.018430562689900398, Test Loss: 0.07206443697214127\n",
      "Epoch 7179, Train Loss: 0.018427064642310143, Test Loss: 0.0720539391040802\n",
      "Epoch 7180, Train Loss: 0.018422776833176613, Test Loss: 0.07205984741449356\n",
      "Epoch 7181, Train Loss: 0.018419068306684494, Test Loss: 0.07206613570451736\n",
      "Epoch 7182, Train Loss: 0.018415475264191628, Test Loss: 0.07206524908542633\n",
      "Epoch 7183, Train Loss: 0.01841193065047264, Test Loss: 0.07206253707408905\n",
      "Epoch 7184, Train Loss: 0.01840823143720627, Test Loss: 0.07206172496080399\n",
      "Epoch 7185, Train Loss: 0.018404394388198853, Test Loss: 0.0720643624663353\n",
      "Epoch 7186, Train Loss: 0.01840175688266754, Test Loss: 0.07206310331821442\n",
      "Epoch 7187, Train Loss: 0.018397679552435875, Test Loss: 0.07205035537481308\n",
      "Epoch 7188, Train Loss: 0.018393442034721375, Test Loss: 0.07206127792596817\n",
      "Epoch 7189, Train Loss: 0.018390251323580742, Test Loss: 0.07205110788345337\n",
      "Epoch 7190, Train Loss: 0.018386216834187508, Test Loss: 0.07205641269683838\n",
      "Epoch 7191, Train Loss: 0.018382666632533073, Test Loss: 0.0720457211136818\n",
      "Epoch 7192, Train Loss: 0.018379610031843185, Test Loss: 0.07203526049852371\n",
      "Epoch 7193, Train Loss: 0.01837611198425293, Test Loss: 0.07203669846057892\n",
      "Epoch 7194, Train Loss: 0.018371878191828728, Test Loss: 0.07205259054899216\n",
      "Epoch 7195, Train Loss: 0.018368737772107124, Test Loss: 0.07204516977071762\n",
      "Epoch 7196, Train Loss: 0.01836489886045456, Test Loss: 0.07204068452119827\n",
      "Epoch 7197, Train Loss: 0.01836126111447811, Test Loss: 0.07203225791454315\n",
      "Epoch 7198, Train Loss: 0.01835714653134346, Test Loss: 0.07203685492277145\n",
      "Epoch 7199, Train Loss: 0.018354346975684166, Test Loss: 0.07203604280948639\n",
      "Epoch 7200, Train Loss: 0.018350254744291306, Test Loss: 0.07203532755374908\n",
      "Epoch 7201, Train Loss: 0.018346542492508888, Test Loss: 0.07203235477209091\n",
      "Epoch 7202, Train Loss: 0.018343154340982437, Test Loss: 0.07203169167041779\n",
      "Epoch 7203, Train Loss: 0.018339309841394424, Test Loss: 0.07203211635351181\n",
      "Epoch 7204, Train Loss: 0.018335862085223198, Test Loss: 0.07202628254890442\n",
      "Epoch 7205, Train Loss: 0.018332546576857567, Test Loss: 0.07202400267124176\n",
      "Epoch 7206, Train Loss: 0.018328359350562096, Test Loss: 0.07202854752540588\n",
      "Epoch 7207, Train Loss: 0.018325528129935265, Test Loss: 0.07201127707958221\n",
      "Epoch 7208, Train Loss: 0.01832122914493084, Test Loss: 0.07202061265707016\n",
      "Epoch 7209, Train Loss: 0.018317371606826782, Test Loss: 0.07202940434217453\n",
      "Epoch 7210, Train Loss: 0.018313877284526825, Test Loss: 0.07202373445034027\n",
      "Epoch 7211, Train Loss: 0.018310198560357094, Test Loss: 0.07201968878507614\n",
      "Epoch 7212, Train Loss: 0.018306687474250793, Test Loss: 0.07202041149139404\n",
      "Epoch 7213, Train Loss: 0.018303094431757927, Test Loss: 0.07201064378023148\n",
      "Epoch 7214, Train Loss: 0.0182995293289423, Test Loss: 0.0720151737332344\n",
      "Epoch 7215, Train Loss: 0.01829594559967518, Test Loss: 0.07200545072555542\n",
      "Epoch 7216, Train Loss: 0.01829245127737522, Test Loss: 0.07200929522514343\n",
      "Epoch 7217, Train Loss: 0.01828889548778534, Test Loss: 0.07200122624635696\n",
      "Epoch 7218, Train Loss: 0.018285615369677544, Test Loss: 0.07201073318719864\n",
      "Epoch 7219, Train Loss: 0.018281228840351105, Test Loss: 0.072007916867733\n",
      "Epoch 7220, Train Loss: 0.018277762457728386, Test Loss: 0.07201074808835983\n",
      "Epoch 7221, Train Loss: 0.01827426627278328, Test Loss: 0.07201077789068222\n",
      "Epoch 7222, Train Loss: 0.018270600587129593, Test Loss: 0.07201283425092697\n",
      "Epoch 7223, Train Loss: 0.018268093466758728, Test Loss: 0.07201147824525833\n",
      "Epoch 7224, Train Loss: 0.01826404593884945, Test Loss: 0.07200175523757935\n",
      "Epoch 7225, Train Loss: 0.018260573968291283, Test Loss: 0.07199545949697495\n",
      "Epoch 7226, Train Loss: 0.0182571429759264, Test Loss: 0.07200565934181213\n",
      "Epoch 7227, Train Loss: 0.018252799287438393, Test Loss: 0.07199782878160477\n",
      "Epoch 7228, Train Loss: 0.01824941672384739, Test Loss: 0.07199231535196304\n",
      "Epoch 7229, Train Loss: 0.01824626699090004, Test Loss: 0.07198672741651535\n",
      "Epoch 7230, Train Loss: 0.018242040649056435, Test Loss: 0.071989044547081\n",
      "Epoch 7231, Train Loss: 0.01823858730494976, Test Loss: 0.0719815045595169\n",
      "Epoch 7232, Train Loss: 0.018235353752970695, Test Loss: 0.07198581099510193\n",
      "Epoch 7233, Train Loss: 0.018231604248285294, Test Loss: 0.07197704911231995\n",
      "Epoch 7234, Train Loss: 0.018227670341730118, Test Loss: 0.07197447121143341\n",
      "Epoch 7235, Train Loss: 0.018223976716399193, Test Loss: 0.07197564840316772\n",
      "Epoch 7236, Train Loss: 0.018220948055386543, Test Loss: 0.07197333127260208\n",
      "Epoch 7237, Train Loss: 0.018216893076896667, Test Loss: 0.07197953015565872\n",
      "Epoch 7238, Train Loss: 0.01821366883814335, Test Loss: 0.07198620587587357\n",
      "Epoch 7239, Train Loss: 0.018209943547844887, Test Loss: 0.07197821885347366\n",
      "Epoch 7240, Train Loss: 0.01820608228445053, Test Loss: 0.07197071611881256\n",
      "Epoch 7241, Train Loss: 0.018203483894467354, Test Loss: 0.0719745084643364\n",
      "Epoch 7242, Train Loss: 0.01820036955177784, Test Loss: 0.07198555767536163\n",
      "Epoch 7243, Train Loss: 0.018195608630776405, Test Loss: 0.07197516411542892\n",
      "Epoch 7244, Train Loss: 0.018192194402217865, Test Loss: 0.07196707278490067\n",
      "Epoch 7245, Train Loss: 0.01818806119263172, Test Loss: 0.07196349650621414\n",
      "Epoch 7246, Train Loss: 0.018185319378972054, Test Loss: 0.07195665687322617\n",
      "Epoch 7247, Train Loss: 0.01818120665848255, Test Loss: 0.0719592347741127\n",
      "Epoch 7248, Train Loss: 0.01817803643643856, Test Loss: 0.07196331024169922\n",
      "Epoch 7249, Train Loss: 0.018173862248659134, Test Loss: 0.07196492701768875\n",
      "Epoch 7250, Train Loss: 0.018170751631259918, Test Loss: 0.07195773720741272\n",
      "Epoch 7251, Train Loss: 0.018167637288570404, Test Loss: 0.07194926589727402\n",
      "Epoch 7252, Train Loss: 0.01816391572356224, Test Loss: 0.07193893939256668\n",
      "Epoch 7253, Train Loss: 0.01815955899655819, Test Loss: 0.07194600999355316\n",
      "Epoch 7254, Train Loss: 0.018155977129936218, Test Loss: 0.07194801419973373\n",
      "Epoch 7255, Train Loss: 0.01815241388976574, Test Loss: 0.0719490796327591\n",
      "Epoch 7256, Train Loss: 0.018149124458432198, Test Loss: 0.0719401016831398\n",
      "Epoch 7257, Train Loss: 0.018145469948649406, Test Loss: 0.07193797826766968\n",
      "Epoch 7258, Train Loss: 0.01814180240035057, Test Loss: 0.071937195956707\n",
      "Epoch 7259, Train Loss: 0.01813807711005211, Test Loss: 0.07194288820028305\n",
      "Epoch 7260, Train Loss: 0.018135644495487213, Test Loss: 0.07193530350923538\n",
      "Epoch 7261, Train Loss: 0.018131205812096596, Test Loss: 0.07193321734666824\n",
      "Epoch 7262, Train Loss: 0.01812790520489216, Test Loss: 0.0719364657998085\n",
      "Epoch 7263, Train Loss: 0.018124153837561607, Test Loss: 0.07192941009998322\n",
      "Epoch 7264, Train Loss: 0.018120914697647095, Test Loss: 0.07193678617477417\n",
      "Epoch 7265, Train Loss: 0.018116949126124382, Test Loss: 0.07192764431238174\n",
      "Epoch 7266, Train Loss: 0.018115336075425148, Test Loss: 0.07193008810281754\n",
      "Epoch 7267, Train Loss: 0.0181108471006155, Test Loss: 0.07192212343215942\n",
      "Epoch 7268, Train Loss: 0.01810632273554802, Test Loss: 0.07191148400306702\n",
      "Epoch 7269, Train Loss: 0.018102634698152542, Test Loss: 0.0719064399600029\n",
      "Epoch 7270, Train Loss: 0.018099645152688026, Test Loss: 0.07190250605344772\n",
      "Epoch 7271, Train Loss: 0.018095925450325012, Test Loss: 0.07190140336751938\n",
      "Epoch 7272, Train Loss: 0.018092507496476173, Test Loss: 0.0719042718410492\n",
      "Epoch 7273, Train Loss: 0.01808883249759674, Test Loss: 0.07191141694784164\n",
      "Epoch 7274, Train Loss: 0.018084678798913956, Test Loss: 0.07191118597984314\n",
      "Epoch 7275, Train Loss: 0.01808175817131996, Test Loss: 0.07191365212202072\n",
      "Epoch 7276, Train Loss: 0.01807805337011814, Test Loss: 0.07191801816225052\n",
      "Epoch 7277, Train Loss: 0.018074292689561844, Test Loss: 0.07191116362810135\n",
      "Epoch 7278, Train Loss: 0.018071169033646584, Test Loss: 0.07190362364053726\n",
      "Epoch 7279, Train Loss: 0.018066968768835068, Test Loss: 0.07190290838479996\n",
      "Epoch 7280, Train Loss: 0.018063297495245934, Test Loss: 0.07191143929958344\n",
      "Epoch 7281, Train Loss: 0.01805993728339672, Test Loss: 0.07190685719251633\n",
      "Epoch 7282, Train Loss: 0.018056247383356094, Test Loss: 0.07190396636724472\n",
      "Epoch 7283, Train Loss: 0.018052687868475914, Test Loss: 0.07190889120101929\n",
      "Epoch 7284, Train Loss: 0.018049288541078568, Test Loss: 0.07189662754535675\n",
      "Epoch 7285, Train Loss: 0.018045926466584206, Test Loss: 0.07188951969146729\n",
      "Epoch 7286, Train Loss: 0.01804238371551037, Test Loss: 0.07188566774129868\n",
      "Epoch 7287, Train Loss: 0.018038423731923103, Test Loss: 0.07188109308481216\n",
      "Epoch 7288, Train Loss: 0.01803511567413807, Test Loss: 0.07188320904970169\n",
      "Epoch 7289, Train Loss: 0.01803128980100155, Test Loss: 0.0718783512711525\n",
      "Epoch 7290, Train Loss: 0.01802816614508629, Test Loss: 0.07188237458467484\n",
      "Epoch 7291, Train Loss: 0.01802491955459118, Test Loss: 0.07188247889280319\n",
      "Epoch 7292, Train Loss: 0.018020600080490112, Test Loss: 0.07188139110803604\n",
      "Epoch 7293, Train Loss: 0.01801707036793232, Test Loss: 0.07187739759683609\n",
      "Epoch 7294, Train Loss: 0.01801377162337303, Test Loss: 0.07187513262033463\n",
      "Epoch 7295, Train Loss: 0.018010033294558525, Test Loss: 0.07188097387552261\n",
      "Epoch 7296, Train Loss: 0.018006738275289536, Test Loss: 0.07188575714826584\n",
      "Epoch 7297, Train Loss: 0.018002904951572418, Test Loss: 0.07188001275062561\n",
      "Epoch 7298, Train Loss: 0.017999127507209778, Test Loss: 0.07187976688146591\n",
      "Epoch 7299, Train Loss: 0.01799606718122959, Test Loss: 0.07188200950622559\n",
      "Epoch 7300, Train Loss: 0.017992669716477394, Test Loss: 0.07187678664922714\n",
      "Epoch 7301, Train Loss: 0.01798870600759983, Test Loss: 0.071867935359478\n",
      "Epoch 7302, Train Loss: 0.017985103651881218, Test Loss: 0.0718667209148407\n",
      "Epoch 7303, Train Loss: 0.017982838675379753, Test Loss: 0.0718577578663826\n",
      "Epoch 7304, Train Loss: 0.017978327348828316, Test Loss: 0.0718492716550827\n",
      "Epoch 7305, Train Loss: 0.017974942922592163, Test Loss: 0.07185463607311249\n",
      "Epoch 7306, Train Loss: 0.01797129213809967, Test Loss: 0.07186149805784225\n",
      "Epoch 7307, Train Loss: 0.01796739362180233, Test Loss: 0.07186602801084518\n",
      "Epoch 7308, Train Loss: 0.01796402782201767, Test Loss: 0.071869857609272\n",
      "Epoch 7309, Train Loss: 0.017960647121071815, Test Loss: 0.07185708731412888\n",
      "Epoch 7310, Train Loss: 0.017957165837287903, Test Loss: 0.07185602188110352\n",
      "Epoch 7311, Train Loss: 0.017954014241695404, Test Loss: 0.0718526616692543\n",
      "Epoch 7312, Train Loss: 0.01794949732720852, Test Loss: 0.07185200601816177\n",
      "Epoch 7313, Train Loss: 0.017946043983101845, Test Loss: 0.07184211909770966\n",
      "Epoch 7314, Train Loss: 0.017943132668733597, Test Loss: 0.07184652984142303\n",
      "Epoch 7315, Train Loss: 0.017939701676368713, Test Loss: 0.07183483988046646\n",
      "Epoch 7316, Train Loss: 0.017935335636138916, Test Loss: 0.07184293121099472\n",
      "Epoch 7317, Train Loss: 0.0179319866001606, Test Loss: 0.07183565944433212\n",
      "Epoch 7318, Train Loss: 0.017928624525666237, Test Loss: 0.0718391165137291\n",
      "Epoch 7319, Train Loss: 0.017924955114722252, Test Loss: 0.07184068858623505\n",
      "Epoch 7320, Train Loss: 0.017921503633260727, Test Loss: 0.07183077186346054\n",
      "Epoch 7321, Train Loss: 0.01791745238006115, Test Loss: 0.07183843851089478\n",
      "Epoch 7322, Train Loss: 0.01791420392692089, Test Loss: 0.07183757424354553\n",
      "Epoch 7323, Train Loss: 0.01791064627468586, Test Loss: 0.07184067368507385\n",
      "Epoch 7324, Train Loss: 0.017907753586769104, Test Loss: 0.07183443754911423\n",
      "Epoch 7325, Train Loss: 0.017903653904795647, Test Loss: 0.07182618975639343\n",
      "Epoch 7326, Train Loss: 0.01790030486881733, Test Loss: 0.07182446867227554\n",
      "Epoch 7327, Train Loss: 0.017896458506584167, Test Loss: 0.07183053344488144\n",
      "Epoch 7328, Train Loss: 0.01789291389286518, Test Loss: 0.07182446867227554\n",
      "Epoch 7329, Train Loss: 0.017889434471726418, Test Loss: 0.07182825356721878\n",
      "Epoch 7330, Train Loss: 0.017885979264974594, Test Loss: 0.07183513045310974\n",
      "Epoch 7331, Train Loss: 0.01788245141506195, Test Loss: 0.07183191180229187\n",
      "Epoch 7332, Train Loss: 0.017878687009215355, Test Loss: 0.07182598114013672\n",
      "Epoch 7333, Train Loss: 0.017875580117106438, Test Loss: 0.0718284472823143\n",
      "Epoch 7334, Train Loss: 0.017871858552098274, Test Loss: 0.0718182697892189\n",
      "Epoch 7335, Train Loss: 0.01786818355321884, Test Loss: 0.07181961089372635\n",
      "Epoch 7336, Train Loss: 0.017865128815174103, Test Loss: 0.07181482017040253\n",
      "Epoch 7337, Train Loss: 0.01786293275654316, Test Loss: 0.07180982083082199\n",
      "Epoch 7338, Train Loss: 0.01785789430141449, Test Loss: 0.07180649042129517\n",
      "Epoch 7339, Train Loss: 0.017854347825050354, Test Loss: 0.07180586457252502\n",
      "Epoch 7340, Train Loss: 0.01785055547952652, Test Loss: 0.0718190148472786\n",
      "Epoch 7341, Train Loss: 0.017846999689936638, Test Loss: 0.07179660350084305\n",
      "Epoch 7342, Train Loss: 0.01784355193376541, Test Loss: 0.0717950165271759\n",
      "Epoch 7343, Train Loss: 0.017840294167399406, Test Loss: 0.07178913056850433\n",
      "Epoch 7344, Train Loss: 0.017836423590779305, Test Loss: 0.07180416584014893\n",
      "Epoch 7345, Train Loss: 0.017833111807703972, Test Loss: 0.07179197669029236\n",
      "Epoch 7346, Train Loss: 0.01782955974340439, Test Loss: 0.07178693264722824\n",
      "Epoch 7347, Train Loss: 0.0178262647241354, Test Loss: 0.07178580015897751\n",
      "Epoch 7348, Train Loss: 0.017822884023189545, Test Loss: 0.07180346548557281\n",
      "Epoch 7349, Train Loss: 0.017819004133343697, Test Loss: 0.07179167866706848\n",
      "Epoch 7350, Train Loss: 0.017815643921494484, Test Loss: 0.07179469615221024\n",
      "Epoch 7351, Train Loss: 0.017812127247452736, Test Loss: 0.07178447395563126\n",
      "Epoch 7352, Train Loss: 0.017808258533477783, Test Loss: 0.07177945971488953\n",
      "Epoch 7353, Train Loss: 0.017805058509111404, Test Loss: 0.07177826762199402\n",
      "Epoch 7354, Train Loss: 0.017801444977521896, Test Loss: 0.07177896797657013\n",
      "Epoch 7355, Train Loss: 0.01779812201857567, Test Loss: 0.0717814713716507\n",
      "Epoch 7356, Train Loss: 0.017795324325561523, Test Loss: 0.07177764922380447\n",
      "Epoch 7357, Train Loss: 0.017791427671909332, Test Loss: 0.07177313417196274\n",
      "Epoch 7358, Train Loss: 0.017787190154194832, Test Loss: 0.07177194207906723\n",
      "Epoch 7359, Train Loss: 0.017783433198928833, Test Loss: 0.0717671811580658\n",
      "Epoch 7360, Train Loss: 0.017780182883143425, Test Loss: 0.07176434248685837\n",
      "Epoch 7361, Train Loss: 0.017776815220713615, Test Loss: 0.07176075875759125\n",
      "Epoch 7362, Train Loss: 0.017773274332284927, Test Loss: 0.07175945490598679\n",
      "Epoch 7363, Train Loss: 0.017769845202565193, Test Loss: 0.07175727188587189\n",
      "Epoch 7364, Train Loss: 0.017765900120139122, Test Loss: 0.07176387310028076\n",
      "Epoch 7365, Train Loss: 0.017762361094355583, Test Loss: 0.07177181541919708\n",
      "Epoch 7366, Train Loss: 0.01775895617902279, Test Loss: 0.07176145166158676\n",
      "Epoch 7367, Train Loss: 0.017755523324012756, Test Loss: 0.07175154983997345\n",
      "Epoch 7368, Train Loss: 0.017752356827259064, Test Loss: 0.07175366580486298\n",
      "Epoch 7369, Train Loss: 0.01774907298386097, Test Loss: 0.07176027446985245\n",
      "Epoch 7370, Train Loss: 0.0177449993789196, Test Loss: 0.07175438106060028\n",
      "Epoch 7371, Train Loss: 0.01774170808494091, Test Loss: 0.071757011115551\n",
      "Epoch 7372, Train Loss: 0.017738129943609238, Test Loss: 0.07174548506736755\n",
      "Epoch 7373, Train Loss: 0.017734600231051445, Test Loss: 0.07174011319875717\n",
      "Epoch 7374, Train Loss: 0.0177310798317194, Test Loss: 0.07174229621887207\n",
      "Epoch 7375, Train Loss: 0.017727883532643318, Test Loss: 0.07173256576061249\n",
      "Epoch 7376, Train Loss: 0.01772387884557247, Test Loss: 0.07174205034971237\n",
      "Epoch 7377, Train Loss: 0.01772012561559677, Test Loss: 0.07173974066972733\n",
      "Epoch 7378, Train Loss: 0.017717186361551285, Test Loss: 0.071732297539711\n",
      "Epoch 7379, Train Loss: 0.017713887616991997, Test Loss: 0.07172328233718872\n",
      "Epoch 7380, Train Loss: 0.017710750922560692, Test Loss: 0.07173486053943634\n",
      "Epoch 7381, Train Loss: 0.017706146463751793, Test Loss: 0.07173433899879456\n",
      "Epoch 7382, Train Loss: 0.017702924087643623, Test Loss: 0.07173279672861099\n",
      "Epoch 7383, Train Loss: 0.01769915036857128, Test Loss: 0.07172448188066483\n",
      "Epoch 7384, Train Loss: 0.01769617386162281, Test Loss: 0.07172581553459167\n",
      "Epoch 7385, Train Loss: 0.017692502588033676, Test Loss: 0.07171431183815002\n",
      "Epoch 7386, Train Loss: 0.017689034342765808, Test Loss: 0.07171212136745453\n",
      "Epoch 7387, Train Loss: 0.017686178907752037, Test Loss: 0.07170567661523819\n",
      "Epoch 7388, Train Loss: 0.017681995406746864, Test Loss: 0.07171452790498734\n",
      "Epoch 7389, Train Loss: 0.01767915114760399, Test Loss: 0.07171103358268738\n",
      "Epoch 7390, Train Loss: 0.017675789073109627, Test Loss: 0.07170529663562775\n",
      "Epoch 7391, Train Loss: 0.017672698944807053, Test Loss: 0.07169883698225021\n",
      "Epoch 7392, Train Loss: 0.017667748034000397, Test Loss: 0.07170362770557404\n",
      "Epoch 7393, Train Loss: 0.017664311453700066, Test Loss: 0.07170773297548294\n",
      "Epoch 7394, Train Loss: 0.017661282792687416, Test Loss: 0.07171730697154999\n",
      "Epoch 7395, Train Loss: 0.01765742525458336, Test Loss: 0.07170697301626205\n",
      "Epoch 7396, Train Loss: 0.017654450610280037, Test Loss: 0.07171432673931122\n",
      "Epoch 7397, Train Loss: 0.01765047200024128, Test Loss: 0.0717005804181099\n",
      "Epoch 7398, Train Loss: 0.017647087574005127, Test Loss: 0.07170002162456512\n",
      "Epoch 7399, Train Loss: 0.017643487080931664, Test Loss: 0.07169060409069061\n",
      "Epoch 7400, Train Loss: 0.017640378326177597, Test Loss: 0.07169159501791\n",
      "Epoch 7401, Train Loss: 0.01763637736439705, Test Loss: 0.07169827818870544\n",
      "Epoch 7402, Train Loss: 0.017632976174354553, Test Loss: 0.07169989496469498\n",
      "Epoch 7403, Train Loss: 0.017629830166697502, Test Loss: 0.07168145477771759\n",
      "Epoch 7404, Train Loss: 0.017626438289880753, Test Loss: 0.07168053090572357\n",
      "Epoch 7405, Train Loss: 0.017622971907258034, Test Loss: 0.0716920793056488\n",
      "Epoch 7406, Train Loss: 0.017619526013731956, Test Loss: 0.07168743014335632\n",
      "Epoch 7407, Train Loss: 0.017615798860788345, Test Loss: 0.07169265300035477\n",
      "Epoch 7408, Train Loss: 0.017612358555197716, Test Loss: 0.07169060409069061\n",
      "Epoch 7409, Train Loss: 0.01760876551270485, Test Loss: 0.07169172167778015\n",
      "Epoch 7410, Train Loss: 0.017605777829885483, Test Loss: 0.07166893035173416\n",
      "Epoch 7411, Train Loss: 0.01760200224816799, Test Loss: 0.07167883217334747\n",
      "Epoch 7412, Train Loss: 0.017598247155547142, Test Loss: 0.07167892158031464\n",
      "Epoch 7413, Train Loss: 0.017594654113054276, Test Loss: 0.07167473435401917\n",
      "Epoch 7414, Train Loss: 0.01759137026965618, Test Loss: 0.07166890799999237\n",
      "Epoch 7415, Train Loss: 0.01758778654038906, Test Loss: 0.07167546451091766\n",
      "Epoch 7416, Train Loss: 0.017584480345249176, Test Loss: 0.0716632828116417\n",
      "Epoch 7417, Train Loss: 0.017580872401595116, Test Loss: 0.07166007906198502\n",
      "Epoch 7418, Train Loss: 0.01757756434381008, Test Loss: 0.07167188823223114\n",
      "Epoch 7419, Train Loss: 0.017574097961187363, Test Loss: 0.07166799157857895\n",
      "Epoch 7420, Train Loss: 0.017570381984114647, Test Loss: 0.07166282832622528\n",
      "Epoch 7421, Train Loss: 0.01756792515516281, Test Loss: 0.07165146619081497\n",
      "Epoch 7422, Train Loss: 0.01756448857486248, Test Loss: 0.0716480240225792\n",
      "Epoch 7423, Train Loss: 0.01756093092262745, Test Loss: 0.07165375351905823\n",
      "Epoch 7424, Train Loss: 0.017556969076395035, Test Loss: 0.0716472789645195\n",
      "Epoch 7425, Train Loss: 0.01755446009337902, Test Loss: 0.07165995985269547\n",
      "Epoch 7426, Train Loss: 0.017550332471728325, Test Loss: 0.07165703922510147\n",
      "Epoch 7427, Train Loss: 0.017546819522976875, Test Loss: 0.07164740562438965\n",
      "Epoch 7428, Train Loss: 0.017542991787195206, Test Loss: 0.07164272665977478\n",
      "Epoch 7429, Train Loss: 0.01753978803753853, Test Loss: 0.07163885980844498\n",
      "Epoch 7430, Train Loss: 0.017536530271172523, Test Loss: 0.07164491713047028\n",
      "Epoch 7431, Train Loss: 0.017533136531710625, Test Loss: 0.07163354754447937\n",
      "Epoch 7432, Train Loss: 0.0175292007625103, Test Loss: 0.0716397762298584\n",
      "Epoch 7433, Train Loss: 0.017526015639305115, Test Loss: 0.071633480489254\n",
      "Epoch 7434, Train Loss: 0.017522357404232025, Test Loss: 0.0716356709599495\n",
      "Epoch 7435, Train Loss: 0.01751979999244213, Test Loss: 0.07164079695940018\n",
      "Epoch 7436, Train Loss: 0.01751609891653061, Test Loss: 0.07163715362548828\n",
      "Epoch 7437, Train Loss: 0.01751212775707245, Test Loss: 0.07164191454648972\n",
      "Epoch 7438, Train Loss: 0.01750892587006092, Test Loss: 0.07163432985544205\n",
      "Epoch 7439, Train Loss: 0.017505420371890068, Test Loss: 0.07163671404123306\n",
      "Epoch 7440, Train Loss: 0.017501750960946083, Test Loss: 0.07163796573877335\n",
      "Epoch 7441, Train Loss: 0.017498238012194633, Test Loss: 0.0716324970126152\n",
      "Epoch 7442, Train Loss: 0.017495067790150642, Test Loss: 0.07162453979253769\n",
      "Epoch 7443, Train Loss: 0.017492074519395828, Test Loss: 0.07161632180213928\n",
      "Epoch 7444, Train Loss: 0.017488500103354454, Test Loss: 0.07160846889019012\n",
      "Epoch 7445, Train Loss: 0.017484555020928383, Test Loss: 0.07162202894687653\n",
      "Epoch 7446, Train Loss: 0.01748139038681984, Test Loss: 0.07161059975624084\n",
      "Epoch 7447, Train Loss: 0.017477868124842644, Test Loss: 0.07161653786897659\n",
      "Epoch 7448, Train Loss: 0.017474399879574776, Test Loss: 0.07161308825016022\n",
      "Epoch 7449, Train Loss: 0.017471393570303917, Test Loss: 0.0716068297624588\n",
      "Epoch 7450, Train Loss: 0.017467686906456947, Test Loss: 0.07160817086696625\n",
      "Epoch 7451, Train Loss: 0.017464229837059975, Test Loss: 0.0716002807021141\n",
      "Epoch 7452, Train Loss: 0.01746181584894657, Test Loss: 0.07159731537103653\n",
      "Epoch 7453, Train Loss: 0.017458252608776093, Test Loss: 0.07159671187400818\n",
      "Epoch 7454, Train Loss: 0.017453664913773537, Test Loss: 0.07160807400941849\n",
      "Epoch 7455, Train Loss: 0.01745053008198738, Test Loss: 0.07160568237304688\n",
      "Epoch 7456, Train Loss: 0.01744709350168705, Test Loss: 0.07159780710935593\n",
      "Epoch 7457, Train Loss: 0.01744355447590351, Test Loss: 0.07160055637359619\n",
      "Epoch 7458, Train Loss: 0.017440082505345345, Test Loss: 0.07160340994596481\n",
      "Epoch 7459, Train Loss: 0.017436549067497253, Test Loss: 0.07160154730081558\n",
      "Epoch 7460, Train Loss: 0.017434006556868553, Test Loss: 0.07158394157886505\n",
      "Epoch 7461, Train Loss: 0.017429707571864128, Test Loss: 0.07159505784511566\n",
      "Epoch 7462, Train Loss: 0.017426198348402977, Test Loss: 0.07159704715013504\n",
      "Epoch 7463, Train Loss: 0.01742291823029518, Test Loss: 0.07158206403255463\n",
      "Epoch 7464, Train Loss: 0.01742047630250454, Test Loss: 0.07158038020133972\n",
      "Epoch 7465, Train Loss: 0.01741626113653183, Test Loss: 0.0715828612446785\n",
      "Epoch 7466, Train Loss: 0.01741328090429306, Test Loss: 0.07158540934324265\n",
      "Epoch 7467, Train Loss: 0.017409244552254677, Test Loss: 0.07157453149557114\n",
      "Epoch 7468, Train Loss: 0.01740608736872673, Test Loss: 0.07157986611127853\n",
      "Epoch 7469, Train Loss: 0.01740230619907379, Test Loss: 0.07158005982637405\n",
      "Epoch 7470, Train Loss: 0.017399031668901443, Test Loss: 0.07157226651906967\n",
      "Epoch 7471, Train Loss: 0.017396828159689903, Test Loss: 0.07157661020755768\n",
      "Epoch 7472, Train Loss: 0.01739220879971981, Test Loss: 0.07158544659614563\n",
      "Epoch 7473, Train Loss: 0.017388682812452316, Test Loss: 0.0715789645910263\n",
      "Epoch 7474, Train Loss: 0.01738518849015236, Test Loss: 0.07157842814922333\n",
      "Epoch 7475, Train Loss: 0.0173820648342371, Test Loss: 0.07157840579748154\n",
      "Epoch 7476, Train Loss: 0.01737864688038826, Test Loss: 0.07158184796571732\n",
      "Epoch 7477, Train Loss: 0.017375782132148743, Test Loss: 0.07158666104078293\n",
      "Epoch 7478, Train Loss: 0.01737169548869133, Test Loss: 0.07156948000192642\n",
      "Epoch 7479, Train Loss: 0.017368368804454803, Test Loss: 0.07156035304069519\n",
      "Epoch 7480, Train Loss: 0.01736483909189701, Test Loss: 0.07156084477901459\n",
      "Epoch 7481, Train Loss: 0.01736132614314556, Test Loss: 0.07155030965805054\n",
      "Epoch 7482, Train Loss: 0.01735827699303627, Test Loss: 0.07156281173229218\n",
      "Epoch 7483, Train Loss: 0.01735461875796318, Test Loss: 0.07154994457960129\n",
      "Epoch 7484, Train Loss: 0.017351316288113594, Test Loss: 0.07155562192201614\n",
      "Epoch 7485, Train Loss: 0.01734803430736065, Test Loss: 0.07156010717153549\n",
      "Epoch 7486, Train Loss: 0.017344487830996513, Test Loss: 0.07155758142471313\n",
      "Epoch 7487, Train Loss: 0.01734112948179245, Test Loss: 0.07156350463628769\n",
      "Epoch 7488, Train Loss: 0.017338013276457787, Test Loss: 0.07155220210552216\n",
      "Epoch 7489, Train Loss: 0.01733415387570858, Test Loss: 0.07154922932386398\n",
      "Epoch 7490, Train Loss: 0.017332138493657112, Test Loss: 0.07155097275972366\n",
      "Epoch 7491, Train Loss: 0.017327453941106796, Test Loss: 0.07155128568410873\n",
      "Epoch 7492, Train Loss: 0.017324019223451614, Test Loss: 0.0715431198477745\n",
      "Epoch 7493, Train Loss: 0.017320701852440834, Test Loss: 0.07153457403182983\n",
      "Epoch 7494, Train Loss: 0.01731778495013714, Test Loss: 0.07152385264635086\n",
      "Epoch 7495, Train Loss: 0.017314178869128227, Test Loss: 0.07151814550161362\n",
      "Epoch 7496, Train Loss: 0.017311476171016693, Test Loss: 0.07152664661407471\n",
      "Epoch 7497, Train Loss: 0.017307138070464134, Test Loss: 0.07152870297431946\n",
      "Epoch 7498, Train Loss: 0.017303723841905594, Test Loss: 0.07152581214904785\n",
      "Epoch 7499, Train Loss: 0.017300400882959366, Test Loss: 0.07152437418699265\n",
      "Epoch 7500, Train Loss: 0.017297614365816116, Test Loss: 0.07151960581541061\n",
      "Epoch 7501, Train Loss: 0.017293758690357208, Test Loss: 0.0715237483382225\n",
      "Epoch 7502, Train Loss: 0.01729024201631546, Test Loss: 0.07152139395475388\n",
      "Epoch 7503, Train Loss: 0.017286814749240875, Test Loss: 0.07151922583580017\n",
      "Epoch 7504, Train Loss: 0.017283199355006218, Test Loss: 0.07151466608047485\n",
      "Epoch 7505, Train Loss: 0.017279770225286484, Test Loss: 0.07152475416660309\n",
      "Epoch 7506, Train Loss: 0.017277630046010017, Test Loss: 0.07151629775762558\n",
      "Epoch 7507, Train Loss: 0.017273379489779472, Test Loss: 0.07151506096124649\n",
      "Epoch 7508, Train Loss: 0.017269909381866455, Test Loss: 0.07151088118553162\n",
      "Epoch 7509, Train Loss: 0.017266325652599335, Test Loss: 0.07151506096124649\n",
      "Epoch 7510, Train Loss: 0.01726299151778221, Test Loss: 0.07150464504957199\n",
      "Epoch 7511, Train Loss: 0.017260154709219933, Test Loss: 0.07149170339107513\n",
      "Epoch 7512, Train Loss: 0.017256099730730057, Test Loss: 0.07149461656808853\n",
      "Epoch 7513, Train Loss: 0.01725374348461628, Test Loss: 0.07150332629680634\n",
      "Epoch 7514, Train Loss: 0.01724923960864544, Test Loss: 0.07149650156497955\n",
      "Epoch 7515, Train Loss: 0.017245911061763763, Test Loss: 0.07149137556552887\n",
      "Epoch 7516, Train Loss: 0.017242688685655594, Test Loss: 0.07150103896856308\n",
      "Epoch 7517, Train Loss: 0.017239026725292206, Test Loss: 0.07149365544319153\n",
      "Epoch 7518, Train Loss: 0.017235616222023964, Test Loss: 0.071494922041893\n",
      "Epoch 7519, Train Loss: 0.017232555896043777, Test Loss: 0.07148654013872147\n",
      "Epoch 7520, Train Loss: 0.017229054123163223, Test Loss: 0.07148427516222\n",
      "Epoch 7521, Train Loss: 0.01722560077905655, Test Loss: 0.07148312777280807\n",
      "Epoch 7522, Train Loss: 0.017222952097654343, Test Loss: 0.07149942964315414\n",
      "Epoch 7523, Train Loss: 0.017219379544258118, Test Loss: 0.07149781286716461\n",
      "Epoch 7524, Train Loss: 0.017215626314282417, Test Loss: 0.0714920163154602\n",
      "Epoch 7525, Train Loss: 0.01721181534230709, Test Loss: 0.07149112969636917\n",
      "Epoch 7526, Train Loss: 0.017208797857165337, Test Loss: 0.07148967683315277\n",
      "Epoch 7527, Train Loss: 0.017205454409122467, Test Loss: 0.0714779794216156\n",
      "Epoch 7528, Train Loss: 0.01720266044139862, Test Loss: 0.07147476822137833\n",
      "Epoch 7529, Train Loss: 0.01719846948981285, Test Loss: 0.07147499173879623\n",
      "Epoch 7530, Train Loss: 0.01719593070447445, Test Loss: 0.07146808505058289\n",
      "Epoch 7531, Train Loss: 0.017193173989653587, Test Loss: 0.07148650288581848\n",
      "Epoch 7532, Train Loss: 0.01718812622129917, Test Loss: 0.07147261500358582\n",
      "Epoch 7533, Train Loss: 0.017184728756546974, Test Loss: 0.07146750390529633\n",
      "Epoch 7534, Train Loss: 0.01718159392476082, Test Loss: 0.07146842777729034\n",
      "Epoch 7535, Train Loss: 0.01717855967581272, Test Loss: 0.07146500051021576\n",
      "Epoch 7536, Train Loss: 0.017175018787384033, Test Loss: 0.07145651429891586\n",
      "Epoch 7537, Train Loss: 0.0171721912920475, Test Loss: 0.07146332412958145\n",
      "Epoch 7538, Train Loss: 0.017168378457427025, Test Loss: 0.07145364582538605\n",
      "Epoch 7539, Train Loss: 0.017164723947644234, Test Loss: 0.07145605236291885\n",
      "Epoch 7540, Train Loss: 0.01716146431863308, Test Loss: 0.07146015763282776\n",
      "Epoch 7541, Train Loss: 0.01715809665620327, Test Loss: 0.07145898789167404\n",
      "Epoch 7542, Train Loss: 0.017155248671770096, Test Loss: 0.0714634582400322\n",
      "Epoch 7543, Train Loss: 0.017153721302747726, Test Loss: 0.07145623862743378\n",
      "Epoch 7544, Train Loss: 0.01714826002717018, Test Loss: 0.07145196199417114\n",
      "Epoch 7545, Train Loss: 0.01714499481022358, Test Loss: 0.07144486159086227\n",
      "Epoch 7546, Train Loss: 0.017141006886959076, Test Loss: 0.07144474983215332\n",
      "Epoch 7547, Train Loss: 0.017138680443167686, Test Loss: 0.07144316285848618\n",
      "Epoch 7548, Train Loss: 0.017135096713900566, Test Loss: 0.0714484304189682\n",
      "Epoch 7549, Train Loss: 0.017131362110376358, Test Loss: 0.07145023345947266\n",
      "Epoch 7550, Train Loss: 0.01712757721543312, Test Loss: 0.07145004719495773\n",
      "Epoch 7551, Train Loss: 0.017124058678746223, Test Loss: 0.07144507020711899\n",
      "Epoch 7552, Train Loss: 0.017121121287345886, Test Loss: 0.07144013047218323\n",
      "Epoch 7553, Train Loss: 0.017117375507950783, Test Loss: 0.07143549621105194\n",
      "Epoch 7554, Train Loss: 0.017113836482167244, Test Loss: 0.0714363232254982\n",
      "Epoch 7555, Train Loss: 0.017110444605350494, Test Loss: 0.071437306702137\n",
      "Epoch 7556, Train Loss: 0.017107361927628517, Test Loss: 0.07143427431583405\n",
      "Epoch 7557, Train Loss: 0.01710435189306736, Test Loss: 0.07144387811422348\n",
      "Epoch 7558, Train Loss: 0.017100580036640167, Test Loss: 0.07144444435834885\n",
      "Epoch 7559, Train Loss: 0.01709790527820587, Test Loss: 0.0714205875992775\n",
      "Epoch 7560, Train Loss: 0.017093827947974205, Test Loss: 0.07142288237810135\n",
      "Epoch 7561, Train Loss: 0.017091097310185432, Test Loss: 0.07141774147748947\n",
      "Epoch 7562, Train Loss: 0.01708713173866272, Test Loss: 0.07142256200313568\n",
      "Epoch 7563, Train Loss: 0.017083484679460526, Test Loss: 0.07142288237810135\n",
      "Epoch 7564, Train Loss: 0.01708057150244713, Test Loss: 0.07142084091901779\n",
      "Epoch 7565, Train Loss: 0.017077773809432983, Test Loss: 0.07140708714723587\n",
      "Epoch 7566, Train Loss: 0.01707357168197632, Test Loss: 0.07140159606933594\n",
      "Epoch 7567, Train Loss: 0.017070414498448372, Test Loss: 0.07139832526445389\n",
      "Epoch 7568, Train Loss: 0.017066845670342445, Test Loss: 0.0714024230837822\n",
      "Epoch 7569, Train Loss: 0.0170641727745533, Test Loss: 0.07141339033842087\n",
      "Epoch 7570, Train Loss: 0.017060253769159317, Test Loss: 0.07140525430440903\n",
      "Epoch 7571, Train Loss: 0.01705675572156906, Test Loss: 0.07139886170625687\n",
      "Epoch 7572, Train Loss: 0.01705373451113701, Test Loss: 0.0714011937379837\n",
      "Epoch 7573, Train Loss: 0.01705044135451317, Test Loss: 0.07140278071165085\n",
      "Epoch 7574, Train Loss: 0.01704716868698597, Test Loss: 0.071390300989151\n",
      "Epoch 7575, Train Loss: 0.017043432220816612, Test Loss: 0.07139193266630173\n",
      "Epoch 7576, Train Loss: 0.0170398261398077, Test Loss: 0.07139507681131363\n",
      "Epoch 7577, Train Loss: 0.017036747187376022, Test Loss: 0.07138462364673615\n",
      "Epoch 7578, Train Loss: 0.01703360490500927, Test Loss: 0.0713905468583107\n",
      "Epoch 7579, Train Loss: 0.0170314759016037, Test Loss: 0.0713806003332138\n",
      "Epoch 7580, Train Loss: 0.01702709123492241, Test Loss: 0.07136818021535873\n",
      "Epoch 7581, Train Loss: 0.017024502158164978, Test Loss: 0.07139319181442261\n",
      "Epoch 7582, Train Loss: 0.017020100727677345, Test Loss: 0.07138039916753769\n",
      "Epoch 7583, Train Loss: 0.017016928642988205, Test Loss: 0.07138471305370331\n",
      "Epoch 7584, Train Loss: 0.01701347902417183, Test Loss: 0.07137969136238098\n",
      "Epoch 7585, Train Loss: 0.017009690403938293, Test Loss: 0.07137788087129593\n",
      "Epoch 7586, Train Loss: 0.017006516456604004, Test Loss: 0.07138440757989883\n",
      "Epoch 7587, Train Loss: 0.017002973705530167, Test Loss: 0.07137931883335114\n",
      "Epoch 7588, Train Loss: 0.01699964702129364, Test Loss: 0.07138066738843918\n",
      "Epoch 7589, Train Loss: 0.016996514052152634, Test Loss: 0.07138103246688843\n",
      "Epoch 7590, Train Loss: 0.01699300855398178, Test Loss: 0.07138989865779877\n",
      "Epoch 7591, Train Loss: 0.01698964834213257, Test Loss: 0.0713704526424408\n",
      "Epoch 7592, Train Loss: 0.016986293718218803, Test Loss: 0.07135576009750366\n",
      "Epoch 7593, Train Loss: 0.01698274351656437, Test Loss: 0.07136427611112595\n",
      "Epoch 7594, Train Loss: 0.016979703679680824, Test Loss: 0.07136949151754379\n",
      "Epoch 7595, Train Loss: 0.016976196318864822, Test Loss: 0.07137271016836166\n",
      "Epoch 7596, Train Loss: 0.016973312944173813, Test Loss: 0.07135885208845139\n",
      "Epoch 7597, Train Loss: 0.016969604417681694, Test Loss: 0.07134735584259033\n",
      "Epoch 7598, Train Loss: 0.016966549679636955, Test Loss: 0.07133376598358154\n",
      "Epoch 7599, Train Loss: 0.016962803900241852, Test Loss: 0.07134079188108444\n",
      "Epoch 7600, Train Loss: 0.016959581524133682, Test Loss: 0.07134127616882324\n",
      "Epoch 7601, Train Loss: 0.016956539824604988, Test Loss: 0.07133975625038147\n",
      "Epoch 7602, Train Loss: 0.016952790319919586, Test Loss: 0.0713457390666008\n",
      "Epoch 7603, Train Loss: 0.016949724406003952, Test Loss: 0.07135344296693802\n",
      "Epoch 7604, Train Loss: 0.016946295276284218, Test Loss: 0.07134059816598892\n",
      "Epoch 7605, Train Loss: 0.016942601650953293, Test Loss: 0.07135190814733505\n",
      "Epoch 7606, Train Loss: 0.01694009080529213, Test Loss: 0.07134810090065002\n",
      "Epoch 7607, Train Loss: 0.01693677343428135, Test Loss: 0.07133407890796661\n",
      "Epoch 7608, Train Loss: 0.016932900995016098, Test Loss: 0.0713309496641159\n",
      "Epoch 7609, Train Loss: 0.01692933402955532, Test Loss: 0.07134250551462173\n",
      "Epoch 7610, Train Loss: 0.01692623645067215, Test Loss: 0.07133264094591141\n",
      "Epoch 7611, Train Loss: 0.016922563314437866, Test Loss: 0.0713309571146965\n",
      "Epoch 7612, Train Loss: 0.016919193789362907, Test Loss: 0.07133237272500992\n",
      "Epoch 7613, Train Loss: 0.01691606640815735, Test Loss: 0.07132171839475632\n",
      "Epoch 7614, Train Loss: 0.016912445425987244, Test Loss: 0.07132759690284729\n",
      "Epoch 7615, Train Loss: 0.016909215599298477, Test Loss: 0.07132096588611603\n",
      "Epoch 7616, Train Loss: 0.0169062539935112, Test Loss: 0.07132349163293839\n",
      "Epoch 7617, Train Loss: 0.01690266653895378, Test Loss: 0.07132378965616226\n",
      "Epoch 7618, Train Loss: 0.016899418085813522, Test Loss: 0.07131824642419815\n",
      "Epoch 7619, Train Loss: 0.016896342858672142, Test Loss: 0.07131338864564896\n",
      "Epoch 7620, Train Loss: 0.016893455758690834, Test Loss: 0.07131689041852951\n",
      "Epoch 7621, Train Loss: 0.016889356076717377, Test Loss: 0.07131505012512207\n",
      "Epoch 7622, Train Loss: 0.016886046156287193, Test Loss: 0.07131790369749069\n",
      "Epoch 7623, Train Loss: 0.016882576048374176, Test Loss: 0.0713203027844429\n",
      "Epoch 7624, Train Loss: 0.016879640519618988, Test Loss: 0.07130735367536545\n",
      "Epoch 7625, Train Loss: 0.016876181587576866, Test Loss: 0.07129968702793121\n",
      "Epoch 7626, Train Loss: 0.016872860491275787, Test Loss: 0.07130538672208786\n",
      "Epoch 7627, Train Loss: 0.01686978153884411, Test Loss: 0.07130385935306549\n",
      "Epoch 7628, Train Loss: 0.016865845769643784, Test Loss: 0.07130664587020874\n",
      "Epoch 7629, Train Loss: 0.016863631084561348, Test Loss: 0.07130838930606842\n",
      "Epoch 7630, Train Loss: 0.01685977168381214, Test Loss: 0.0712934285402298\n",
      "Epoch 7631, Train Loss: 0.01685629040002823, Test Loss: 0.07129465043544769\n",
      "Epoch 7632, Train Loss: 0.016852691769599915, Test Loss: 0.07129571586847305\n",
      "Epoch 7633, Train Loss: 0.016849899664521217, Test Loss: 0.07129540294408798\n",
      "Epoch 7634, Train Loss: 0.01684677042067051, Test Loss: 0.07128550857305527\n",
      "Epoch 7635, Train Loss: 0.016842778772115707, Test Loss: 0.07129999250173569\n",
      "Epoch 7636, Train Loss: 0.016840482130646706, Test Loss: 0.07131720334291458\n",
      "Epoch 7637, Train Loss: 0.01683647185564041, Test Loss: 0.0713106095790863\n",
      "Epoch 7638, Train Loss: 0.016832813620567322, Test Loss: 0.07129990309476852\n",
      "Epoch 7639, Train Loss: 0.01683005690574646, Test Loss: 0.07130737602710724\n",
      "Epoch 7640, Train Loss: 0.016826234757900238, Test Loss: 0.07129622250795364\n",
      "Epoch 7641, Train Loss: 0.01682414673268795, Test Loss: 0.07129397243261337\n",
      "Epoch 7642, Train Loss: 0.016820335760712624, Test Loss: 0.07127320021390915\n",
      "Epoch 7643, Train Loss: 0.01681693270802498, Test Loss: 0.07127174735069275\n",
      "Epoch 7644, Train Loss: 0.016813699156045914, Test Loss: 0.07126649469137192\n",
      "Epoch 7645, Train Loss: 0.016809862107038498, Test Loss: 0.07127048820257187\n",
      "Epoch 7646, Train Loss: 0.016806812956929207, Test Loss: 0.0712665319442749\n",
      "Epoch 7647, Train Loss: 0.016803205013275146, Test Loss: 0.0712742954492569\n",
      "Epoch 7648, Train Loss: 0.016799718141555786, Test Loss: 0.07126497477293015\n",
      "Epoch 7649, Train Loss: 0.016796668991446495, Test Loss: 0.0712590366601944\n",
      "Epoch 7650, Train Loss: 0.016793133690953255, Test Loss: 0.07126159965991974\n",
      "Epoch 7651, Train Loss: 0.01679055206477642, Test Loss: 0.07125288993120193\n",
      "Epoch 7652, Train Loss: 0.016786636784672737, Test Loss: 0.07126463949680328\n",
      "Epoch 7653, Train Loss: 0.016783898696303368, Test Loss: 0.07125363498926163\n",
      "Epoch 7654, Train Loss: 0.0167803056538105, Test Loss: 0.07125917077064514\n",
      "Epoch 7655, Train Loss: 0.016776572912931442, Test Loss: 0.07126361131668091\n",
      "Epoch 7656, Train Loss: 0.016773341223597527, Test Loss: 0.07126207649707794\n",
      "Epoch 7657, Train Loss: 0.016769925132393837, Test Loss: 0.07125736027956009\n",
      "Epoch 7658, Train Loss: 0.01676669716835022, Test Loss: 0.07125021517276764\n",
      "Epoch 7659, Train Loss: 0.016763919964432716, Test Loss: 0.0712471529841423\n",
      "Epoch 7660, Train Loss: 0.016760261729359627, Test Loss: 0.07125850021839142\n",
      "Epoch 7661, Train Loss: 0.016756827011704445, Test Loss: 0.0712471604347229\n",
      "Epoch 7662, Train Loss: 0.016753440722823143, Test Loss: 0.07124882936477661\n",
      "Epoch 7663, Train Loss: 0.016750432550907135, Test Loss: 0.07123701274394989\n",
      "Epoch 7664, Train Loss: 0.016746830195188522, Test Loss: 0.07123434543609619\n",
      "Epoch 7665, Train Loss: 0.016743764281272888, Test Loss: 0.07123468071222305\n",
      "Epoch 7666, Train Loss: 0.016740266233682632, Test Loss: 0.07122991234064102\n",
      "Epoch 7667, Train Loss: 0.0167369581758976, Test Loss: 0.07122848182916641\n",
      "Epoch 7668, Train Loss: 0.016733810305595398, Test Loss: 0.07122186571359634\n",
      "Epoch 7669, Train Loss: 0.016730690374970436, Test Loss: 0.07122869044542313\n",
      "Epoch 7670, Train Loss: 0.016727115958929062, Test Loss: 0.07122495770454407\n",
      "Epoch 7671, Train Loss: 0.0167238749563694, Test Loss: 0.07121822983026505\n",
      "Epoch 7672, Train Loss: 0.016720609739422798, Test Loss: 0.07122122496366501\n",
      "Epoch 7673, Train Loss: 0.016717493534088135, Test Loss: 0.07122447341680527\n",
      "Epoch 7674, Train Loss: 0.016714656725525856, Test Loss: 0.07123153656721115\n",
      "Epoch 7675, Train Loss: 0.01671062596142292, Test Loss: 0.0712265744805336\n",
      "Epoch 7676, Train Loss: 0.016707364469766617, Test Loss: 0.07122064381837845\n",
      "Epoch 7677, Train Loss: 0.01670415885746479, Test Loss: 0.07121742516756058\n",
      "Epoch 7678, Train Loss: 0.016700943931937218, Test Loss: 0.07121691107749939\n",
      "Epoch 7679, Train Loss: 0.01669781468808651, Test Loss: 0.07120802998542786\n",
      "Epoch 7680, Train Loss: 0.016694338992238045, Test Loss: 0.07120402902364731\n",
      "Epoch 7681, Train Loss: 0.016691917553544044, Test Loss: 0.07119853049516678\n",
      "Epoch 7682, Train Loss: 0.016687937080860138, Test Loss: 0.07119562476873398\n",
      "Epoch 7683, Train Loss: 0.016685444861650467, Test Loss: 0.07118962705135345\n",
      "Epoch 7684, Train Loss: 0.016681313514709473, Test Loss: 0.07120320200920105\n",
      "Epoch 7685, Train Loss: 0.01667824760079384, Test Loss: 0.0712026059627533\n",
      "Epoch 7686, Train Loss: 0.01667506992816925, Test Loss: 0.07119685411453247\n",
      "Epoch 7687, Train Loss: 0.016671113669872284, Test Loss: 0.07120461761951447\n",
      "Epoch 7688, Train Loss: 0.016667893156409264, Test Loss: 0.07119756191968918\n",
      "Epoch 7689, Train Loss: 0.01666582189500332, Test Loss: 0.07117986679077148\n",
      "Epoch 7690, Train Loss: 0.016661854460835457, Test Loss: 0.07118919491767883\n",
      "Epoch 7691, Train Loss: 0.016658132895827293, Test Loss: 0.07118892669677734\n",
      "Epoch 7692, Train Loss: 0.016654836013913155, Test Loss: 0.07119658589363098\n",
      "Epoch 7693, Train Loss: 0.016651855781674385, Test Loss: 0.07119106501340866\n",
      "Epoch 7694, Train Loss: 0.016648175194859505, Test Loss: 0.07119717448949814\n",
      "Epoch 7695, Train Loss: 0.016644921153783798, Test Loss: 0.07118891924619675\n",
      "Epoch 7696, Train Loss: 0.016642747446894646, Test Loss: 0.07119045406579971\n",
      "Epoch 7697, Train Loss: 0.016639119014143944, Test Loss: 0.07118327915668488\n",
      "Epoch 7698, Train Loss: 0.016635160893201828, Test Loss: 0.07118473201990128\n",
      "Epoch 7699, Train Loss: 0.016631897538900375, Test Loss: 0.07118560373783112\n",
      "Epoch 7700, Train Loss: 0.016628701239824295, Test Loss: 0.07118018716573715\n",
      "Epoch 7701, Train Loss: 0.01662527211010456, Test Loss: 0.07117773592472076\n",
      "Epoch 7702, Train Loss: 0.016622208058834076, Test Loss: 0.07116932421922684\n",
      "Epoch 7703, Train Loss: 0.01661856845021248, Test Loss: 0.07117479294538498\n",
      "Epoch 7704, Train Loss: 0.01661570370197296, Test Loss: 0.07116387039422989\n",
      "Epoch 7705, Train Loss: 0.01661256141960621, Test Loss: 0.07116879522800446\n",
      "Epoch 7706, Train Loss: 0.016608979552984238, Test Loss: 0.0711713582277298\n",
      "Epoch 7707, Train Loss: 0.0166054405272007, Test Loss: 0.07117290049791336\n",
      "Epoch 7708, Train Loss: 0.016603287309408188, Test Loss: 0.07115114480257034\n",
      "Epoch 7709, Train Loss: 0.016598908230662346, Test Loss: 0.07116081565618515\n",
      "Epoch 7710, Train Loss: 0.016595715656876564, Test Loss: 0.07115933299064636\n",
      "Epoch 7711, Train Loss: 0.01659265160560608, Test Loss: 0.07115590572357178\n",
      "Epoch 7712, Train Loss: 0.01658942550420761, Test Loss: 0.07115522027015686\n",
      "Epoch 7713, Train Loss: 0.016586126759648323, Test Loss: 0.07116179913282394\n",
      "Epoch 7714, Train Loss: 0.016582688316702843, Test Loss: 0.0711546242237091\n",
      "Epoch 7715, Train Loss: 0.016580278053879738, Test Loss: 0.0711478590965271\n",
      "Epoch 7716, Train Loss: 0.016576390713453293, Test Loss: 0.07114151120185852\n",
      "Epoch 7717, Train Loss: 0.01657281257212162, Test Loss: 0.07115452736616135\n",
      "Epoch 7718, Train Loss: 0.016569752246141434, Test Loss: 0.07113834470510483\n",
      "Epoch 7719, Train Loss: 0.016566352918744087, Test Loss: 0.07115340232849121\n",
      "Epoch 7720, Train Loss: 0.016563663259148598, Test Loss: 0.07115227729082108\n",
      "Epoch 7721, Train Loss: 0.016560228541493416, Test Loss: 0.07113740593194962\n",
      "Epoch 7722, Train Loss: 0.01655738614499569, Test Loss: 0.07113710790872574\n",
      "Epoch 7723, Train Loss: 0.01655331440269947, Test Loss: 0.07113970816135406\n",
      "Epoch 7724, Train Loss: 0.016550103202462196, Test Loss: 0.07113100588321686\n",
      "Epoch 7725, Train Loss: 0.016546791419386864, Test Loss: 0.0711340606212616\n",
      "Epoch 7726, Train Loss: 0.016543623059988022, Test Loss: 0.07113923132419586\n",
      "Epoch 7727, Train Loss: 0.01654060184955597, Test Loss: 0.07113298028707504\n",
      "Epoch 7728, Train Loss: 0.016537291929125786, Test Loss: 0.07112742960453033\n",
      "Epoch 7729, Train Loss: 0.016534246504306793, Test Loss: 0.07111485302448273\n",
      "Epoch 7730, Train Loss: 0.016531003639101982, Test Loss: 0.07110907137393951\n",
      "Epoch 7731, Train Loss: 0.016528289765119553, Test Loss: 0.07112281769514084\n",
      "Epoch 7732, Train Loss: 0.01652415469288826, Test Loss: 0.07112808525562286\n",
      "Epoch 7733, Train Loss: 0.016520950943231583, Test Loss: 0.07111935317516327\n",
      "Epoch 7734, Train Loss: 0.01651759445667267, Test Loss: 0.0711207464337349\n",
      "Epoch 7735, Train Loss: 0.016514336690306664, Test Loss: 0.07110598683357239\n",
      "Epoch 7736, Train Loss: 0.01651192642748356, Test Loss: 0.07110988348722458\n",
      "Epoch 7737, Train Loss: 0.016507836058735847, Test Loss: 0.07110528647899628\n",
      "Epoch 7738, Train Loss: 0.016504796221852303, Test Loss: 0.07110267132520676\n",
      "Epoch 7739, Train Loss: 0.016502011567354202, Test Loss: 0.07109829783439636\n",
      "Epoch 7740, Train Loss: 0.016498364508152008, Test Loss: 0.07109955698251724\n",
      "Epoch 7741, Train Loss: 0.01649458520114422, Test Loss: 0.07110574841499329\n",
      "Epoch 7742, Train Loss: 0.016491426154971123, Test Loss: 0.071104995906353\n",
      "Epoch 7743, Train Loss: 0.016488609835505486, Test Loss: 0.07109353691339493\n",
      "Epoch 7744, Train Loss: 0.01648489385843277, Test Loss: 0.07110100984573364\n",
      "Epoch 7745, Train Loss: 0.016481731086969376, Test Loss: 0.07111237198114395\n",
      "Epoch 7746, Train Loss: 0.016478441655635834, Test Loss: 0.07110250741243362\n",
      "Epoch 7747, Train Loss: 0.016476107761263847, Test Loss: 0.07108410447835922\n",
      "Epoch 7748, Train Loss: 0.01647205650806427, Test Loss: 0.07108750194311142\n",
      "Epoch 7749, Train Loss: 0.01646849513053894, Test Loss: 0.0710955485701561\n",
      "Epoch 7750, Train Loss: 0.016465533524751663, Test Loss: 0.07108619809150696\n",
      "Epoch 7751, Train Loss: 0.01646186225116253, Test Loss: 0.07109745591878891\n",
      "Epoch 7752, Train Loss: 0.016458816826343536, Test Loss: 0.07109402865171432\n",
      "Epoch 7753, Train Loss: 0.016455749049782753, Test Loss: 0.07110264897346497\n",
      "Epoch 7754, Train Loss: 0.016452424228191376, Test Loss: 0.07108798623085022\n",
      "Epoch 7755, Train Loss: 0.016449524089694023, Test Loss: 0.0710878074169159\n",
      "Epoch 7756, Train Loss: 0.016445985063910484, Test Loss: 0.07108085602521896\n",
      "Epoch 7757, Train Loss: 0.01644282601773739, Test Loss: 0.07107039541006088\n",
      "Epoch 7758, Train Loss: 0.016439253464341164, Test Loss: 0.07107026875019073\n",
      "Epoch 7759, Train Loss: 0.016435936093330383, Test Loss: 0.07107419520616531\n",
      "Epoch 7760, Train Loss: 0.01643262803554535, Test Loss: 0.07107090950012207\n",
      "Epoch 7761, Train Loss: 0.016429398208856583, Test Loss: 0.07107661664485931\n",
      "Epoch 7762, Train Loss: 0.016427168622612953, Test Loss: 0.0710597038269043\n",
      "Epoch 7763, Train Loss: 0.016423705965280533, Test Loss: 0.07106159627437592\n",
      "Epoch 7764, Train Loss: 0.01641971804201603, Test Loss: 0.07107250392436981\n",
      "Epoch 7765, Train Loss: 0.016416478902101517, Test Loss: 0.07106707245111465\n",
      "Epoch 7766, Train Loss: 0.016413094475865364, Test Loss: 0.07106303423643112\n",
      "Epoch 7767, Train Loss: 0.01640988700091839, Test Loss: 0.07106820493936539\n",
      "Epoch 7768, Train Loss: 0.016406727954745293, Test Loss: 0.07106292992830276\n",
      "Epoch 7769, Train Loss: 0.016403736546635628, Test Loss: 0.07105350494384766\n",
      "Epoch 7770, Train Loss: 0.016400542110204697, Test Loss: 0.07104582339525223\n",
      "Epoch 7771, Train Loss: 0.016397632658481598, Test Loss: 0.07104427367448807\n",
      "Epoch 7772, Train Loss: 0.016393674537539482, Test Loss: 0.07105377316474915\n",
      "Epoch 7773, Train Loss: 0.016390664502978325, Test Loss: 0.07105480879545212\n",
      "Epoch 7774, Train Loss: 0.01638716086745262, Test Loss: 0.07104625552892685\n",
      "Epoch 7775, Train Loss: 0.016384044662117958, Test Loss: 0.07104077190160751\n",
      "Epoch 7776, Train Loss: 0.016381287947297096, Test Loss: 0.07103576511144638\n",
      "Epoch 7777, Train Loss: 0.016378307715058327, Test Loss: 0.0710357278585434\n",
      "Epoch 7778, Train Loss: 0.016374336555600166, Test Loss: 0.07105004042387009\n",
      "Epoch 7779, Train Loss: 0.016370931640267372, Test Loss: 0.07103727757930756\n",
      "Epoch 7780, Train Loss: 0.016368268057703972, Test Loss: 0.07104233652353287\n",
      "Epoch 7781, Train Loss: 0.01636447012424469, Test Loss: 0.0710369274020195\n",
      "Epoch 7782, Train Loss: 0.016361653804779053, Test Loss: 0.0710325613617897\n",
      "Epoch 7783, Train Loss: 0.016358232125639915, Test Loss: 0.07102929800748825\n",
      "Epoch 7784, Train Loss: 0.01635521464049816, Test Loss: 0.07102397829294205\n",
      "Epoch 7785, Train Loss: 0.01635248027741909, Test Loss: 0.07101514935493469\n",
      "Epoch 7786, Train Loss: 0.01634852960705757, Test Loss: 0.07102493941783905\n",
      "Epoch 7787, Train Loss: 0.016345283016562462, Test Loss: 0.0710161030292511\n",
      "Epoch 7788, Train Loss: 0.0163420420140028, Test Loss: 0.07102184742689133\n",
      "Epoch 7789, Train Loss: 0.016338719055056572, Test Loss: 0.07101760804653168\n",
      "Epoch 7790, Train Loss: 0.016336040571331978, Test Loss: 0.07101454585790634\n",
      "Epoch 7791, Train Loss: 0.016332268714904785, Test Loss: 0.0710168108344078\n",
      "Epoch 7792, Train Loss: 0.01632966287434101, Test Loss: 0.071022629737854\n",
      "Epoch 7793, Train Loss: 0.016325630247592926, Test Loss: 0.07102495431900024\n",
      "Epoch 7794, Train Loss: 0.01632290706038475, Test Loss: 0.07100831717252731\n",
      "Epoch 7795, Train Loss: 0.016319457441568375, Test Loss: 0.07100697606801987\n",
      "Epoch 7796, Train Loss: 0.016316555440425873, Test Loss: 0.07100628316402435\n",
      "Epoch 7797, Train Loss: 0.016312645748257637, Test Loss: 0.0710141658782959\n",
      "Epoch 7798, Train Loss: 0.01630978472530842, Test Loss: 0.07102053612470627\n",
      "Epoch 7799, Train Loss: 0.016306504607200623, Test Loss: 0.07100730389356613\n",
      "Epoch 7800, Train Loss: 0.01630333811044693, Test Loss: 0.0709940493106842\n",
      "Epoch 7801, Train Loss: 0.016300451010465622, Test Loss: 0.07099474966526031\n",
      "Epoch 7802, Train Loss: 0.016296837478876114, Test Loss: 0.07100535929203033\n",
      "Epoch 7803, Train Loss: 0.016293765977025032, Test Loss: 0.07099327445030212\n",
      "Epoch 7804, Train Loss: 0.016290077939629555, Test Loss: 0.07099234312772751\n",
      "Epoch 7805, Train Loss: 0.016287097707390785, Test Loss: 0.070989690721035\n",
      "Epoch 7806, Train Loss: 0.016283927485346794, Test Loss: 0.07099152356386185\n",
      "Epoch 7807, Train Loss: 0.016280608251690865, Test Loss: 0.07097970694303513\n",
      "Epoch 7808, Train Loss: 0.016277378425002098, Test Loss: 0.07098828256130219\n",
      "Epoch 7809, Train Loss: 0.016274020075798035, Test Loss: 0.07099275290966034\n",
      "Epoch 7810, Train Loss: 0.01627093181014061, Test Loss: 0.07099203765392303\n",
      "Epoch 7811, Train Loss: 0.016267526894807816, Test Loss: 0.07098895311355591\n",
      "Epoch 7812, Train Loss: 0.016264313831925392, Test Loss: 0.07098987698554993\n",
      "Epoch 7813, Train Loss: 0.01626112498342991, Test Loss: 0.07098166644573212\n",
      "Epoch 7814, Train Loss: 0.016257790848612785, Test Loss: 0.07097826153039932\n",
      "Epoch 7815, Train Loss: 0.016255278140306473, Test Loss: 0.07096803933382034\n",
      "Epoch 7816, Train Loss: 0.01625136286020279, Test Loss: 0.07097595185041428\n",
      "Epoch 7817, Train Loss: 0.016248047351837158, Test Loss: 0.07097449153661728\n",
      "Epoch 7818, Train Loss: 0.016245685517787933, Test Loss: 0.07095741480588913\n",
      "Epoch 7819, Train Loss: 0.01624172367155552, Test Loss: 0.07096592336893082\n",
      "Epoch 7820, Train Loss: 0.016238853335380554, Test Loss: 0.07095795124769211\n",
      "Epoch 7821, Train Loss: 0.016235720366239548, Test Loss: 0.07095333933830261\n",
      "Epoch 7822, Train Loss: 0.016232529655098915, Test Loss: 0.07095135003328323\n",
      "Epoch 7823, Train Loss: 0.01622895896434784, Test Loss: 0.0709613561630249\n",
      "Epoch 7824, Train Loss: 0.01622575893998146, Test Loss: 0.0709613710641861\n",
      "Epoch 7825, Train Loss: 0.016222458332777023, Test Loss: 0.07096267491579056\n",
      "Epoch 7826, Train Loss: 0.016219936311244965, Test Loss: 0.07094957679510117\n",
      "Epoch 7827, Train Loss: 0.016216237097978592, Test Loss: 0.0709599107503891\n",
      "Epoch 7828, Train Loss: 0.016213415190577507, Test Loss: 0.07095865905284882\n",
      "Epoch 7829, Train Loss: 0.01621001772582531, Test Loss: 0.07095018029212952\n",
      "Epoch 7830, Train Loss: 0.01620679907500744, Test Loss: 0.07094608247280121\n",
      "Epoch 7831, Train Loss: 0.016203956678509712, Test Loss: 0.0709528923034668\n",
      "Epoch 7832, Train Loss: 0.01620052568614483, Test Loss: 0.07094330340623856\n",
      "Epoch 7833, Train Loss: 0.01619804836809635, Test Loss: 0.07094097137451172\n",
      "Epoch 7834, Train Loss: 0.01619368977844715, Test Loss: 0.07094161957502365\n",
      "Epoch 7835, Train Loss: 0.01619056984782219, Test Loss: 0.0709523856639862\n",
      "Epoch 7836, Train Loss: 0.01618749275803566, Test Loss: 0.07094036042690277\n",
      "Epoch 7837, Train Loss: 0.016184166073799133, Test Loss: 0.07094378024339676\n",
      "Epoch 7838, Train Loss: 0.01618076302111149, Test Loss: 0.07093990594148636\n",
      "Epoch 7839, Train Loss: 0.016177630051970482, Test Loss: 0.07093574106693268\n",
      "Epoch 7840, Train Loss: 0.016174763441085815, Test Loss: 0.07093625515699387\n",
      "Epoch 7841, Train Loss: 0.016172081232070923, Test Loss: 0.07092328369617462\n",
      "Epoch 7842, Train Loss: 0.016168637201189995, Test Loss: 0.07092767208814621\n",
      "Epoch 7843, Train Loss: 0.016164768487215042, Test Loss: 0.07092466205358505\n",
      "Epoch 7844, Train Loss: 0.016161374747753143, Test Loss: 0.0709293782711029\n",
      "Epoch 7845, Train Loss: 0.016158845275640488, Test Loss: 0.07092627137899399\n",
      "Epoch 7846, Train Loss: 0.016155127435922623, Test Loss: 0.07093026489019394\n",
      "Epoch 7847, Train Loss: 0.0161531213670969, Test Loss: 0.07092007994651794\n",
      "Epoch 7848, Train Loss: 0.016148941591382027, Test Loss: 0.07092069089412689\n",
      "Epoch 7849, Train Loss: 0.016145434230566025, Test Loss: 0.07091636955738068\n",
      "Epoch 7850, Train Loss: 0.016142357140779495, Test Loss: 0.0709042027592659\n",
      "Epoch 7851, Train Loss: 0.016138965263962746, Test Loss: 0.07091394066810608\n",
      "Epoch 7852, Train Loss: 0.016135849058628082, Test Loss: 0.0709058865904808\n",
      "Epoch 7853, Train Loss: 0.01613301783800125, Test Loss: 0.0709083303809166\n",
      "Epoch 7854, Train Loss: 0.016129929572343826, Test Loss: 0.07090545445680618\n",
      "Epoch 7855, Train Loss: 0.01612650416791439, Test Loss: 0.07090973109006882\n",
      "Epoch 7856, Train Loss: 0.01612386666238308, Test Loss: 0.07091313600540161\n",
      "Epoch 7857, Train Loss: 0.016120651736855507, Test Loss: 0.07092002034187317\n",
      "Epoch 7858, Train Loss: 0.016116440296173096, Test Loss: 0.07090412080287933\n",
      "Epoch 7859, Train Loss: 0.016113614663481712, Test Loss: 0.07090369611978531\n",
      "Epoch 7860, Train Loss: 0.01611027866601944, Test Loss: 0.07090380042791367\n",
      "Epoch 7861, Train Loss: 0.01610703580081463, Test Loss: 0.0709027424454689\n",
      "Epoch 7862, Train Loss: 0.016103969886898994, Test Loss: 0.07090280205011368\n",
      "Epoch 7863, Train Loss: 0.016101162880659103, Test Loss: 0.07089114189147949\n",
      "Epoch 7864, Train Loss: 0.016097497195005417, Test Loss: 0.07089411467313766\n",
      "Epoch 7865, Train Loss: 0.016094403341412544, Test Loss: 0.07089085131883621\n",
      "Epoch 7866, Train Loss: 0.016091138124465942, Test Loss: 0.07088210433721542\n",
      "Epoch 7867, Train Loss: 0.016087817028164864, Test Loss: 0.07088129222393036\n",
      "Epoch 7868, Train Loss: 0.01608477719128132, Test Loss: 0.07088130712509155\n",
      "Epoch 7869, Train Loss: 0.016081631183624268, Test Loss: 0.07087994366884232\n",
      "Epoch 7870, Train Loss: 0.016078174114227295, Test Loss: 0.0708807110786438\n",
      "Epoch 7871, Train Loss: 0.01607522740960121, Test Loss: 0.07088086009025574\n",
      "Epoch 7872, Train Loss: 0.016072126105427742, Test Loss: 0.0708845928311348\n",
      "Epoch 7873, Train Loss: 0.01606893166899681, Test Loss: 0.07087917625904083\n",
      "Epoch 7874, Train Loss: 0.01606599986553192, Test Loss: 0.07087482511997223\n",
      "Epoch 7875, Train Loss: 0.016063066199421883, Test Loss: 0.07088528573513031\n",
      "Epoch 7876, Train Loss: 0.01605924218893051, Test Loss: 0.07087822258472443\n",
      "Epoch 7877, Train Loss: 0.016055919229984283, Test Loss: 0.07087460905313492\n",
      "Epoch 7878, Train Loss: 0.016052762046456337, Test Loss: 0.07087439298629761\n",
      "Epoch 7879, Train Loss: 0.01604977250099182, Test Loss: 0.0708751380443573\n",
      "Epoch 7880, Train Loss: 0.016046617180109024, Test Loss: 0.07087724655866623\n",
      "Epoch 7881, Train Loss: 0.016043435782194138, Test Loss: 0.0708625391125679\n",
      "Epoch 7882, Train Loss: 0.01604052074253559, Test Loss: 0.07086177170276642\n",
      "Epoch 7883, Train Loss: 0.016036968678236008, Test Loss: 0.0708550438284874\n",
      "Epoch 7884, Train Loss: 0.01603413186967373, Test Loss: 0.0708579495549202\n",
      "Epoch 7885, Train Loss: 0.01603076606988907, Test Loss: 0.07085797935724258\n",
      "Epoch 7886, Train Loss: 0.016027409583330154, Test Loss: 0.07086142152547836\n",
      "Epoch 7887, Train Loss: 0.01602410152554512, Test Loss: 0.0708586797118187\n",
      "Epoch 7888, Train Loss: 0.0160216074436903, Test Loss: 0.07084584981203079\n",
      "Epoch 7889, Train Loss: 0.016018223017454147, Test Loss: 0.07083864510059357\n",
      "Epoch 7890, Train Loss: 0.016014516353607178, Test Loss: 0.07084236294031143\n",
      "Epoch 7891, Train Loss: 0.01601153053343296, Test Loss: 0.07084053009748459\n",
      "Epoch 7892, Train Loss: 0.01600899174809456, Test Loss: 0.0708387941122055\n",
      "Epoch 7893, Train Loss: 0.016005633398890495, Test Loss: 0.0708375796675682\n",
      "Epoch 7894, Train Loss: 0.01600256748497486, Test Loss: 0.07083497941493988\n",
      "Epoch 7895, Train Loss: 0.01599873974919319, Test Loss: 0.07083724439144135\n",
      "Epoch 7896, Train Loss: 0.015995653346180916, Test Loss: 0.0708329975605011\n",
      "Epoch 7897, Train Loss: 0.015993021428585052, Test Loss: 0.07082869857549667\n",
      "Epoch 7898, Train Loss: 0.01598934270441532, Test Loss: 0.07082361727952957\n",
      "Epoch 7899, Train Loss: 0.01598643884062767, Test Loss: 0.07082918286323547\n",
      "Epoch 7900, Train Loss: 0.015983086079359055, Test Loss: 0.07083545625209808\n",
      "Epoch 7901, Train Loss: 0.01597975380718708, Test Loss: 0.0708296075463295\n",
      "Epoch 7902, Train Loss: 0.015976563096046448, Test Loss: 0.07082805037498474\n",
      "Epoch 7903, Train Loss: 0.015973519533872604, Test Loss: 0.07081817835569382\n",
      "Epoch 7904, Train Loss: 0.015970420092344284, Test Loss: 0.07082132250070572\n",
      "Epoch 7905, Train Loss: 0.01596769317984581, Test Loss: 0.07083788514137268\n",
      "Epoch 7906, Train Loss: 0.015964224934577942, Test Loss: 0.07083262503147125\n",
      "Epoch 7907, Train Loss: 0.01596096158027649, Test Loss: 0.0708266943693161\n",
      "Epoch 7908, Train Loss: 0.015957875177264214, Test Loss: 0.07081397622823715\n",
      "Epoch 7909, Train Loss: 0.015954704955220222, Test Loss: 0.07081615179777145\n",
      "Epoch 7910, Train Loss: 0.015951961278915405, Test Loss: 0.07080681622028351\n",
      "Epoch 7911, Train Loss: 0.015948286280035973, Test Loss: 0.07080983370542526\n",
      "Epoch 7912, Train Loss: 0.015945281833410263, Test Loss: 0.07081431895494461\n",
      "Epoch 7913, Train Loss: 0.015941837802529335, Test Loss: 0.07080835849046707\n",
      "Epoch 7914, Train Loss: 0.015938622877001762, Test Loss: 0.07080096006393433\n",
      "Epoch 7915, Train Loss: 0.01593555510044098, Test Loss: 0.0708087757229805\n",
      "Epoch 7916, Train Loss: 0.015932701528072357, Test Loss: 0.07080189883708954\n",
      "Epoch 7917, Train Loss: 0.015930892899632454, Test Loss: 0.07079320400953293\n",
      "Epoch 7918, Train Loss: 0.015926122665405273, Test Loss: 0.0708116963505745\n",
      "Epoch 7919, Train Loss: 0.01592315174639225, Test Loss: 0.07081010937690735\n",
      "Epoch 7920, Train Loss: 0.015919717028737068, Test Loss: 0.07079781591892242\n",
      "Epoch 7921, Train Loss: 0.01591668091714382, Test Loss: 0.07079043239355087\n",
      "Epoch 7922, Train Loss: 0.01591387949883938, Test Loss: 0.07077927887439728\n",
      "Epoch 7923, Train Loss: 0.01591016910970211, Test Loss: 0.07077468931674957\n",
      "Epoch 7924, Train Loss: 0.01590788923203945, Test Loss: 0.07077871263027191\n",
      "Epoch 7925, Train Loss: 0.01590416207909584, Test Loss: 0.07078295201063156\n",
      "Epoch 7926, Train Loss: 0.015901552513241768, Test Loss: 0.07077988237142563\n",
      "Epoch 7927, Train Loss: 0.015898043289780617, Test Loss: 0.07077621668577194\n",
      "Epoch 7928, Train Loss: 0.015895182266831398, Test Loss: 0.07076379656791687\n",
      "Epoch 7929, Train Loss: 0.015891628339886665, Test Loss: 0.07076629251241684\n",
      "Epoch 7930, Train Loss: 0.015888303518295288, Test Loss: 0.07077576220035553\n",
      "Epoch 7931, Train Loss: 0.01588495448231697, Test Loss: 0.07077154517173767\n",
      "Epoch 7932, Train Loss: 0.015881888568401337, Test Loss: 0.07077167928218842\n",
      "Epoch 7933, Train Loss: 0.015879565849900246, Test Loss: 0.070767842233181\n",
      "Epoch 7934, Train Loss: 0.015875760465860367, Test Loss: 0.07077690958976746\n",
      "Epoch 7935, Train Loss: 0.015872696414589882, Test Loss: 0.07075976580381393\n",
      "Epoch 7936, Train Loss: 0.01586945727467537, Test Loss: 0.07076390087604523\n",
      "Epoch 7937, Train Loss: 0.015866389498114586, Test Loss: 0.07077214121818542\n",
      "Epoch 7938, Train Loss: 0.01586303673684597, Test Loss: 0.07075697183609009\n",
      "Epoch 7939, Train Loss: 0.01586057059466839, Test Loss: 0.07075456529855728\n",
      "Epoch 7940, Train Loss: 0.015856722369790077, Test Loss: 0.0707489624619484\n",
      "Epoch 7941, Train Loss: 0.01585467904806137, Test Loss: 0.07075256109237671\n",
      "Epoch 7942, Train Loss: 0.015850434079766273, Test Loss: 0.0707482323050499\n",
      "Epoch 7943, Train Loss: 0.01584741659462452, Test Loss: 0.07074335217475891\n",
      "Epoch 7944, Train Loss: 0.01584443636238575, Test Loss: 0.07075231522321701\n",
      "Epoch 7945, Train Loss: 0.01584126614034176, Test Loss: 0.07074540853500366\n",
      "Epoch 7946, Train Loss: 0.015837866812944412, Test Loss: 0.07074630260467529\n",
      "Epoch 7947, Train Loss: 0.015835575759410858, Test Loss: 0.07072803378105164\n",
      "Epoch 7948, Train Loss: 0.015832146629691124, Test Loss: 0.0707247331738472\n",
      "Epoch 7949, Train Loss: 0.015828654170036316, Test Loss: 0.07072848826646805\n",
      "Epoch 7950, Train Loss: 0.015826307237148285, Test Loss: 0.07071609795093536\n",
      "Epoch 7951, Train Loss: 0.015822965651750565, Test Loss: 0.07073140144348145\n",
      "Epoch 7952, Train Loss: 0.01581963151693344, Test Loss: 0.07073565572500229\n",
      "Epoch 7953, Train Loss: 0.0158158540725708, Test Loss: 0.07073283195495605\n",
      "Epoch 7954, Train Loss: 0.01581272855401039, Test Loss: 0.07072819769382477\n",
      "Epoch 7955, Train Loss: 0.01581077091395855, Test Loss: 0.07074258476495743\n",
      "Epoch 7956, Train Loss: 0.0158066526055336, Test Loss: 0.07073335349559784\n",
      "Epoch 7957, Train Loss: 0.015804016962647438, Test Loss: 0.07072247564792633\n",
      "Epoch 7958, Train Loss: 0.01580098085105419, Test Loss: 0.0707232877612114\n",
      "Epoch 7959, Train Loss: 0.015797028318047523, Test Loss: 0.07071801275014877\n",
      "Epoch 7960, Train Loss: 0.015794111415743828, Test Loss: 0.07071014493703842\n",
      "Epoch 7961, Train Loss: 0.015791339799761772, Test Loss: 0.07072238624095917\n",
      "Epoch 7962, Train Loss: 0.015787703916430473, Test Loss: 0.07071397453546524\n",
      "Epoch 7963, Train Loss: 0.015784548595547676, Test Loss: 0.07071669399738312\n",
      "Epoch 7964, Train Loss: 0.015781331807374954, Test Loss: 0.07071879506111145\n",
      "Epoch 7965, Train Loss: 0.015778595581650734, Test Loss: 0.07072148472070694\n",
      "Epoch 7966, Train Loss: 0.015775278210639954, Test Loss: 0.07071586698293686\n",
      "Epoch 7967, Train Loss: 0.015771908685564995, Test Loss: 0.0707068219780922\n",
      "Epoch 7968, Train Loss: 0.015768922865390778, Test Loss: 0.07070878893136978\n",
      "Epoch 7969, Train Loss: 0.01576581783592701, Test Loss: 0.07070637494325638\n",
      "Epoch 7970, Train Loss: 0.01576298661530018, Test Loss: 0.07070600241422653\n",
      "Epoch 7971, Train Loss: 0.015759799629449844, Test Loss: 0.07069658488035202\n",
      "Epoch 7972, Train Loss: 0.01575665920972824, Test Loss: 0.07069064676761627\n",
      "Epoch 7973, Train Loss: 0.015753304585814476, Test Loss: 0.07068826258182526\n",
      "Epoch 7974, Train Loss: 0.015750866383314133, Test Loss: 0.07068230211734772\n",
      "Epoch 7975, Train Loss: 0.015747053548693657, Test Loss: 0.07069028168916702\n",
      "Epoch 7976, Train Loss: 0.015743961557745934, Test Loss: 0.07069078087806702\n",
      "Epoch 7977, Train Loss: 0.015741216018795967, Test Loss: 0.07068847119808197\n",
      "Epoch 7978, Train Loss: 0.015737637877464294, Test Loss: 0.07068736106157303\n",
      "Epoch 7979, Train Loss: 0.01573452726006508, Test Loss: 0.07069087028503418\n",
      "Epoch 7980, Train Loss: 0.015731535851955414, Test Loss: 0.07068301737308502\n",
      "Epoch 7981, Train Loss: 0.01572842337191105, Test Loss: 0.07067454606294632\n",
      "Epoch 7982, Train Loss: 0.015725836157798767, Test Loss: 0.07068316638469696\n",
      "Epoch 7983, Train Loss: 0.015722010284662247, Test Loss: 0.07068035006523132\n",
      "Epoch 7984, Train Loss: 0.01571980118751526, Test Loss: 0.07067592442035675\n",
      "Epoch 7985, Train Loss: 0.015715662389993668, Test Loss: 0.07067326456308365\n",
      "Epoch 7986, Train Loss: 0.015712818130850792, Test Loss: 0.07067117840051651\n",
      "Epoch 7987, Train Loss: 0.015709668397903442, Test Loss: 0.07067163288593292\n",
      "Epoch 7988, Train Loss: 0.015706395730376244, Test Loss: 0.07067357003688812\n",
      "Epoch 7989, Train Loss: 0.01570335030555725, Test Loss: 0.07067578285932541\n",
      "Epoch 7990, Train Loss: 0.015700669959187508, Test Loss: 0.07067140191793442\n",
      "Epoch 7991, Train Loss: 0.015697484835982323, Test Loss: 0.07066436111927032\n",
      "Epoch 7992, Train Loss: 0.015694092959165573, Test Loss: 0.0706615149974823\n",
      "Epoch 7993, Train Loss: 0.01569082774221897, Test Loss: 0.07066522538661957\n",
      "Epoch 7994, Train Loss: 0.015688199549913406, Test Loss: 0.07065705209970474\n",
      "Epoch 7995, Train Loss: 0.015685291960835457, Test Loss: 0.07067231833934784\n",
      "Epoch 7996, Train Loss: 0.015681443735957146, Test Loss: 0.0706678181886673\n",
      "Epoch 7997, Train Loss: 0.01567842625081539, Test Loss: 0.0706537738442421\n",
      "Epoch 7998, Train Loss: 0.015675300732254982, Test Loss: 0.07065664976835251\n",
      "Epoch 7999, Train Loss: 0.015672050416469574, Test Loss: 0.07064799219369888\n",
      "Epoch 8000, Train Loss: 0.015668995678424835, Test Loss: 0.07064498960971832\n",
      "Epoch 8001, Train Loss: 0.015665816143155098, Test Loss: 0.07063773274421692\n",
      "Epoch 8002, Train Loss: 0.01566270738840103, Test Loss: 0.07063662260770798\n",
      "Epoch 8003, Train Loss: 0.015659639611840248, Test Loss: 0.07063840329647064\n",
      "Epoch 8004, Train Loss: 0.015656568109989166, Test Loss: 0.07064060866832733\n",
      "Epoch 8005, Train Loss: 0.01565348356962204, Test Loss: 0.07063660025596619\n",
      "Epoch 8006, Train Loss: 0.015650542452931404, Test Loss: 0.07062631845474243\n",
      "Epoch 8007, Train Loss: 0.015647783875465393, Test Loss: 0.07062646746635437\n",
      "Epoch 8008, Train Loss: 0.015644146129488945, Test Loss: 0.07062724232673645\n",
      "Epoch 8009, Train Loss: 0.01564098708331585, Test Loss: 0.07062651962041855\n",
      "Epoch 8010, Train Loss: 0.015637651085853577, Test Loss: 0.0706295296549797\n",
      "Epoch 8011, Train Loss: 0.015634853392839432, Test Loss: 0.07063603401184082\n",
      "Epoch 8012, Train Loss: 0.015631478279829025, Test Loss: 0.070622518658638\n",
      "Epoch 8013, Train Loss: 0.01562894694507122, Test Loss: 0.07060869038105011\n",
      "Epoch 8014, Train Loss: 0.01562533527612686, Test Loss: 0.07060812413692474\n",
      "Epoch 8015, Train Loss: 0.015622596256434917, Test Loss: 0.07060569524765015\n",
      "Epoch 8016, Train Loss: 0.015619919635355473, Test Loss: 0.0706050843000412\n",
      "Epoch 8017, Train Loss: 0.015616429969668388, Test Loss: 0.07060785591602325\n",
      "Epoch 8018, Train Loss: 0.01561292540282011, Test Loss: 0.0706123411655426\n",
      "Epoch 8019, Train Loss: 0.01561016496270895, Test Loss: 0.0706108957529068\n",
      "Epoch 8020, Train Loss: 0.015606744214892387, Test Loss: 0.07061444967985153\n",
      "Epoch 8021, Train Loss: 0.015603548847138882, Test Loss: 0.0706145316362381\n",
      "Epoch 8022, Train Loss: 0.015600664541125298, Test Loss: 0.07060188800096512\n",
      "Epoch 8023, Train Loss: 0.015598099678754807, Test Loss: 0.07058648765087128\n",
      "Epoch 8024, Train Loss: 0.015594486147165298, Test Loss: 0.07059139758348465\n",
      "Epoch 8025, Train Loss: 0.01559198834002018, Test Loss: 0.07059095799922943\n",
      "Epoch 8026, Train Loss: 0.015588296577334404, Test Loss: 0.07059352099895477\n",
      "Epoch 8027, Train Loss: 0.015585196204483509, Test Loss: 0.07058389484882355\n",
      "Epoch 8028, Train Loss: 0.015582245774567127, Test Loss: 0.07058041542768478\n",
      "Epoch 8029, Train Loss: 0.01557888276875019, Test Loss: 0.07058647274971008\n",
      "Epoch 8030, Train Loss: 0.015576125122606754, Test Loss: 0.07058192044496536\n",
      "Epoch 8031, Train Loss: 0.0155728654935956, Test Loss: 0.07059017568826675\n",
      "Epoch 8032, Train Loss: 0.015570083633065224, Test Loss: 0.07059255242347717\n",
      "Epoch 8033, Train Loss: 0.01556637603789568, Test Loss: 0.0705859586596489\n",
      "Epoch 8034, Train Loss: 0.015564416535198689, Test Loss: 0.07059485465288162\n",
      "Epoch 8035, Train Loss: 0.01556070614606142, Test Loss: 0.07058053463697433\n",
      "Epoch 8036, Train Loss: 0.015557046979665756, Test Loss: 0.07058093696832657\n",
      "Epoch 8037, Train Loss: 0.01555484440177679, Test Loss: 0.07056649029254913\n",
      "Epoch 8038, Train Loss: 0.015551142394542694, Test Loss: 0.07056508213281631\n",
      "Epoch 8039, Train Loss: 0.01554802618920803, Test Loss: 0.07057179510593414\n",
      "Epoch 8040, Train Loss: 0.015545104630291462, Test Loss: 0.07056330889463425\n",
      "Epoch 8041, Train Loss: 0.015542023815214634, Test Loss: 0.07055794447660446\n",
      "Epoch 8042, Train Loss: 0.015538953244686127, Test Loss: 0.0705571398139\n",
      "Epoch 8043, Train Loss: 0.015535814687609673, Test Loss: 0.07055992633104324\n",
      "Epoch 8044, Train Loss: 0.015532519668340683, Test Loss: 0.07056300342082977\n",
      "Epoch 8045, Train Loss: 0.01552988588809967, Test Loss: 0.07057549059391022\n",
      "Epoch 8046, Train Loss: 0.015526579692959785, Test Loss: 0.0705670490860939\n",
      "Epoch 8047, Train Loss: 0.015523607842624187, Test Loss: 0.07056373357772827\n",
      "Epoch 8048, Train Loss: 0.015520903281867504, Test Loss: 0.07056477665901184\n",
      "Epoch 8049, Train Loss: 0.015517965890467167, Test Loss: 0.07054099440574646\n",
      "Epoch 8050, Train Loss: 0.015514668077230453, Test Loss: 0.07056605070829391\n",
      "Epoch 8051, Train Loss: 0.015511035919189453, Test Loss: 0.0705522745847702\n",
      "Epoch 8052, Train Loss: 0.015508006326854229, Test Loss: 0.07055067270994186\n",
      "Epoch 8053, Train Loss: 0.015504925511777401, Test Loss: 0.07054779678583145\n",
      "Epoch 8054, Train Loss: 0.01550194714218378, Test Loss: 0.0705387070775032\n",
      "Epoch 8055, Train Loss: 0.015498843044042587, Test Loss: 0.07054321467876434\n",
      "Epoch 8056, Train Loss: 0.015495744533836842, Test Loss: 0.07054105401039124\n",
      "Epoch 8057, Train Loss: 0.015492750331759453, Test Loss: 0.0705404132604599\n",
      "Epoch 8058, Train Loss: 0.015490129590034485, Test Loss: 0.07054397463798523\n",
      "Epoch 8059, Train Loss: 0.015486580319702625, Test Loss: 0.07053671032190323\n",
      "Epoch 8060, Train Loss: 0.015483524650335312, Test Loss: 0.07054045796394348\n",
      "Epoch 8061, Train Loss: 0.015480486676096916, Test Loss: 0.07052893936634064\n",
      "Epoch 8062, Train Loss: 0.01547771506011486, Test Loss: 0.0705169290304184\n",
      "Epoch 8063, Train Loss: 0.01547488197684288, Test Loss: 0.07050608843564987\n",
      "Epoch 8064, Train Loss: 0.015472164377570152, Test Loss: 0.07051590085029602\n",
      "Epoch 8065, Train Loss: 0.015467997640371323, Test Loss: 0.0705287978053093\n",
      "Epoch 8066, Train Loss: 0.015465510077774525, Test Loss: 0.07051745802164078\n",
      "Epoch 8067, Train Loss: 0.015462181530892849, Test Loss: 0.07051622867584229\n",
      "Epoch 8068, Train Loss: 0.015459476970136166, Test Loss: 0.07050729542970657\n",
      "Epoch 8069, Train Loss: 0.015456023626029491, Test Loss: 0.07051338255405426\n",
      "Epoch 8070, Train Loss: 0.0154528534039855, Test Loss: 0.07051653414964676\n",
      "Epoch 8071, Train Loss: 0.01545009296387434, Test Loss: 0.07051818817853928\n",
      "Epoch 8072, Train Loss: 0.01544671319425106, Test Loss: 0.07051844894886017\n",
      "Epoch 8073, Train Loss: 0.015443573705852032, Test Loss: 0.07051355391740799\n",
      "Epoch 8074, Train Loss: 0.015441237017512321, Test Loss: 0.07051371783018112\n",
      "Epoch 8075, Train Loss: 0.015437966212630272, Test Loss: 0.07051651924848557\n",
      "Epoch 8076, Train Loss: 0.01543612964451313, Test Loss: 0.07051154226064682\n",
      "Epoch 8077, Train Loss: 0.015431363135576248, Test Loss: 0.07050461322069168\n",
      "Epoch 8078, Train Loss: 0.015428382903337479, Test Loss: 0.07050442695617676\n",
      "Epoch 8079, Train Loss: 0.015425723046064377, Test Loss: 0.07049991935491562\n",
      "Epoch 8080, Train Loss: 0.015422326512634754, Test Loss: 0.070492222905159\n",
      "Epoch 8081, Train Loss: 0.015419216826558113, Test Loss: 0.07049257308244705\n",
      "Epoch 8082, Train Loss: 0.015416151843965054, Test Loss: 0.07049538940191269\n",
      "Epoch 8083, Train Loss: 0.015413112938404083, Test Loss: 0.07049625366926193\n",
      "Epoch 8084, Train Loss: 0.015410322695970535, Test Loss: 0.0704897940158844\n",
      "Epoch 8085, Train Loss: 0.015407456085085869, Test Loss: 0.07047629356384277\n",
      "Epoch 8086, Train Loss: 0.01540418341755867, Test Loss: 0.07047837972640991\n",
      "Epoch 8087, Train Loss: 0.015401863493025303, Test Loss: 0.07047948986291885\n",
      "Epoch 8088, Train Loss: 0.01539782527834177, Test Loss: 0.07048030197620392\n",
      "Epoch 8089, Train Loss: 0.015394938178360462, Test Loss: 0.07048824429512024\n",
      "Epoch 8090, Train Loss: 0.015391988679766655, Test Loss: 0.070484958589077\n",
      "Epoch 8091, Train Loss: 0.015388723462820053, Test Loss: 0.07047898322343826\n",
      "Epoch 8092, Train Loss: 0.015385784208774567, Test Loss: 0.07047849893569946\n",
      "Epoch 8093, Train Loss: 0.015383243560791016, Test Loss: 0.07046905905008316\n",
      "Epoch 8094, Train Loss: 0.015380407683551311, Test Loss: 0.0704604983329773\n",
      "Epoch 8095, Train Loss: 0.01537751778960228, Test Loss: 0.07045616954565048\n",
      "Epoch 8096, Train Loss: 0.015373893082141876, Test Loss: 0.07046350091695786\n",
      "Epoch 8097, Train Loss: 0.015370698645710945, Test Loss: 0.07046356052160263\n",
      "Epoch 8098, Train Loss: 0.0153676001355052, Test Loss: 0.07046454399824142\n",
      "Epoch 8099, Train Loss: 0.0153650576248765, Test Loss: 0.07045868039131165\n",
      "Epoch 8100, Train Loss: 0.015362061560153961, Test Loss: 0.07044921815395355\n",
      "Epoch 8101, Train Loss: 0.01535840891301632, Test Loss: 0.07044976949691772\n",
      "Epoch 8102, Train Loss: 0.015355493873357773, Test Loss: 0.07045072317123413\n",
      "Epoch 8103, Train Loss: 0.015352399088442326, Test Loss: 0.07045255601406097\n",
      "Epoch 8104, Train Loss: 0.01534970197826624, Test Loss: 0.0704563707113266\n",
      "Epoch 8105, Train Loss: 0.015346318483352661, Test Loss: 0.07045391947031021\n",
      "Epoch 8106, Train Loss: 0.015343190170824528, Test Loss: 0.07044392824172974\n",
      "Epoch 8107, Train Loss: 0.015340464189648628, Test Loss: 0.07043647766113281\n",
      "Epoch 8108, Train Loss: 0.015337284654378891, Test Loss: 0.07044337689876556\n",
      "Epoch 8109, Train Loss: 0.015334684401750565, Test Loss: 0.0704268217086792\n",
      "Epoch 8110, Train Loss: 0.015331719070672989, Test Loss: 0.07042808830738068\n",
      "Epoch 8111, Train Loss: 0.0153282405808568, Test Loss: 0.07043731957674026\n",
      "Epoch 8112, Train Loss: 0.015325283631682396, Test Loss: 0.07044407725334167\n",
      "Epoch 8113, Train Loss: 0.01532203983515501, Test Loss: 0.0704396665096283\n",
      "Epoch 8114, Train Loss: 0.015319149941205978, Test Loss: 0.07043334096670151\n",
      "Epoch 8115, Train Loss: 0.015316023491322994, Test Loss: 0.070427305996418\n",
      "Epoch 8116, Train Loss: 0.015313488431274891, Test Loss: 0.07042016088962555\n",
      "Epoch 8117, Train Loss: 0.015310468152165413, Test Loss: 0.07041914761066437\n",
      "Epoch 8118, Train Loss: 0.015307016670703888, Test Loss: 0.07042594999074936\n",
      "Epoch 8119, Train Loss: 0.015304327011108398, Test Loss: 0.07042286545038223\n",
      "Epoch 8120, Train Loss: 0.015300760045647621, Test Loss: 0.07042931765317917\n",
      "Epoch 8121, Train Loss: 0.015297888778150082, Test Loss: 0.07042307406663895\n",
      "Epoch 8122, Train Loss: 0.015294946730136871, Test Loss: 0.07042250037193298\n",
      "Epoch 8123, Train Loss: 0.015291825868189335, Test Loss: 0.07042455673217773\n",
      "Epoch 8124, Train Loss: 0.015288867056369781, Test Loss: 0.07041691243648529\n",
      "Epoch 8125, Train Loss: 0.015285922214388847, Test Loss: 0.0704205110669136\n",
      "Epoch 8126, Train Loss: 0.015283157117664814, Test Loss: 0.07041817158460617\n",
      "Epoch 8127, Train Loss: 0.015279733575880527, Test Loss: 0.07040838152170181\n",
      "Epoch 8128, Train Loss: 0.01527647115290165, Test Loss: 0.07040558010339737\n",
      "Epoch 8129, Train Loss: 0.01527358591556549, Test Loss: 0.0704011544585228\n",
      "Epoch 8130, Train Loss: 0.015270673669874668, Test Loss: 0.0704004243016243\n",
      "Epoch 8131, Train Loss: 0.015267973765730858, Test Loss: 0.07039668411016464\n",
      "Epoch 8132, Train Loss: 0.01526445709168911, Test Loss: 0.07039789110422134\n",
      "Epoch 8133, Train Loss: 0.01526190061122179, Test Loss: 0.07039197534322739\n",
      "Epoch 8134, Train Loss: 0.015258532017469406, Test Loss: 0.07039560377597809\n",
      "Epoch 8135, Train Loss: 0.015256387181580067, Test Loss: 0.07038884609937668\n",
      "Epoch 8136, Train Loss: 0.015252836979925632, Test Loss: 0.07037900388240814\n",
      "Epoch 8137, Train Loss: 0.015249937772750854, Test Loss: 0.07037831842899323\n",
      "Epoch 8138, Train Loss: 0.015246542170643806, Test Loss: 0.07038211822509766\n",
      "Epoch 8139, Train Loss: 0.015243631787598133, Test Loss: 0.07037405669689178\n",
      "Epoch 8140, Train Loss: 0.015240361914038658, Test Loss: 0.07037453353404999\n",
      "Epoch 8141, Train Loss: 0.015237723477184772, Test Loss: 0.07037845999002457\n",
      "Epoch 8142, Train Loss: 0.01523447036743164, Test Loss: 0.0703759416937828\n",
      "Epoch 8143, Train Loss: 0.015232236124575138, Test Loss: 0.07038762420415878\n",
      "Epoch 8144, Train Loss: 0.015228388831019402, Test Loss: 0.0703745186328888\n",
      "Epoch 8145, Train Loss: 0.015225301496684551, Test Loss: 0.07036922127008438\n",
      "Epoch 8146, Train Loss: 0.015223376452922821, Test Loss: 0.0703706368803978\n",
      "Epoch 8147, Train Loss: 0.015219321474432945, Test Loss: 0.07036610692739487\n",
      "Epoch 8148, Train Loss: 0.015218217857182026, Test Loss: 0.07037287950515747\n",
      "Epoch 8149, Train Loss: 0.01521332561969757, Test Loss: 0.07036744803190231\n",
      "Epoch 8150, Train Loss: 0.015210561454296112, Test Loss: 0.07036790996789932\n",
      "Epoch 8151, Train Loss: 0.015208056196570396, Test Loss: 0.07035135477781296\n",
      "Epoch 8152, Train Loss: 0.015205048955976963, Test Loss: 0.0703529641032219\n",
      "Epoch 8153, Train Loss: 0.01520140003412962, Test Loss: 0.07035236805677414\n",
      "Epoch 8154, Train Loss: 0.015199011191725731, Test Loss: 0.0703420341014862\n",
      "Epoch 8155, Train Loss: 0.015196389518678188, Test Loss: 0.07033629715442657\n",
      "Epoch 8156, Train Loss: 0.015192187391221523, Test Loss: 0.0703459084033966\n",
      "Epoch 8157, Train Loss: 0.015189748257398605, Test Loss: 0.07034322619438171\n",
      "Epoch 8158, Train Loss: 0.015186565928161144, Test Loss: 0.07033279538154602\n",
      "Epoch 8159, Train Loss: 0.015183149836957455, Test Loss: 0.07033977657556534\n",
      "Epoch 8160, Train Loss: 0.015180195681750774, Test Loss: 0.0703350082039833\n",
      "Epoch 8161, Train Loss: 0.015177563764154911, Test Loss: 0.07033323496580124\n",
      "Epoch 8162, Train Loss: 0.01517442986369133, Test Loss: 0.07034415751695633\n",
      "Epoch 8163, Train Loss: 0.015171632170677185, Test Loss: 0.07033825665712357\n",
      "Epoch 8164, Train Loss: 0.015168285928666592, Test Loss: 0.07034141570329666\n",
      "Epoch 8165, Train Loss: 0.015165631659328938, Test Loss: 0.07033559679985046\n",
      "Epoch 8166, Train Loss: 0.015162713825702667, Test Loss: 0.07033741474151611\n",
      "Epoch 8167, Train Loss: 0.015159395523369312, Test Loss: 0.07034189254045486\n",
      "Epoch 8168, Train Loss: 0.01515648327767849, Test Loss: 0.07032881677150726\n",
      "Epoch 8169, Train Loss: 0.015153447166085243, Test Loss: 0.07033456116914749\n",
      "Epoch 8170, Train Loss: 0.015150332823395729, Test Loss: 0.0703265368938446\n",
      "Epoch 8171, Train Loss: 0.015147421509027481, Test Loss: 0.07032464444637299\n",
      "Epoch 8172, Train Loss: 0.015144594945013523, Test Loss: 0.07033168524503708\n",
      "Epoch 8173, Train Loss: 0.015141346491873264, Test Loss: 0.07031849026679993\n",
      "Epoch 8174, Train Loss: 0.01513843983411789, Test Loss: 0.07032003998756409\n",
      "Epoch 8175, Train Loss: 0.015135382302105427, Test Loss: 0.0703144446015358\n",
      "Epoch 8176, Train Loss: 0.015132828615605831, Test Loss: 0.07031869888305664\n",
      "Epoch 8177, Train Loss: 0.01512913964688778, Test Loss: 0.07031950354576111\n",
      "Epoch 8178, Train Loss: 0.01512638758867979, Test Loss: 0.07030439376831055\n",
      "Epoch 8179, Train Loss: 0.015123102813959122, Test Loss: 0.0703054890036583\n",
      "Epoch 8180, Train Loss: 0.015120347961783409, Test Loss: 0.07031282037496567\n",
      "Epoch 8181, Train Loss: 0.015117395669221878, Test Loss: 0.07030992209911346\n",
      "Epoch 8182, Train Loss: 0.015114516951143742, Test Loss: 0.07030748575925827\n",
      "Epoch 8183, Train Loss: 0.015111332759261131, Test Loss: 0.0703129693865776\n",
      "Epoch 8184, Train Loss: 0.015108424238860607, Test Loss: 0.07031795382499695\n",
      "Epoch 8185, Train Loss: 0.015105366706848145, Test Loss: 0.07032088935375214\n",
      "Epoch 8186, Train Loss: 0.015103024430572987, Test Loss: 0.07032537460327148\n",
      "Epoch 8187, Train Loss: 0.015099572017788887, Test Loss: 0.07031665742397308\n",
      "Epoch 8188, Train Loss: 0.015096334740519524, Test Loss: 0.07029839605093002\n",
      "Epoch 8189, Train Loss: 0.015093094669282436, Test Loss: 0.07029693573713303\n",
      "Epoch 8190, Train Loss: 0.015090328641235828, Test Loss: 0.07029271125793457\n",
      "Epoch 8191, Train Loss: 0.015087357722222805, Test Loss: 0.07029405236244202\n",
      "Epoch 8192, Train Loss: 0.01508425548672676, Test Loss: 0.07029645144939423\n",
      "Epoch 8193, Train Loss: 0.015081604942679405, Test Loss: 0.0702868178486824\n",
      "Epoch 8194, Train Loss: 0.015078478492796421, Test Loss: 0.07028083503246307\n",
      "Epoch 8195, Train Loss: 0.01507504377514124, Test Loss: 0.07029175758361816\n",
      "Epoch 8196, Train Loss: 0.015072357840836048, Test Loss: 0.07028008997440338\n",
      "Epoch 8197, Train Loss: 0.015069819986820221, Test Loss: 0.07027539610862732\n",
      "Epoch 8198, Train Loss: 0.0150659354403615, Test Loss: 0.07028458267450333\n",
      "Epoch 8199, Train Loss: 0.015063250437378883, Test Loss: 0.07027905434370041\n",
      "Epoch 8200, Train Loss: 0.015060159377753735, Test Loss: 0.07028347998857498\n",
      "Epoch 8201, Train Loss: 0.015057709068059921, Test Loss: 0.07026685029268265\n",
      "Epoch 8202, Train Loss: 0.015054388903081417, Test Loss: 0.0702667236328125\n",
      "Epoch 8203, Train Loss: 0.015051222406327724, Test Loss: 0.07027239352464676\n",
      "Epoch 8204, Train Loss: 0.015048284083604813, Test Loss: 0.07028337568044662\n",
      "Epoch 8205, Train Loss: 0.01504526287317276, Test Loss: 0.07027129083871841\n",
      "Epoch 8206, Train Loss: 0.015042409300804138, Test Loss: 0.07025722414255142\n",
      "Epoch 8207, Train Loss: 0.01503922138363123, Test Loss: 0.07026664912700653\n",
      "Epoch 8208, Train Loss: 0.01503608375787735, Test Loss: 0.0702635869383812\n",
      "Epoch 8209, Train Loss: 0.015032977797091007, Test Loss: 0.07026322931051254\n",
      "Epoch 8210, Train Loss: 0.015030445531010628, Test Loss: 0.07025612145662308\n",
      "Epoch 8211, Train Loss: 0.015027443878352642, Test Loss: 0.07025549560785294\n",
      "Epoch 8212, Train Loss: 0.015024647116661072, Test Loss: 0.07024374604225159\n",
      "Epoch 8213, Train Loss: 0.01502139586955309, Test Loss: 0.07024442404508591\n",
      "Epoch 8214, Train Loss: 0.015018590725958347, Test Loss: 0.07024800777435303\n",
      "Epoch 8215, Train Loss: 0.015015191398561, Test Loss: 0.07024162262678146\n",
      "Epoch 8216, Train Loss: 0.015012489631772041, Test Loss: 0.07024045288562775\n",
      "Epoch 8217, Train Loss: 0.015009974129498005, Test Loss: 0.07025092840194702\n",
      "Epoch 8218, Train Loss: 0.015006535686552525, Test Loss: 0.07024648040533066\n",
      "Epoch 8219, Train Loss: 0.01500355452299118, Test Loss: 0.0702534094452858\n",
      "Epoch 8220, Train Loss: 0.015000240877270699, Test Loss: 0.07024676352739334\n",
      "Epoch 8221, Train Loss: 0.0149970268830657, Test Loss: 0.07024659961462021\n",
      "Epoch 8222, Train Loss: 0.014994082041084766, Test Loss: 0.07023698836565018\n",
      "Epoch 8223, Train Loss: 0.014991510659456253, Test Loss: 0.07023261487483978\n",
      "Epoch 8224, Train Loss: 0.014988171868026257, Test Loss: 0.07023885101079941\n",
      "Epoch 8225, Train Loss: 0.014985106885433197, Test Loss: 0.07023859024047852\n",
      "Epoch 8226, Train Loss: 0.014982634223997593, Test Loss: 0.07023901492357254\n",
      "Epoch 8227, Train Loss: 0.014979137107729912, Test Loss: 0.07022734731435776\n",
      "Epoch 8228, Train Loss: 0.01497627329081297, Test Loss: 0.07022510468959808\n",
      "Epoch 8229, Train Loss: 0.01497337594628334, Test Loss: 0.0702313557267189\n",
      "Epoch 8230, Train Loss: 0.014970325864851475, Test Loss: 0.07023259997367859\n",
      "Epoch 8231, Train Loss: 0.014967324212193489, Test Loss: 0.07022584974765778\n",
      "Epoch 8232, Train Loss: 0.014964441768825054, Test Loss: 0.0702250525355339\n",
      "Epoch 8233, Train Loss: 0.014961514621973038, Test Loss: 0.07023259997367859\n",
      "Epoch 8234, Train Loss: 0.014958306215703487, Test Loss: 0.07022838294506073\n",
      "Epoch 8235, Train Loss: 0.014955537393689156, Test Loss: 0.07021558284759521\n",
      "Epoch 8236, Train Loss: 0.014953930862247944, Test Loss: 0.07021771371364594\n",
      "Epoch 8237, Train Loss: 0.01494965236634016, Test Loss: 0.07021027058362961\n",
      "Epoch 8238, Train Loss: 0.014946726150810719, Test Loss: 0.07019952684640884\n",
      "Epoch 8239, Train Loss: 0.014943724498152733, Test Loss: 0.07020550221204758\n",
      "Epoch 8240, Train Loss: 0.014940650202333927, Test Loss: 0.07020280510187149\n",
      "Epoch 8241, Train Loss: 0.014938652515411377, Test Loss: 0.07019853591918945\n",
      "Epoch 8242, Train Loss: 0.014934624545276165, Test Loss: 0.07020474225282669\n",
      "Epoch 8243, Train Loss: 0.014931818470358849, Test Loss: 0.07020221650600433\n",
      "Epoch 8244, Train Loss: 0.014928842894732952, Test Loss: 0.07019820064306259\n",
      "Epoch 8245, Train Loss: 0.014925681985914707, Test Loss: 0.07020184397697449\n",
      "Epoch 8246, Train Loss: 0.01492302305996418, Test Loss: 0.07019290328025818\n",
      "Epoch 8247, Train Loss: 0.014920086599886417, Test Loss: 0.07018593698740005\n",
      "Epoch 8248, Train Loss: 0.014917240478098392, Test Loss: 0.07018512487411499\n",
      "Epoch 8249, Train Loss: 0.014914040453732014, Test Loss: 0.0701855942606926\n",
      "Epoch 8250, Train Loss: 0.014910933561623096, Test Loss: 0.07018354535102844\n",
      "Epoch 8251, Train Loss: 0.014907770790159702, Test Loss: 0.07019064575433731\n",
      "Epoch 8252, Train Loss: 0.014905066229403019, Test Loss: 0.07017666101455688\n",
      "Epoch 8253, Train Loss: 0.014901875518262386, Test Loss: 0.07017743587493896\n",
      "Epoch 8254, Train Loss: 0.014899089932441711, Test Loss: 0.07017163187265396\n",
      "Epoch 8255, Train Loss: 0.014896219596266747, Test Loss: 0.07017485052347183\n",
      "Epoch 8256, Train Loss: 0.014893143437802792, Test Loss: 0.07016895711421967\n",
      "Epoch 8257, Train Loss: 0.014889971353113651, Test Loss: 0.07017488777637482\n",
      "Epoch 8258, Train Loss: 0.014887161552906036, Test Loss: 0.07016602158546448\n",
      "Epoch 8259, Train Loss: 0.014884553849697113, Test Loss: 0.07016922533512115\n",
      "Epoch 8260, Train Loss: 0.014881146140396595, Test Loss: 0.0701640173792839\n",
      "Epoch 8261, Train Loss: 0.014878561720252037, Test Loss: 0.07016604393720627\n",
      "Epoch 8262, Train Loss: 0.014876359142363071, Test Loss: 0.07016000151634216\n",
      "Epoch 8263, Train Loss: 0.014872288331389427, Test Loss: 0.07016962021589279\n",
      "Epoch 8264, Train Loss: 0.014869381673634052, Test Loss: 0.07015740126371384\n",
      "Epoch 8265, Train Loss: 0.01486663892865181, Test Loss: 0.07016324251890182\n",
      "Epoch 8266, Train Loss: 0.014864640310406685, Test Loss: 0.0701407715678215\n",
      "Epoch 8267, Train Loss: 0.01486053504049778, Test Loss: 0.07015884667634964\n",
      "Epoch 8268, Train Loss: 0.014857569709420204, Test Loss: 0.07016009837388992\n",
      "Epoch 8269, Train Loss: 0.01485468354076147, Test Loss: 0.07015521824359894\n",
      "Epoch 8270, Train Loss: 0.014851848594844341, Test Loss: 0.07015407085418701\n",
      "Epoch 8271, Train Loss: 0.014848719350993633, Test Loss: 0.07015708088874817\n",
      "Epoch 8272, Train Loss: 0.014846333302557468, Test Loss: 0.07013992965221405\n",
      "Epoch 8273, Train Loss: 0.014842705801129341, Test Loss: 0.0701446607708931\n",
      "Epoch 8274, Train Loss: 0.01484000589698553, Test Loss: 0.07014506310224533\n",
      "Epoch 8275, Train Loss: 0.014837098307907581, Test Loss: 0.07015358656644821\n",
      "Epoch 8276, Train Loss: 0.014833816327154636, Test Loss: 0.07015207409858704\n",
      "Epoch 8277, Train Loss: 0.014831438660621643, Test Loss: 0.07014476507902145\n",
      "Epoch 8278, Train Loss: 0.014828468672931194, Test Loss: 0.0701432079076767\n",
      "Epoch 8279, Train Loss: 0.014825375750660896, Test Loss: 0.07014358043670654\n",
      "Epoch 8280, Train Loss: 0.014823016710579395, Test Loss: 0.07014283537864685\n",
      "Epoch 8281, Train Loss: 0.014819125644862652, Test Loss: 0.07013219594955444\n",
      "Epoch 8282, Train Loss: 0.014816360548138618, Test Loss: 0.07012399286031723\n",
      "Epoch 8283, Train Loss: 0.014814062975347042, Test Loss: 0.07011927664279938\n",
      "Epoch 8284, Train Loss: 0.014810916036367416, Test Loss: 0.07012180238962173\n",
      "Epoch 8285, Train Loss: 0.014807871542870998, Test Loss: 0.07011672109365463\n",
      "Epoch 8286, Train Loss: 0.014804541133344173, Test Loss: 0.07012005150318146\n",
      "Epoch 8287, Train Loss: 0.01480166520923376, Test Loss: 0.07011431455612183\n",
      "Epoch 8288, Train Loss: 0.014798684045672417, Test Loss: 0.07011201977729797\n",
      "Epoch 8289, Train Loss: 0.014795859344303608, Test Loss: 0.07010994106531143\n",
      "Epoch 8290, Train Loss: 0.01479312963783741, Test Loss: 0.07010683417320251\n",
      "Epoch 8291, Train Loss: 0.01478980015963316, Test Loss: 0.07010931521654129\n",
      "Epoch 8292, Train Loss: 0.014787032268941402, Test Loss: 0.0701073557138443\n",
      "Epoch 8293, Train Loss: 0.014783860184252262, Test Loss: 0.07010366022586823\n",
      "Epoch 8294, Train Loss: 0.014781532809138298, Test Loss: 0.07009492814540863\n",
      "Epoch 8295, Train Loss: 0.014778666198253632, Test Loss: 0.07008852809667587\n",
      "Epoch 8296, Train Loss: 0.014774898998439312, Test Loss: 0.07009671628475189\n",
      "Epoch 8297, Train Loss: 0.014772582799196243, Test Loss: 0.07010781019926071\n",
      "Epoch 8298, Train Loss: 0.01476916205137968, Test Loss: 0.07009939849376678\n",
      "Epoch 8299, Train Loss: 0.014766051433980465, Test Loss: 0.07010598480701447\n",
      "Epoch 8300, Train Loss: 0.01476355642080307, Test Loss: 0.07010091096162796\n",
      "Epoch 8301, Train Loss: 0.014760655350983143, Test Loss: 0.0700971931219101\n",
      "Epoch 8302, Train Loss: 0.014757957309484482, Test Loss: 0.07008235156536102\n",
      "Epoch 8303, Train Loss: 0.014755015261471272, Test Loss: 0.07007904350757599\n",
      "Epoch 8304, Train Loss: 0.014751322567462921, Test Loss: 0.07008921355009079\n",
      "Epoch 8305, Train Loss: 0.014748722314834595, Test Loss: 0.07008711248636246\n",
      "Epoch 8306, Train Loss: 0.014745686203241348, Test Loss: 0.07008958607912064\n",
      "Epoch 8307, Train Loss: 0.014743340201675892, Test Loss: 0.07008513808250427\n",
      "Epoch 8308, Train Loss: 0.014739622361958027, Test Loss: 0.07009314000606537\n",
      "Epoch 8309, Train Loss: 0.014737007208168507, Test Loss: 0.07009977847337723\n",
      "Epoch 8310, Train Loss: 0.014734293334186077, Test Loss: 0.07009375840425491\n",
      "Epoch 8311, Train Loss: 0.014731518924236298, Test Loss: 0.07009731978178024\n",
      "Epoch 8312, Train Loss: 0.014728611335158348, Test Loss: 0.07008684426546097\n",
      "Epoch 8313, Train Loss: 0.014725285582244396, Test Loss: 0.07008235156536102\n",
      "Epoch 8314, Train Loss: 0.014722266234457493, Test Loss: 0.07008227705955505\n",
      "Epoch 8315, Train Loss: 0.014719356782734394, Test Loss: 0.07006970793008804\n",
      "Epoch 8316, Train Loss: 0.014716527424752712, Test Loss: 0.07006508111953735\n",
      "Epoch 8317, Train Loss: 0.014713597483932972, Test Loss: 0.07005983591079712\n",
      "Epoch 8318, Train Loss: 0.01471097394824028, Test Loss: 0.07005278021097183\n",
      "Epoch 8319, Train Loss: 0.014707930386066437, Test Loss: 0.07005590945482254\n",
      "Epoch 8320, Train Loss: 0.014705757610499859, Test Loss: 0.07005265355110168\n",
      "Epoch 8321, Train Loss: 0.014701616950333118, Test Loss: 0.07005496323108673\n",
      "Epoch 8322, Train Loss: 0.014699164777994156, Test Loss: 0.07005484402179718\n",
      "Epoch 8323, Train Loss: 0.01469628605991602, Test Loss: 0.07004601508378983\n",
      "Epoch 8324, Train Loss: 0.014692913740873337, Test Loss: 0.07005266845226288\n",
      "Epoch 8325, Train Loss: 0.014690042473375797, Test Loss: 0.07005303353071213\n",
      "Epoch 8326, Train Loss: 0.014687628485262394, Test Loss: 0.07005110383033752\n",
      "Epoch 8327, Train Loss: 0.014684050343930721, Test Loss: 0.0700533390045166\n",
      "Epoch 8328, Train Loss: 0.014681385830044746, Test Loss: 0.0700400322675705\n",
      "Epoch 8329, Train Loss: 0.014678393490612507, Test Loss: 0.07004409283399582\n",
      "Epoch 8330, Train Loss: 0.014675658196210861, Test Loss: 0.07004140317440033\n",
      "Epoch 8331, Train Loss: 0.014672886580228806, Test Loss: 0.07004065066576004\n",
      "Epoch 8332, Train Loss: 0.014669891446828842, Test Loss: 0.07003781199455261\n",
      "Epoch 8333, Train Loss: 0.014666727744042873, Test Loss: 0.07004084438085556\n",
      "Epoch 8334, Train Loss: 0.01466371025890112, Test Loss: 0.07003235071897507\n",
      "Epoch 8335, Train Loss: 0.014661367982625961, Test Loss: 0.07002217322587967\n",
      "Epoch 8336, Train Loss: 0.014658058062195778, Test Loss: 0.07003162056207657\n",
      "Epoch 8337, Train Loss: 0.014655687846243382, Test Loss: 0.07003424316644669\n",
      "Epoch 8338, Train Loss: 0.014652020297944546, Test Loss: 0.07002643495798111\n",
      "Epoch 8339, Train Loss: 0.014649019576609135, Test Loss: 0.07002751529216766\n",
      "Epoch 8340, Train Loss: 0.014646247960627079, Test Loss: 0.07002749294042587\n",
      "Epoch 8341, Train Loss: 0.014644511044025421, Test Loss: 0.07003457844257355\n",
      "Epoch 8342, Train Loss: 0.014640646055340767, Test Loss: 0.07002653181552887\n",
      "Epoch 8343, Train Loss: 0.014637995511293411, Test Loss: 0.07002806663513184\n",
      "Epoch 8344, Train Loss: 0.014634734019637108, Test Loss: 0.0700242668390274\n",
      "Epoch 8345, Train Loss: 0.014632086269557476, Test Loss: 0.07002943009138107\n",
      "Epoch 8346, Train Loss: 0.014628958888351917, Test Loss: 0.07002652436494827\n",
      "Epoch 8347, Train Loss: 0.014625808224081993, Test Loss: 0.07001633942127228\n",
      "Epoch 8348, Train Loss: 0.014623301103711128, Test Loss: 0.07001236081123352\n",
      "Epoch 8349, Train Loss: 0.014620079658925533, Test Loss: 0.0700175017118454\n",
      "Epoch 8350, Train Loss: 0.014617064036428928, Test Loss: 0.07001513242721558\n",
      "Epoch 8351, Train Loss: 0.014614274725317955, Test Loss: 0.070008285343647\n",
      "Epoch 8352, Train Loss: 0.014612783677875996, Test Loss: 0.06998993456363678\n",
      "Epoch 8353, Train Loss: 0.014609487727284431, Test Loss: 0.07000906765460968\n",
      "Epoch 8354, Train Loss: 0.014605658128857613, Test Loss: 0.07000234723091125\n",
      "Epoch 8355, Train Loss: 0.014602660201489925, Test Loss: 0.06999944150447845\n",
      "Epoch 8356, Train Loss: 0.014600163325667381, Test Loss: 0.06999211013317108\n",
      "Epoch 8357, Train Loss: 0.014597819186747074, Test Loss: 0.06998288631439209\n",
      "Epoch 8358, Train Loss: 0.014594090171158314, Test Loss: 0.06999583542346954\n",
      "Epoch 8359, Train Loss: 0.014591234736144543, Test Loss: 0.06999193876981735\n",
      "Epoch 8360, Train Loss: 0.014588627964258194, Test Loss: 0.070006363093853\n",
      "Epoch 8361, Train Loss: 0.014585928060114384, Test Loss: 0.0700044184923172\n",
      "Epoch 8362, Train Loss: 0.014582368545234203, Test Loss: 0.06999439746141434\n",
      "Epoch 8363, Train Loss: 0.014579295180737972, Test Loss: 0.06998845934867859\n",
      "Epoch 8364, Train Loss: 0.014576554298400879, Test Loss: 0.06998826563358307\n",
      "Epoch 8365, Train Loss: 0.014573637396097183, Test Loss: 0.06999116390943527\n",
      "Epoch 8366, Train Loss: 0.014571799896657467, Test Loss: 0.06997615098953247\n",
      "Epoch 8367, Train Loss: 0.014568144455552101, Test Loss: 0.069981649518013\n",
      "Epoch 8368, Train Loss: 0.014565383084118366, Test Loss: 0.06997328251600266\n",
      "Epoch 8369, Train Loss: 0.014562025666236877, Test Loss: 0.06997958570718765\n",
      "Epoch 8370, Train Loss: 0.014559036120772362, Test Loss: 0.06998134404420853\n",
      "Epoch 8371, Train Loss: 0.014556650072336197, Test Loss: 0.06997464597225189\n",
      "Epoch 8372, Train Loss: 0.01455378346145153, Test Loss: 0.06995729357004166\n",
      "Epoch 8373, Train Loss: 0.014550763182342052, Test Loss: 0.06995754688978195\n",
      "Epoch 8374, Train Loss: 0.014547877013683319, Test Loss: 0.06995554268360138\n",
      "Epoch 8375, Train Loss: 0.014545092359185219, Test Loss: 0.06995195150375366\n",
      "Epoch 8376, Train Loss: 0.014542966149747372, Test Loss: 0.06995908170938492\n",
      "Epoch 8377, Train Loss: 0.014539284631609917, Test Loss: 0.06996291130781174\n",
      "Epoch 8378, Train Loss: 0.014536624774336815, Test Loss: 0.06995842605829239\n",
      "Epoch 8379, Train Loss: 0.01453321147710085, Test Loss: 0.06996384263038635\n",
      "Epoch 8380, Train Loss: 0.014530415646731853, Test Loss: 0.06995585560798645\n",
      "Epoch 8381, Train Loss: 0.014527724124491215, Test Loss: 0.06995860487222672\n",
      "Epoch 8382, Train Loss: 0.01452489010989666, Test Loss: 0.069953553378582\n",
      "Epoch 8383, Train Loss: 0.014522272162139416, Test Loss: 0.06994522362947464\n",
      "Epoch 8384, Train Loss: 0.014518863521516323, Test Loss: 0.06994504481554031\n",
      "Epoch 8385, Train Loss: 0.014516220428049564, Test Loss: 0.06994558870792389\n",
      "Epoch 8386, Train Loss: 0.014513361267745495, Test Loss: 0.06994888931512833\n",
      "Epoch 8387, Train Loss: 0.014510230161249638, Test Loss: 0.06994292885065079\n",
      "Epoch 8388, Train Loss: 0.014507458545267582, Test Loss: 0.06994367390871048\n",
      "Epoch 8389, Train Loss: 0.014505190774798393, Test Loss: 0.06993217766284943\n",
      "Epoch 8390, Train Loss: 0.014501698315143585, Test Loss: 0.06993161141872406\n",
      "Epoch 8391, Train Loss: 0.014498770236968994, Test Loss: 0.06993179768323898\n",
      "Epoch 8392, Train Loss: 0.01449593435972929, Test Loss: 0.06993882358074188\n",
      "Epoch 8393, Train Loss: 0.014493128284811974, Test Loss: 0.06994360685348511\n",
      "Epoch 8394, Train Loss: 0.014490466564893723, Test Loss: 0.06993567943572998\n",
      "Epoch 8395, Train Loss: 0.014487561769783497, Test Loss: 0.06993617117404938\n",
      "Epoch 8396, Train Loss: 0.014485517516732216, Test Loss: 0.06993678212165833\n",
      "Epoch 8397, Train Loss: 0.014482235535979271, Test Loss: 0.06993047147989273\n",
      "Epoch 8398, Train Loss: 0.01447931956499815, Test Loss: 0.06993252784013748\n",
      "Epoch 8399, Train Loss: 0.014476047828793526, Test Loss: 0.06992547959089279\n",
      "Epoch 8400, Train Loss: 0.014474212191998959, Test Loss: 0.06992581486701965\n",
      "Epoch 8401, Train Loss: 0.014470408670604229, Test Loss: 0.06991910189390182\n",
      "Epoch 8402, Train Loss: 0.014468190260231495, Test Loss: 0.06991753727197647\n",
      "Epoch 8403, Train Loss: 0.01446497905999422, Test Loss: 0.06991501152515411\n",
      "Epoch 8404, Train Loss: 0.014462449587881565, Test Loss: 0.06990620493888855\n",
      "Epoch 8405, Train Loss: 0.014460284262895584, Test Loss: 0.06989366561174393\n",
      "Epoch 8406, Train Loss: 0.014456268399953842, Test Loss: 0.0699046328663826\n",
      "Epoch 8407, Train Loss: 0.014453539624810219, Test Loss: 0.0698927491903305\n",
      "Epoch 8408, Train Loss: 0.014451390132308006, Test Loss: 0.06988835334777832\n",
      "Epoch 8409, Train Loss: 0.014447792433202267, Test Loss: 0.0698956549167633\n",
      "Epoch 8410, Train Loss: 0.014444956555962563, Test Loss: 0.0698961392045021\n",
      "Epoch 8411, Train Loss: 0.014442093670368195, Test Loss: 0.06989019364118576\n",
      "Epoch 8412, Train Loss: 0.014439488761126995, Test Loss: 0.06988094747066498\n",
      "Epoch 8413, Train Loss: 0.01443605124950409, Test Loss: 0.06988595426082611\n",
      "Epoch 8414, Train Loss: 0.014433221891522408, Test Loss: 0.06988417357206345\n",
      "Epoch 8415, Train Loss: 0.014430582523345947, Test Loss: 0.06988231092691422\n",
      "Epoch 8416, Train Loss: 0.01442861370742321, Test Loss: 0.0698680505156517\n",
      "Epoch 8417, Train Loss: 0.01442547794431448, Test Loss: 0.06986454874277115\n",
      "Epoch 8418, Train Loss: 0.014422178268432617, Test Loss: 0.06987503170967102\n",
      "Epoch 8419, Train Loss: 0.014419056475162506, Test Loss: 0.06987594813108444\n",
      "Epoch 8420, Train Loss: 0.014416628517210484, Test Loss: 0.06986930221319199\n",
      "Epoch 8421, Train Loss: 0.014414353296160698, Test Loss: 0.06986677646636963\n",
      "Epoch 8422, Train Loss: 0.014411184936761856, Test Loss: 0.06986980885267258\n",
      "Epoch 8423, Train Loss: 0.014408132061362267, Test Loss: 0.06987431645393372\n",
      "Epoch 8424, Train Loss: 0.014405407011508942, Test Loss: 0.06986475735902786\n",
      "Epoch 8425, Train Loss: 0.014402809552848339, Test Loss: 0.06986240297555923\n",
      "Epoch 8426, Train Loss: 0.014399508014321327, Test Loss: 0.06986786425113678\n",
      "Epoch 8427, Train Loss: 0.014396334066987038, Test Loss: 0.06987115740776062\n",
      "Epoch 8428, Train Loss: 0.014393502846360207, Test Loss: 0.06986559182405472\n",
      "Epoch 8429, Train Loss: 0.014391059055924416, Test Loss: 0.06986870616674423\n",
      "Epoch 8430, Train Loss: 0.0143886748701334, Test Loss: 0.06986460089683533\n",
      "Epoch 8431, Train Loss: 0.014385410584509373, Test Loss: 0.06985390186309814\n",
      "Epoch 8432, Train Loss: 0.014382638968527317, Test Loss: 0.06985487788915634\n",
      "Epoch 8433, Train Loss: 0.01437950786203146, Test Loss: 0.06985266506671906\n",
      "Epoch 8434, Train Loss: 0.014377138577401638, Test Loss: 0.06984367966651917\n",
      "Epoch 8435, Train Loss: 0.01437362376600504, Test Loss: 0.06985161453485489\n",
      "Epoch 8436, Train Loss: 0.014370848424732685, Test Loss: 0.06985438615083694\n",
      "Epoch 8437, Train Loss: 0.014368481002748013, Test Loss: 0.06986778229475021\n",
      "Epoch 8438, Train Loss: 0.014365595765411854, Test Loss: 0.06985139846801758\n",
      "Epoch 8439, Train Loss: 0.014362629503011703, Test Loss: 0.0698477253317833\n",
      "Epoch 8440, Train Loss: 0.014359608292579651, Test Loss: 0.06985065340995789\n",
      "Epoch 8441, Train Loss: 0.01435703132301569, Test Loss: 0.06983866542577744\n",
      "Epoch 8442, Train Loss: 0.014353957027196884, Test Loss: 0.06983603537082672\n",
      "Epoch 8443, Train Loss: 0.014351463876664639, Test Loss: 0.06983354687690735\n",
      "Epoch 8444, Train Loss: 0.014348444528877735, Test Loss: 0.06983517855405807\n",
      "Epoch 8445, Train Loss: 0.014345386996865273, Test Loss: 0.0698363408446312\n",
      "Epoch 8446, Train Loss: 0.014342444017529488, Test Loss: 0.06983623653650284\n",
      "Epoch 8447, Train Loss: 0.014339684508740902, Test Loss: 0.06983312964439392\n",
      "Epoch 8448, Train Loss: 0.0143369035795331, Test Loss: 0.06983505934476852\n",
      "Epoch 8449, Train Loss: 0.014334163628518581, Test Loss: 0.0698365792632103\n",
      "Epoch 8450, Train Loss: 0.014331254176795483, Test Loss: 0.0698361024260521\n",
      "Epoch 8451, Train Loss: 0.014329181984066963, Test Loss: 0.06983522325754166\n",
      "Epoch 8452, Train Loss: 0.014325694181025028, Test Loss: 0.069823257625103\n",
      "Epoch 8453, Train Loss: 0.014322875998914242, Test Loss: 0.0698259025812149\n",
      "Epoch 8454, Train Loss: 0.01432011928409338, Test Loss: 0.06981738656759262\n",
      "Epoch 8455, Train Loss: 0.014317302964627743, Test Loss: 0.06981375813484192\n",
      "Epoch 8456, Train Loss: 0.01431426964700222, Test Loss: 0.06980810314416885\n",
      "Epoch 8457, Train Loss: 0.014312371611595154, Test Loss: 0.06980527937412262\n",
      "Epoch 8458, Train Loss: 0.014308660291135311, Test Loss: 0.06980744004249573\n",
      "Epoch 8459, Train Loss: 0.014306662604212761, Test Loss: 0.06979621201753616\n",
      "Epoch 8460, Train Loss: 0.014303822070360184, Test Loss: 0.06980627775192261\n",
      "Epoch 8461, Train Loss: 0.014301082119345665, Test Loss: 0.06980612874031067\n",
      "Epoch 8462, Train Loss: 0.014297599904239178, Test Loss: 0.06980156153440475\n",
      "Epoch 8463, Train Loss: 0.014294790104031563, Test Loss: 0.06980538368225098\n",
      "Epoch 8464, Train Loss: 0.014291631989181042, Test Loss: 0.069800965487957\n",
      "Epoch 8465, Train Loss: 0.014288819395005703, Test Loss: 0.06979677826166153\n",
      "Epoch 8466, Train Loss: 0.014285925775766373, Test Loss: 0.06980534642934799\n",
      "Epoch 8467, Train Loss: 0.014283283613622189, Test Loss: 0.0697827860713005\n",
      "Epoch 8468, Train Loss: 0.014280184172093868, Test Loss: 0.06978506594896317\n",
      "Epoch 8469, Train Loss: 0.014277408830821514, Test Loss: 0.06978578865528107\n",
      "Epoch 8470, Train Loss: 0.014274710789322853, Test Loss: 0.06978318840265274\n",
      "Epoch 8471, Train Loss: 0.014272518455982208, Test Loss: 0.06977777183055878\n",
      "Epoch 8472, Train Loss: 0.014269036240875721, Test Loss: 0.06978722661733627\n",
      "Epoch 8473, Train Loss: 0.014266147278249264, Test Loss: 0.06977686285972595\n",
      "Epoch 8474, Train Loss: 0.014263474382460117, Test Loss: 0.06977999955415726\n",
      "Epoch 8475, Train Loss: 0.014260395430028439, Test Loss: 0.06978264451026917\n",
      "Epoch 8476, Train Loss: 0.014258187264204025, Test Loss: 0.06976861506700516\n",
      "Epoch 8477, Train Loss: 0.014255350455641747, Test Loss: 0.06977301090955734\n",
      "Epoch 8478, Train Loss: 0.014252065680921078, Test Loss: 0.0697658583521843\n",
      "Epoch 8479, Train Loss: 0.014249236322939396, Test Loss: 0.06976594030857086\n",
      "Epoch 8480, Train Loss: 0.014246758073568344, Test Loss: 0.0697765201330185\n",
      "Epoch 8481, Train Loss: 0.014243489131331444, Test Loss: 0.06977107375860214\n",
      "Epoch 8482, Train Loss: 0.014241412281990051, Test Loss: 0.0697835236787796\n",
      "Epoch 8483, Train Loss: 0.014238209463655949, Test Loss: 0.06976194679737091\n",
      "Epoch 8484, Train Loss: 0.014235151931643486, Test Loss: 0.06976708024740219\n",
      "Epoch 8485, Train Loss: 0.014232448302209377, Test Loss: 0.06975911557674408\n",
      "Epoch 8486, Train Loss: 0.014229520224034786, Test Loss: 0.06975802779197693\n",
      "Epoch 8487, Train Loss: 0.014227043837308884, Test Loss: 0.06975270807743073\n",
      "Epoch 8488, Train Loss: 0.014224235899746418, Test Loss: 0.06975140422582626\n",
      "Epoch 8489, Train Loss: 0.01422154437750578, Test Loss: 0.06974561512470245\n",
      "Epoch 8490, Train Loss: 0.014218623749911785, Test Loss: 0.0697520449757576\n",
      "Epoch 8491, Train Loss: 0.014215560629963875, Test Loss: 0.0697479397058487\n",
      "Epoch 8492, Train Loss: 0.014212418347597122, Test Loss: 0.06975582987070084\n",
      "Epoch 8493, Train Loss: 0.01421000249683857, Test Loss: 0.06974811106920242\n",
      "Epoch 8494, Train Loss: 0.014207092113792896, Test Loss: 0.06974948197603226\n",
      "Epoch 8495, Train Loss: 0.014204506762325764, Test Loss: 0.06973837316036224\n",
      "Epoch 8496, Train Loss: 0.014202981255948544, Test Loss: 0.06973324716091156\n",
      "Epoch 8497, Train Loss: 0.014198796823620796, Test Loss: 0.06973715126514435\n",
      "Epoch 8498, Train Loss: 0.014196210540831089, Test Loss: 0.06973627209663391\n",
      "Epoch 8499, Train Loss: 0.014192936941981316, Test Loss: 0.06973762810230255\n",
      "Epoch 8500, Train Loss: 0.014190617017447948, Test Loss: 0.06973056495189667\n",
      "Epoch 8501, Train Loss: 0.014187468215823174, Test Loss: 0.069737508893013\n",
      "Epoch 8502, Train Loss: 0.014184793457388878, Test Loss: 0.06973950564861298\n",
      "Epoch 8503, Train Loss: 0.014181727543473244, Test Loss: 0.06974270194768906\n",
      "Epoch 8504, Train Loss: 0.014179871417582035, Test Loss: 0.06972217559814453\n",
      "Epoch 8505, Train Loss: 0.014176527969539165, Test Loss: 0.0697271078824997\n",
      "Epoch 8506, Train Loss: 0.014173343777656555, Test Loss: 0.06972772628068924\n",
      "Epoch 8507, Train Loss: 0.014171098358929157, Test Loss: 0.06970982998609543\n",
      "Epoch 8508, Train Loss: 0.014168481342494488, Test Loss: 0.06970928609371185\n",
      "Epoch 8509, Train Loss: 0.01416538842022419, Test Loss: 0.06971944868564606\n",
      "Epoch 8510, Train Loss: 0.014162270352244377, Test Loss: 0.06971236318349838\n",
      "Epoch 8511, Train Loss: 0.014159252867102623, Test Loss: 0.06972674280405045\n",
      "Epoch 8512, Train Loss: 0.01415659487247467, Test Loss: 0.06971874088048935\n",
      "Epoch 8513, Train Loss: 0.014153910800814629, Test Loss: 0.06971485167741776\n",
      "Epoch 8514, Train Loss: 0.014151236042380333, Test Loss: 0.06971579790115356\n",
      "Epoch 8515, Train Loss: 0.014148300513625145, Test Loss: 0.06971356272697449\n",
      "Epoch 8516, Train Loss: 0.014145647175610065, Test Loss: 0.06972050666809082\n",
      "Epoch 8517, Train Loss: 0.014142745174467564, Test Loss: 0.06971079111099243\n",
      "Epoch 8518, Train Loss: 0.014139775186777115, Test Loss: 0.0697116032242775\n",
      "Epoch 8519, Train Loss: 0.01413763128221035, Test Loss: 0.06970323622226715\n",
      "Epoch 8520, Train Loss: 0.014134547673165798, Test Loss: 0.06970192492008209\n",
      "Epoch 8521, Train Loss: 0.014131594449281693, Test Loss: 0.06969641149044037\n",
      "Epoch 8522, Train Loss: 0.01412864588201046, Test Loss: 0.06969202309846878\n",
      "Epoch 8523, Train Loss: 0.014125863090157509, Test Loss: 0.06968719512224197\n",
      "Epoch 8524, Train Loss: 0.01412269938737154, Test Loss: 0.06969386339187622\n",
      "Epoch 8525, Train Loss: 0.014120209030807018, Test Loss: 0.06968716531991959\n",
      "Epoch 8526, Train Loss: 0.014117224141955376, Test Loss: 0.06968872994184494\n",
      "Epoch 8527, Train Loss: 0.014115409925580025, Test Loss: 0.06967900693416595\n",
      "Epoch 8528, Train Loss: 0.01411202922463417, Test Loss: 0.0696825310587883\n",
      "Epoch 8529, Train Loss: 0.014109128154814243, Test Loss: 0.06967988610267639\n",
      "Epoch 8530, Train Loss: 0.014106307178735733, Test Loss: 0.06967491656541824\n",
      "Epoch 8531, Train Loss: 0.014103548601269722, Test Loss: 0.069674052298069\n",
      "Epoch 8532, Train Loss: 0.014100522734224796, Test Loss: 0.06968318670988083\n",
      "Epoch 8533, Train Loss: 0.01409772876650095, Test Loss: 0.06967484205961227\n",
      "Epoch 8534, Train Loss: 0.01409570686519146, Test Loss: 0.06968630105257034\n",
      "Epoch 8535, Train Loss: 0.014092001132667065, Test Loss: 0.0696813240647316\n",
      "Epoch 8536, Train Loss: 0.014089157804846764, Test Loss: 0.0696835070848465\n",
      "Epoch 8537, Train Loss: 0.014086687006056309, Test Loss: 0.06967578828334808\n",
      "Epoch 8538, Train Loss: 0.014084059745073318, Test Loss: 0.0696754977107048\n",
      "Epoch 8539, Train Loss: 0.014080889523029327, Test Loss: 0.06968077272176743\n",
      "Epoch 8540, Train Loss: 0.014078522101044655, Test Loss: 0.06966903805732727\n",
      "Epoch 8541, Train Loss: 0.014075392857193947, Test Loss: 0.06967281550168991\n",
      "Epoch 8542, Train Loss: 0.014072619378566742, Test Loss: 0.06966432183980942\n",
      "Epoch 8543, Train Loss: 0.014069592580199242, Test Loss: 0.06966409832239151\n",
      "Epoch 8544, Train Loss: 0.014067101292312145, Test Loss: 0.06965604424476624\n",
      "Epoch 8545, Train Loss: 0.014064247719943523, Test Loss: 0.06965643912553787\n",
      "Epoch 8546, Train Loss: 0.014061683788895607, Test Loss: 0.06965666264295578\n",
      "Epoch 8547, Train Loss: 0.014058811590075493, Test Loss: 0.0696667805314064\n",
      "Epoch 8548, Train Loss: 0.014055704697966576, Test Loss: 0.06965422630310059\n",
      "Epoch 8549, Train Loss: 0.01405311282724142, Test Loss: 0.06965077668428421\n",
      "Epoch 8550, Train Loss: 0.014050504192709923, Test Loss: 0.06965911388397217\n",
      "Epoch 8551, Train Loss: 0.014047612436115742, Test Loss: 0.06964629143476486\n",
      "Epoch 8552, Train Loss: 0.014044673182070255, Test Loss: 0.06965039670467377\n",
      "Epoch 8553, Train Loss: 0.01404272299259901, Test Loss: 0.06964217126369476\n",
      "Epoch 8554, Train Loss: 0.014039360918104649, Test Loss: 0.06964454799890518\n",
      "Epoch 8555, Train Loss: 0.01403631828725338, Test Loss: 0.06964369863271713\n",
      "Epoch 8556, Train Loss: 0.014033818617463112, Test Loss: 0.06963489949703217\n",
      "Epoch 8557, Train Loss: 0.014030729420483112, Test Loss: 0.06964019685983658\n",
      "Epoch 8558, Train Loss: 0.014028439298272133, Test Loss: 0.06963412463665009\n",
      "Epoch 8559, Train Loss: 0.014025446958839893, Test Loss: 0.06962645798921585\n",
      "Epoch 8560, Train Loss: 0.014022520743310452, Test Loss: 0.06962868571281433\n",
      "Epoch 8561, Train Loss: 0.014019732363522053, Test Loss: 0.06963271647691727\n",
      "Epoch 8562, Train Loss: 0.014016922563314438, Test Loss: 0.06963708996772766\n",
      "Epoch 8563, Train Loss: 0.014014150947332382, Test Loss: 0.06962136179208755\n",
      "Epoch 8564, Train Loss: 0.014012415893375874, Test Loss: 0.06962615996599197\n",
      "Epoch 8565, Train Loss: 0.014009261503815651, Test Loss: 0.0696321576833725\n",
      "Epoch 8566, Train Loss: 0.014006190933287144, Test Loss: 0.06962352246046066\n",
      "Epoch 8567, Train Loss: 0.014002968557178974, Test Loss: 0.0696244165301323\n",
      "Epoch 8568, Train Loss: 0.014000264927744865, Test Loss: 0.06962261348962784\n",
      "Epoch 8569, Train Loss: 0.013997417874634266, Test Loss: 0.06962541490793228\n",
      "Epoch 8570, Train Loss: 0.013995595276355743, Test Loss: 0.06962139904499054\n",
      "Epoch 8571, Train Loss: 0.013992167077958584, Test Loss: 0.06962189078330994\n",
      "Epoch 8572, Train Loss: 0.013989206403493881, Test Loss: 0.06961877644062042\n",
      "Epoch 8573, Train Loss: 0.013987039215862751, Test Loss: 0.06961491703987122\n",
      "Epoch 8574, Train Loss: 0.013984180055558681, Test Loss: 0.0696084052324295\n",
      "Epoch 8575, Train Loss: 0.013981674797832966, Test Loss: 0.06959867477416992\n",
      "Epoch 8576, Train Loss: 0.013978234492242336, Test Loss: 0.06960315257310867\n",
      "Epoch 8577, Train Loss: 0.013975235633552074, Test Loss: 0.06961169093847275\n",
      "Epoch 8578, Train Loss: 0.013972821645438671, Test Loss: 0.0696181207895279\n",
      "Epoch 8579, Train Loss: 0.013970516622066498, Test Loss: 0.06961993128061295\n",
      "Epoch 8580, Train Loss: 0.013967323116958141, Test Loss: 0.06960665434598923\n",
      "Epoch 8581, Train Loss: 0.01396449375897646, Test Loss: 0.06960274279117584\n",
      "Epoch 8582, Train Loss: 0.013961805030703545, Test Loss: 0.0695989727973938\n",
      "Epoch 8583, Train Loss: 0.013958853669464588, Test Loss: 0.06959641724824905\n",
      "Epoch 8584, Train Loss: 0.013955817557871342, Test Loss: 0.0695931613445282\n",
      "Epoch 8585, Train Loss: 0.013953444547951221, Test Loss: 0.06959022581577301\n",
      "Epoch 8586, Train Loss: 0.013950558379292488, Test Loss: 0.06959015130996704\n",
      "Epoch 8587, Train Loss: 0.013947740197181702, Test Loss: 0.06958764791488647\n",
      "Epoch 8588, Train Loss: 0.01394512876868248, Test Loss: 0.0695849284529686\n",
      "Epoch 8589, Train Loss: 0.013942601159214973, Test Loss: 0.06959129869937897\n",
      "Epoch 8590, Train Loss: 0.013940124772489071, Test Loss: 0.06958194822072983\n",
      "Epoch 8591, Train Loss: 0.013936777599155903, Test Loss: 0.06957634538412094\n",
      "Epoch 8592, Train Loss: 0.013934390619397163, Test Loss: 0.06958445906639099\n",
      "Epoch 8593, Train Loss: 0.01393121201545, Test Loss: 0.06956926733255386\n",
      "Epoch 8594, Train Loss: 0.013928590342402458, Test Loss: 0.06956308335065842\n",
      "Epoch 8595, Train Loss: 0.013925629667937756, Test Loss: 0.06956636160612106\n",
      "Epoch 8596, Train Loss: 0.013923018239438534, Test Loss: 0.06957504153251648\n",
      "Epoch 8597, Train Loss: 0.013920025900006294, Test Loss: 0.06957989931106567\n",
      "Epoch 8598, Train Loss: 0.01391721423715353, Test Loss: 0.0695655569434166\n",
      "Epoch 8599, Train Loss: 0.013914446346461773, Test Loss: 0.06956247240304947\n",
      "Epoch 8600, Train Loss: 0.01391187310218811, Test Loss: 0.06956551223993301\n",
      "Epoch 8601, Train Loss: 0.013909297063946724, Test Loss: 0.06956006586551666\n",
      "Epoch 8602, Train Loss: 0.013906201347708702, Test Loss: 0.06956054270267487\n",
      "Epoch 8603, Train Loss: 0.013903466053307056, Test Loss: 0.06956014782190323\n",
      "Epoch 8604, Train Loss: 0.013900776393711567, Test Loss: 0.06956062465906143\n",
      "Epoch 8605, Train Loss: 0.013897980563342571, Test Loss: 0.06955602020025253\n",
      "Epoch 8606, Train Loss: 0.013895135372877121, Test Loss: 0.06955103576183319\n",
      "Epoch 8607, Train Loss: 0.013892593793570995, Test Loss: 0.0695454329252243\n",
      "Epoch 8608, Train Loss: 0.013890271075069904, Test Loss: 0.069551981985569\n",
      "Epoch 8609, Train Loss: 0.01388712041079998, Test Loss: 0.06955668330192566\n",
      "Epoch 8610, Train Loss: 0.013884364627301693, Test Loss: 0.06955259293317795\n",
      "Epoch 8611, Train Loss: 0.01388182770460844, Test Loss: 0.0695548728108406\n",
      "Epoch 8612, Train Loss: 0.013879221864044666, Test Loss: 0.06955111026763916\n",
      "Epoch 8613, Train Loss: 0.013876046054065228, Test Loss: 0.06954923272132874\n",
      "Epoch 8614, Train Loss: 0.013873574323952198, Test Loss: 0.0695548951625824\n",
      "Epoch 8615, Train Loss: 0.01387071330100298, Test Loss: 0.06955160200595856\n",
      "Epoch 8616, Train Loss: 0.013868078589439392, Test Loss: 0.06953325122594833\n",
      "Epoch 8617, Train Loss: 0.013866590335965157, Test Loss: 0.0695241168141365\n",
      "Epoch 8618, Train Loss: 0.013863458298146725, Test Loss: 0.06952980160713196\n",
      "Epoch 8619, Train Loss: 0.013859537430107594, Test Loss: 0.0695323497056961\n",
      "Epoch 8620, Train Loss: 0.013856932520866394, Test Loss: 0.06952948123216629\n",
      "Epoch 8621, Train Loss: 0.013853758573532104, Test Loss: 0.06953048706054688\n",
      "Epoch 8622, Train Loss: 0.013851432129740715, Test Loss: 0.06952110677957535\n",
      "Epoch 8623, Train Loss: 0.013848566450178623, Test Loss: 0.06951751559972763\n",
      "Epoch 8624, Train Loss: 0.013845670968294144, Test Loss: 0.06952568143606186\n",
      "Epoch 8625, Train Loss: 0.013843058608472347, Test Loss: 0.06951671838760376\n",
      "Epoch 8626, Train Loss: 0.01384060364216566, Test Loss: 0.06951335817575455\n",
      "Epoch 8627, Train Loss: 0.013838174752891064, Test Loss: 0.06950943917036057\n",
      "Epoch 8628, Train Loss: 0.013834751211106777, Test Loss: 0.0695144459605217\n",
      "Epoch 8629, Train Loss: 0.013832289725542068, Test Loss: 0.06952403485774994\n",
      "Epoch 8630, Train Loss: 0.013829328119754791, Test Loss: 0.06950732320547104\n",
      "Epoch 8631, Train Loss: 0.013826422393321991, Test Loss: 0.06950791925191879\n",
      "Epoch 8632, Train Loss: 0.01382387150079012, Test Loss: 0.06951519846916199\n",
      "Epoch 8633, Train Loss: 0.013821044005453587, Test Loss: 0.06951871514320374\n",
      "Epoch 8634, Train Loss: 0.013818244449794292, Test Loss: 0.06950879096984863\n",
      "Epoch 8635, Train Loss: 0.01381556037813425, Test Loss: 0.06950853019952774\n",
      "Epoch 8636, Train Loss: 0.013813208788633347, Test Loss: 0.06949139386415482\n",
      "Epoch 8637, Train Loss: 0.01380995661020279, Test Loss: 0.06949406862258911\n",
      "Epoch 8638, Train Loss: 0.01380751933902502, Test Loss: 0.0694957748055458\n",
      "Epoch 8639, Train Loss: 0.013804502785205841, Test Loss: 0.06950290501117706\n",
      "Epoch 8640, Train Loss: 0.013801958411931992, Test Loss: 0.0694890171289444\n",
      "Epoch 8641, Train Loss: 0.013800029642879963, Test Loss: 0.06947677582502365\n",
      "Epoch 8642, Train Loss: 0.01379649993032217, Test Loss: 0.06947781890630722\n",
      "Epoch 8643, Train Loss: 0.013793709687888622, Test Loss: 0.06948298215866089\n",
      "Epoch 8644, Train Loss: 0.013790963217616081, Test Loss: 0.06948495656251907\n",
      "Epoch 8645, Train Loss: 0.013787991367280483, Test Loss: 0.06948764622211456\n",
      "Epoch 8646, Train Loss: 0.013785290531814098, Test Loss: 0.06948357075452805\n",
      "Epoch 8647, Train Loss: 0.013782684691250324, Test Loss: 0.06947603076696396\n",
      "Epoch 8648, Train Loss: 0.0137800183147192, Test Loss: 0.06947647780179977\n",
      "Epoch 8649, Train Loss: 0.013777134008705616, Test Loss: 0.06948080658912659\n",
      "Epoch 8650, Train Loss: 0.013774443417787552, Test Loss: 0.0694836750626564\n",
      "Epoch 8651, Train Loss: 0.013771695084869862, Test Loss: 0.06947607547044754\n",
      "Epoch 8652, Train Loss: 0.013769191689789295, Test Loss: 0.06946291029453278\n",
      "Epoch 8653, Train Loss: 0.01376629900187254, Test Loss: 0.06946496665477753\n",
      "Epoch 8654, Train Loss: 0.01376342959702015, Test Loss: 0.06946493685245514\n",
      "Epoch 8655, Train Loss: 0.01376069150865078, Test Loss: 0.06946172565221786\n",
      "Epoch 8656, Train Loss: 0.013758123852312565, Test Loss: 0.0694720670580864\n",
      "Epoch 8657, Train Loss: 0.013755406253039837, Test Loss: 0.06946278363466263\n",
      "Epoch 8658, Train Loss: 0.013753785751760006, Test Loss: 0.06945554167032242\n",
      "Epoch 8659, Train Loss: 0.01375043485313654, Test Loss: 0.06945300847291946\n",
      "Epoch 8660, Train Loss: 0.013747269287705421, Test Loss: 0.06946344673633575\n",
      "Epoch 8661, Train Loss: 0.01374486181885004, Test Loss: 0.06944925338029861\n",
      "Epoch 8662, Train Loss: 0.013742711395025253, Test Loss: 0.06943453103303909\n",
      "Epoch 8663, Train Loss: 0.013738973066210747, Test Loss: 0.06944514811038971\n",
      "Epoch 8664, Train Loss: 0.013736093416810036, Test Loss: 0.06945543736219406\n",
      "Epoch 8665, Train Loss: 0.013733377680182457, Test Loss: 0.06945157051086426\n",
      "Epoch 8666, Train Loss: 0.013731446117162704, Test Loss: 0.06943606585264206\n",
      "Epoch 8667, Train Loss: 0.013728641904890537, Test Loss: 0.06942915171384811\n",
      "Epoch 8668, Train Loss: 0.013725485652685165, Test Loss: 0.0694349929690361\n",
      "Epoch 8669, Train Loss: 0.013722687028348446, Test Loss: 0.06943819671869278\n",
      "Epoch 8670, Train Loss: 0.01371996384114027, Test Loss: 0.06944111734628677\n",
      "Epoch 8671, Train Loss: 0.013717728666961193, Test Loss: 0.0694376677274704\n",
      "Epoch 8672, Train Loss: 0.013714543543756008, Test Loss: 0.06945082545280457\n",
      "Epoch 8673, Train Loss: 0.013711636886000633, Test Loss: 0.0694444328546524\n",
      "Epoch 8674, Train Loss: 0.013709169812500477, Test Loss: 0.06944407522678375\n",
      "Epoch 8675, Train Loss: 0.013706539757549763, Test Loss: 0.06943802535533905\n",
      "Epoch 8676, Train Loss: 0.013704156503081322, Test Loss: 0.06943198293447495\n",
      "Epoch 8677, Train Loss: 0.01370249968022108, Test Loss: 0.06942768394947052\n",
      "Epoch 8678, Train Loss: 0.013698296621441841, Test Loss: 0.06943408399820328\n",
      "Epoch 8679, Train Loss: 0.013695565052330494, Test Loss: 0.06942154467105865\n",
      "Epoch 8680, Train Loss: 0.013693006709218025, Test Loss: 0.06941855698823929\n",
      "Epoch 8681, Train Loss: 0.013690114952623844, Test Loss: 0.06942399591207504\n",
      "Epoch 8682, Train Loss: 0.01368789654225111, Test Loss: 0.06940800696611404\n",
      "Epoch 8683, Train Loss: 0.013684984296560287, Test Loss: 0.06940881162881851\n",
      "Epoch 8684, Train Loss: 0.013682181015610695, Test Loss: 0.06941202282905579\n",
      "Epoch 8685, Train Loss: 0.013679361902177334, Test Loss: 0.06941253691911697\n",
      "Epoch 8686, Train Loss: 0.013676976785063744, Test Loss: 0.06941266357898712\n",
      "Epoch 8687, Train Loss: 0.013674065470695496, Test Loss: 0.0694039836525917\n",
      "Epoch 8688, Train Loss: 0.01367101538926363, Test Loss: 0.06941502541303635\n",
      "Epoch 8689, Train Loss: 0.013668406754732132, Test Loss: 0.06942108273506165\n",
      "Epoch 8690, Train Loss: 0.013665979728102684, Test Loss: 0.06942646205425262\n",
      "Epoch 8691, Train Loss: 0.013663187623023987, Test Loss: 0.06942146271467209\n",
      "Epoch 8692, Train Loss: 0.01366057712584734, Test Loss: 0.06940451264381409\n",
      "Epoch 8693, Train Loss: 0.013658222742378712, Test Loss: 0.06940881162881851\n",
      "Epoch 8694, Train Loss: 0.013654918409883976, Test Loss: 0.06941896677017212\n",
      "Epoch 8695, Train Loss: 0.01365220919251442, Test Loss: 0.06940759718418121\n",
      "Epoch 8696, Train Loss: 0.013649805448949337, Test Loss: 0.06940784305334091\n",
      "Epoch 8697, Train Loss: 0.013646994717419147, Test Loss: 0.06940556317567825\n",
      "Epoch 8698, Train Loss: 0.013644716702401638, Test Loss: 0.06939692795276642\n",
      "Epoch 8699, Train Loss: 0.01364212017506361, Test Loss: 0.06939596682786942\n",
      "Epoch 8700, Train Loss: 0.013638656586408615, Test Loss: 0.06938844174146652\n",
      "Epoch 8701, Train Loss: 0.013636085204780102, Test Loss: 0.06938038766384125\n",
      "Epoch 8702, Train Loss: 0.013633417896926403, Test Loss: 0.06938809901475906\n",
      "Epoch 8703, Train Loss: 0.01363102812319994, Test Loss: 0.06938392668962479\n",
      "Epoch 8704, Train Loss: 0.013627945445477962, Test Loss: 0.06939183175563812\n",
      "Epoch 8705, Train Loss: 0.013625681400299072, Test Loss: 0.06938198953866959\n",
      "Epoch 8706, Train Loss: 0.013622654601931572, Test Loss: 0.06938337534666061\n",
      "Epoch 8707, Train Loss: 0.013620475307106972, Test Loss: 0.06938078999519348\n",
      "Epoch 8708, Train Loss: 0.013617527671158314, Test Loss: 0.06938064098358154\n",
      "Epoch 8709, Train Loss: 0.013615080155432224, Test Loss: 0.06938272714614868\n",
      "Epoch 8710, Train Loss: 0.01361179817467928, Test Loss: 0.06938415765762329\n",
      "Epoch 8711, Train Loss: 0.01360910851508379, Test Loss: 0.06937239319086075\n",
      "Epoch 8712, Train Loss: 0.013606373220682144, Test Loss: 0.06937344372272491\n",
      "Epoch 8713, Train Loss: 0.013603848405182362, Test Loss: 0.06936665624380112\n",
      "Epoch 8714, Train Loss: 0.013601504266262054, Test Loss: 0.06936948001384735\n",
      "Epoch 8715, Train Loss: 0.013598896563053131, Test Loss: 0.06936247646808624\n",
      "Epoch 8716, Train Loss: 0.013595983386039734, Test Loss: 0.06934938579797745\n",
      "Epoch 8717, Train Loss: 0.013592837378382683, Test Loss: 0.06935875117778778\n",
      "Epoch 8718, Train Loss: 0.01359058078378439, Test Loss: 0.06935455650091171\n",
      "Epoch 8719, Train Loss: 0.01358761079609394, Test Loss: 0.06935355812311172\n",
      "Epoch 8720, Train Loss: 0.013584982603788376, Test Loss: 0.06935908645391464\n",
      "Epoch 8721, Train Loss: 0.013582369312644005, Test Loss: 0.06934908777475357\n",
      "Epoch 8722, Train Loss: 0.01357952132821083, Test Loss: 0.0693485215306282\n",
      "Epoch 8723, Train Loss: 0.013576668687164783, Test Loss: 0.06934988498687744\n",
      "Epoch 8724, Train Loss: 0.013574125245213509, Test Loss: 0.06934620440006256\n",
      "Epoch 8725, Train Loss: 0.013571545481681824, Test Loss: 0.06934804469347\n",
      "Epoch 8726, Train Loss: 0.013568882830440998, Test Loss: 0.06933785229921341\n",
      "Epoch 8727, Train Loss: 0.013565817847847939, Test Loss: 0.06934375315904617\n",
      "Epoch 8728, Train Loss: 0.013563237152993679, Test Loss: 0.06934288144111633\n",
      "Epoch 8729, Train Loss: 0.013560600578784943, Test Loss: 0.06934098154306412\n",
      "Epoch 8730, Train Loss: 0.013558214530348778, Test Loss: 0.06933344155550003\n",
      "Epoch 8731, Train Loss: 0.013555106706917286, Test Loss: 0.06934402883052826\n",
      "Epoch 8732, Train Loss: 0.01355247013270855, Test Loss: 0.06933984905481339\n",
      "Epoch 8733, Train Loss: 0.013549746945500374, Test Loss: 0.0693415030837059\n",
      "Epoch 8734, Train Loss: 0.013547316193580627, Test Loss: 0.06933696568012238\n",
      "Epoch 8735, Train Loss: 0.01354582142084837, Test Loss: 0.06932762265205383\n",
      "Epoch 8736, Train Loss: 0.013541985303163528, Test Loss: 0.06934259086847305\n",
      "Epoch 8737, Train Loss: 0.013539145700633526, Test Loss: 0.06933457404375076\n",
      "Epoch 8738, Train Loss: 0.013536347076296806, Test Loss: 0.06933752447366714\n",
      "Epoch 8739, Train Loss: 0.013533501885831356, Test Loss: 0.06933248788118362\n",
      "Epoch 8740, Train Loss: 0.013531429693102837, Test Loss: 0.06931957602500916\n",
      "Epoch 8741, Train Loss: 0.013528275303542614, Test Loss: 0.06932025402784348\n",
      "Epoch 8742, Train Loss: 0.013525715097784996, Test Loss: 0.06933195888996124\n",
      "Epoch 8743, Train Loss: 0.013523814268410206, Test Loss: 0.06932374089956284\n",
      "Epoch 8744, Train Loss: 0.013520398177206516, Test Loss: 0.06931460648775101\n",
      "Epoch 8745, Train Loss: 0.013517548330128193, Test Loss: 0.06931958347558975\n",
      "Epoch 8746, Train Loss: 0.013514803722500801, Test Loss: 0.06931579113006592\n",
      "Epoch 8747, Train Loss: 0.013512255623936653, Test Loss: 0.0693272277712822\n",
      "Epoch 8748, Train Loss: 0.013509413227438927, Test Loss: 0.06931459158658981\n",
      "Epoch 8749, Train Loss: 0.013506630435585976, Test Loss: 0.06932203471660614\n",
      "Epoch 8750, Train Loss: 0.013504225760698318, Test Loss: 0.06930834800004959\n",
      "Epoch 8751, Train Loss: 0.013501902110874653, Test Loss: 0.06930235773324966\n",
      "Epoch 8752, Train Loss: 0.013499163091182709, Test Loss: 0.06930658966302872\n",
      "Epoch 8753, Train Loss: 0.013496780768036842, Test Loss: 0.06929375231266022\n",
      "Epoch 8754, Train Loss: 0.013493462465703487, Test Loss: 0.06929636746644974\n",
      "Epoch 8755, Train Loss: 0.013490916229784489, Test Loss: 0.0692976638674736\n",
      "Epoch 8756, Train Loss: 0.013488692231476307, Test Loss: 0.06928765028715134\n",
      "Epoch 8757, Train Loss: 0.013485160656273365, Test Loss: 0.06929752230644226\n",
      "Epoch 8758, Train Loss: 0.013482512906193733, Test Loss: 0.0692949965596199\n",
      "Epoch 8759, Train Loss: 0.013479952700436115, Test Loss: 0.06929028034210205\n",
      "Epoch 8760, Train Loss: 0.013477237895131111, Test Loss: 0.06929609179496765\n",
      "Epoch 8761, Train Loss: 0.013474761508405209, Test Loss: 0.06928861141204834\n",
      "Epoch 8762, Train Loss: 0.013472081162035465, Test Loss: 0.06928848475217819\n",
      "Epoch 8763, Train Loss: 0.01346936821937561, Test Loss: 0.06928575783967972\n",
      "Epoch 8764, Train Loss: 0.013467012904584408, Test Loss: 0.06928351521492004\n",
      "Epoch 8765, Train Loss: 0.013464470393955708, Test Loss: 0.06929092109203339\n",
      "Epoch 8766, Train Loss: 0.013461303897202015, Test Loss: 0.06928685307502747\n",
      "Epoch 8767, Train Loss: 0.013459241017699242, Test Loss: 0.0692957267165184\n",
      "Epoch 8768, Train Loss: 0.013456160202622414, Test Loss: 0.0692831426858902\n",
      "Epoch 8769, Train Loss: 0.01345338486135006, Test Loss: 0.06927476823329926\n",
      "Epoch 8770, Train Loss: 0.013450480066239834, Test Loss: 0.0692829042673111\n",
      "Epoch 8771, Train Loss: 0.01344777550548315, Test Loss: 0.0692838653922081\n",
      "Epoch 8772, Train Loss: 0.013445263728499413, Test Loss: 0.0692824274301529\n",
      "Epoch 8773, Train Loss: 0.013442503288388252, Test Loss: 0.06927662342786789\n",
      "Epoch 8774, Train Loss: 0.013440270908176899, Test Loss: 0.0692758709192276\n",
      "Epoch 8775, Train Loss: 0.013437374494969845, Test Loss: 0.06926392018795013\n",
      "Epoch 8776, Train Loss: 0.01343444548547268, Test Loss: 0.06925603002309799\n",
      "Epoch 8777, Train Loss: 0.013431854546070099, Test Loss: 0.06926243752241135\n",
      "Epoch 8778, Train Loss: 0.013428857550024986, Test Loss: 0.06925555318593979\n",
      "Epoch 8779, Train Loss: 0.013426929712295532, Test Loss: 0.06923925876617432\n",
      "Epoch 8780, Train Loss: 0.013423689641058445, Test Loss: 0.06924661248922348\n",
      "Epoch 8781, Train Loss: 0.013421382755041122, Test Loss: 0.0692484900355339\n",
      "Epoch 8782, Train Loss: 0.013418312184512615, Test Loss: 0.06925204396247864\n",
      "Epoch 8783, Train Loss: 0.013415728695690632, Test Loss: 0.06924912333488464\n",
      "Epoch 8784, Train Loss: 0.013412857428193092, Test Loss: 0.06926041096448898\n",
      "Epoch 8785, Train Loss: 0.013410273008048534, Test Loss: 0.06925009191036224\n",
      "Epoch 8786, Train Loss: 0.013408586382865906, Test Loss: 0.0692363753914833\n",
      "Epoch 8787, Train Loss: 0.013405393809080124, Test Loss: 0.0692429393529892\n",
      "Epoch 8788, Train Loss: 0.013402299955487251, Test Loss: 0.06923946738243103\n",
      "Epoch 8789, Train Loss: 0.01339977327734232, Test Loss: 0.06923072785139084\n",
      "Epoch 8790, Train Loss: 0.013396691530942917, Test Loss: 0.06924136728048325\n",
      "Epoch 8791, Train Loss: 0.013394591398537159, Test Loss: 0.06922763586044312\n",
      "Epoch 8792, Train Loss: 0.013391485437750816, Test Loss: 0.06922929733991623\n",
      "Epoch 8793, Train Loss: 0.01338900439441204, Test Loss: 0.06922262907028198\n",
      "Epoch 8794, Train Loss: 0.013385966420173645, Test Loss: 0.06923048943281174\n",
      "Epoch 8795, Train Loss: 0.013383573852479458, Test Loss: 0.06922128796577454\n",
      "Epoch 8796, Train Loss: 0.013380615040659904, Test Loss: 0.06922534853219986\n",
      "Epoch 8797, Train Loss: 0.013378347270190716, Test Loss: 0.06923199445009232\n",
      "Epoch 8798, Train Loss: 0.013375922106206417, Test Loss: 0.06922943145036697\n",
      "Epoch 8799, Train Loss: 0.01337309181690216, Test Loss: 0.06922248005867004\n",
      "Epoch 8800, Train Loss: 0.01336998213082552, Test Loss: 0.06922415643930435\n",
      "Epoch 8801, Train Loss: 0.013367285951972008, Test Loss: 0.06922530382871628\n",
      "Epoch 8802, Train Loss: 0.013364648446440697, Test Loss: 0.06922530382871628\n",
      "Epoch 8803, Train Loss: 0.013362026773393154, Test Loss: 0.06922034174203873\n",
      "Epoch 8804, Train Loss: 0.013359732925891876, Test Loss: 0.0692196935415268\n",
      "Epoch 8805, Train Loss: 0.013356714509427547, Test Loss: 0.06921901553869247\n",
      "Epoch 8806, Train Loss: 0.013354011811316013, Test Loss: 0.0692179873585701\n",
      "Epoch 8807, Train Loss: 0.013351315632462502, Test Loss: 0.06921958178281784\n",
      "Epoch 8808, Train Loss: 0.013349019922316074, Test Loss: 0.06922179460525513\n",
      "Epoch 8809, Train Loss: 0.013346070423722267, Test Loss: 0.06921740621328354\n",
      "Epoch 8810, Train Loss: 0.013343450613319874, Test Loss: 0.06921467185020447\n",
      "Epoch 8811, Train Loss: 0.013340742327272892, Test Loss: 0.06920211762189865\n",
      "Epoch 8812, Train Loss: 0.013338034972548485, Test Loss: 0.06920398026704788\n",
      "Epoch 8813, Train Loss: 0.01333556231111288, Test Loss: 0.06919984519481659\n",
      "Epoch 8814, Train Loss: 0.013333589769899845, Test Loss: 0.06918469816446304\n",
      "Epoch 8815, Train Loss: 0.013329928740859032, Test Loss: 0.06919952481985092\n",
      "Epoch 8816, Train Loss: 0.013327456079423428, Test Loss: 0.06919454038143158\n",
      "Epoch 8817, Train Loss: 0.01332533173263073, Test Loss: 0.06918754428625107\n",
      "Epoch 8818, Train Loss: 0.013322471641004086, Test Loss: 0.06918381154537201\n",
      "Epoch 8819, Train Loss: 0.013319774530827999, Test Loss: 0.06917952001094818\n",
      "Epoch 8820, Train Loss: 0.01331658847630024, Test Loss: 0.06918232142925262\n",
      "Epoch 8821, Train Loss: 0.013313955627381802, Test Loss: 0.06918881088495255\n",
      "Epoch 8822, Train Loss: 0.013311865739524364, Test Loss: 0.06919258087873459\n",
      "Epoch 8823, Train Loss: 0.013308589346706867, Test Loss: 0.0691869780421257\n",
      "Epoch 8824, Train Loss: 0.01330616232007742, Test Loss: 0.06918593496084213\n",
      "Epoch 8825, Train Loss: 0.013303155079483986, Test Loss: 0.06918161362409592\n",
      "Epoch 8826, Train Loss: 0.013300854712724686, Test Loss: 0.06917926669120789\n",
      "Epoch 8827, Train Loss: 0.013297897763550282, Test Loss: 0.06918454170227051\n",
      "Epoch 8828, Train Loss: 0.013295218348503113, Test Loss: 0.06918779015541077\n",
      "Epoch 8829, Train Loss: 0.013292786665260792, Test Loss: 0.06917896121740341\n",
      "Epoch 8830, Train Loss: 0.013290224596858025, Test Loss: 0.0691823735833168\n",
      "Epoch 8831, Train Loss: 0.013287398032844067, Test Loss: 0.06917449831962585\n",
      "Epoch 8832, Train Loss: 0.013284659944474697, Test Loss: 0.06916515529155731\n",
      "Epoch 8833, Train Loss: 0.01328230556100607, Test Loss: 0.06915716826915741\n",
      "Epoch 8834, Train Loss: 0.013279525563120842, Test Loss: 0.06916195899248123\n",
      "Epoch 8835, Train Loss: 0.013276953250169754, Test Loss: 0.069157674908638\n",
      "Epoch 8836, Train Loss: 0.013274266384541988, Test Loss: 0.069161057472229\n",
      "Epoch 8837, Train Loss: 0.013271385803818703, Test Loss: 0.06915945559740067\n",
      "Epoch 8838, Train Loss: 0.013268944807350636, Test Loss: 0.06916391104459763\n",
      "Epoch 8839, Train Loss: 0.013266478665173054, Test Loss: 0.06914643943309784\n",
      "Epoch 8840, Train Loss: 0.013263617642223835, Test Loss: 0.06914752721786499\n",
      "Epoch 8841, Train Loss: 0.013261343352496624, Test Loss: 0.06915631890296936\n",
      "Epoch 8842, Train Loss: 0.013259246945381165, Test Loss: 0.06914063543081284\n",
      "Epoch 8843, Train Loss: 0.01325557753443718, Test Loss: 0.06914923340082169\n",
      "Epoch 8844, Train Loss: 0.013253351673483849, Test Loss: 0.06914041936397552\n",
      "Epoch 8845, Train Loss: 0.013250095769762993, Test Loss: 0.06915291398763657\n",
      "Epoch 8846, Train Loss: 0.013247599825263023, Test Loss: 0.0691499412059784\n",
      "Epoch 8847, Train Loss: 0.013245203532278538, Test Loss: 0.06915351003408432\n",
      "Epoch 8848, Train Loss: 0.01324258092790842, Test Loss: 0.06913448125123978\n",
      "Epoch 8849, Train Loss: 0.01323962677270174, Test Loss: 0.06913449615240097\n",
      "Epoch 8850, Train Loss: 0.013237274251878262, Test Loss: 0.0691317766904831\n",
      "Epoch 8851, Train Loss: 0.01323474757373333, Test Loss: 0.06913480162620544\n",
      "Epoch 8852, Train Loss: 0.01323150098323822, Test Loss: 0.06913614273071289\n",
      "Epoch 8853, Train Loss: 0.013228924944996834, Test Loss: 0.06913116574287415\n",
      "Epoch 8854, Train Loss: 0.013226532377302647, Test Loss: 0.06912118196487427\n",
      "Epoch 8855, Train Loss: 0.01322360523045063, Test Loss: 0.0691273957490921\n",
      "Epoch 8856, Train Loss: 0.013221347704529762, Test Loss: 0.06911596655845642\n",
      "Epoch 8857, Train Loss: 0.013218567706644535, Test Loss: 0.06911791115999222\n",
      "Epoch 8858, Train Loss: 0.013216628693044186, Test Loss: 0.06912735849618912\n",
      "Epoch 8859, Train Loss: 0.013213109225034714, Test Loss: 0.06913106888532639\n",
      "Epoch 8860, Train Loss: 0.013210524804890156, Test Loss: 0.06911604851484299\n",
      "Epoch 8861, Train Loss: 0.013207634910941124, Test Loss: 0.06911720335483551\n",
      "Epoch 8862, Train Loss: 0.013205445371568203, Test Loss: 0.06912079453468323\n",
      "Epoch 8863, Train Loss: 0.013202526606619358, Test Loss: 0.06911934167146683\n",
      "Epoch 8864, Train Loss: 0.01319977268576622, Test Loss: 0.06912021338939667\n",
      "Epoch 8865, Train Loss: 0.013197747059166431, Test Loss: 0.06912419199943542\n",
      "Epoch 8866, Train Loss: 0.013195148669183254, Test Loss: 0.06912588328123093\n",
      "Epoch 8867, Train Loss: 0.013192172162234783, Test Loss: 0.0691106915473938\n",
      "Epoch 8868, Train Loss: 0.013189621269702911, Test Loss: 0.06910008937120438\n",
      "Epoch 8869, Train Loss: 0.013187719509005547, Test Loss: 0.06908924132585526\n",
      "Epoch 8870, Train Loss: 0.013184193521738052, Test Loss: 0.06909798085689545\n",
      "Epoch 8871, Train Loss: 0.013181442394852638, Test Loss: 0.06910216063261032\n",
      "Epoch 8872, Train Loss: 0.013179993256926537, Test Loss: 0.06909701973199844\n",
      "Epoch 8873, Train Loss: 0.013176463544368744, Test Loss: 0.06909161061048508\n",
      "Epoch 8874, Train Loss: 0.013173828832805157, Test Loss: 0.06908716261386871\n",
      "Epoch 8875, Train Loss: 0.013171364553272724, Test Loss: 0.06908494234085083\n",
      "Epoch 8876, Train Loss: 0.013168430887162685, Test Loss: 0.06908024847507477\n",
      "Epoch 8877, Train Loss: 0.013165933080017567, Test Loss: 0.06907840073108673\n",
      "Epoch 8878, Train Loss: 0.013163072988390923, Test Loss: 0.0690818652510643\n",
      "Epoch 8879, Train Loss: 0.01316090952605009, Test Loss: 0.06908673793077469\n",
      "Epoch 8880, Train Loss: 0.013158202171325684, Test Loss: 0.06908807903528214\n",
      "Epoch 8881, Train Loss: 0.01315544918179512, Test Loss: 0.06909141689538956\n",
      "Epoch 8882, Train Loss: 0.0131527753546834, Test Loss: 0.0690881758928299\n",
      "Epoch 8883, Train Loss: 0.013150394894182682, Test Loss: 0.06907548755407333\n",
      "Epoch 8884, Train Loss: 0.013147223740816116, Test Loss: 0.06908129155635834\n",
      "Epoch 8885, Train Loss: 0.013144630938768387, Test Loss: 0.0690794289112091\n",
      "Epoch 8886, Train Loss: 0.013142150826752186, Test Loss: 0.06908117979764938\n",
      "Epoch 8887, Train Loss: 0.013139918446540833, Test Loss: 0.06907761842012405\n",
      "Epoch 8888, Train Loss: 0.013137148693203926, Test Loss: 0.06907293945550919\n",
      "Epoch 8889, Train Loss: 0.013134262524545193, Test Loss: 0.06906991451978683\n",
      "Epoch 8890, Train Loss: 0.013131526298820972, Test Loss: 0.06906641274690628\n",
      "Epoch 8891, Train Loss: 0.013128957711160183, Test Loss: 0.0690801590681076\n",
      "Epoch 8892, Train Loss: 0.013126252219080925, Test Loss: 0.06906652450561523\n",
      "Epoch 8893, Train Loss: 0.013124254532158375, Test Loss: 0.06906456500291824\n",
      "Epoch 8894, Train Loss: 0.01312164030969143, Test Loss: 0.0690610483288765\n",
      "Epoch 8895, Train Loss: 0.013119397684931755, Test Loss: 0.06905485689640045\n",
      "Epoch 8896, Train Loss: 0.01311593409627676, Test Loss: 0.06906277686357498\n",
      "Epoch 8897, Train Loss: 0.01311317179352045, Test Loss: 0.069058857858181\n",
      "Epoch 8898, Train Loss: 0.013110868632793427, Test Loss: 0.0690556988120079\n",
      "Epoch 8899, Train Loss: 0.013109436258673668, Test Loss: 0.06904904544353485\n",
      "Epoch 8900, Train Loss: 0.013105486519634724, Test Loss: 0.06904271245002747\n",
      "Epoch 8901, Train Loss: 0.013103343546390533, Test Loss: 0.06905520707368851\n",
      "Epoch 8902, Train Loss: 0.013100720010697842, Test Loss: 0.0690542459487915\n",
      "Epoch 8903, Train Loss: 0.013097611255943775, Test Loss: 0.06905292719602585\n",
      "Epoch 8904, Train Loss: 0.013095013797283173, Test Loss: 0.06904879957437515\n",
      "Epoch 8905, Train Loss: 0.01309269294142723, Test Loss: 0.06904653459787369\n",
      "Epoch 8906, Train Loss: 0.01308971457183361, Test Loss: 0.06903887540102005\n",
      "Epoch 8907, Train Loss: 0.01308922003954649, Test Loss: 0.06904660165309906\n",
      "Epoch 8908, Train Loss: 0.01308541651815176, Test Loss: 0.06903472542762756\n",
      "Epoch 8909, Train Loss: 0.01308212336152792, Test Loss: 0.06902742385864258\n",
      "Epoch 8910, Train Loss: 0.013079386204481125, Test Loss: 0.06903254240751266\n",
      "Epoch 8911, Train Loss: 0.013076982460916042, Test Loss: 0.06902240216732025\n",
      "Epoch 8912, Train Loss: 0.013074276968836784, Test Loss: 0.06902384757995605\n",
      "Epoch 8913, Train Loss: 0.013071619905531406, Test Loss: 0.06902232766151428\n",
      "Epoch 8914, Train Loss: 0.013069038279354572, Test Loss: 0.06902685016393661\n",
      "Epoch 8915, Train Loss: 0.013066885992884636, Test Loss: 0.06901491433382034\n",
      "Epoch 8916, Train Loss: 0.01306456234306097, Test Loss: 0.06901135295629501\n",
      "Epoch 8917, Train Loss: 0.013061611913144588, Test Loss: 0.06901393085718155\n",
      "Epoch 8918, Train Loss: 0.013060448691248894, Test Loss: 0.06900488585233688\n",
      "Epoch 8919, Train Loss: 0.013056695461273193, Test Loss: 0.06901492923498154\n",
      "Epoch 8920, Train Loss: 0.013053731061518192, Test Loss: 0.06900889426469803\n",
      "Epoch 8921, Train Loss: 0.013051682151854038, Test Loss: 0.06900360435247421\n",
      "Epoch 8922, Train Loss: 0.013049094006419182, Test Loss: 0.06900632381439209\n",
      "Epoch 8923, Train Loss: 0.013046768493950367, Test Loss: 0.06900671869516373\n",
      "Epoch 8924, Train Loss: 0.013043384067714214, Test Loss: 0.06901880353689194\n",
      "Epoch 8925, Train Loss: 0.013040604069828987, Test Loss: 0.06902150064706802\n",
      "Epoch 8926, Train Loss: 0.013038121163845062, Test Loss: 0.06901341676712036\n",
      "Epoch 8927, Train Loss: 0.013035441748797894, Test Loss: 0.06901062279939651\n",
      "Epoch 8928, Train Loss: 0.013032677583396435, Test Loss: 0.06899981945753098\n",
      "Epoch 8929, Train Loss: 0.013030066154897213, Test Loss: 0.06900352984666824\n",
      "Epoch 8930, Train Loss: 0.013027404434978962, Test Loss: 0.06900963187217712\n",
      "Epoch 8931, Train Loss: 0.013025091029703617, Test Loss: 0.06900562345981598\n",
      "Epoch 8932, Train Loss: 0.013022484257817268, Test Loss: 0.06899534165859222\n",
      "Epoch 8933, Train Loss: 0.013019992038607597, Test Loss: 0.06899242848157883\n",
      "Epoch 8934, Train Loss: 0.013017464429140091, Test Loss: 0.06898535788059235\n",
      "Epoch 8935, Train Loss: 0.013015467673540115, Test Loss: 0.06899615377187729\n",
      "Epoch 8936, Train Loss: 0.013011853210628033, Test Loss: 0.0689995214343071\n",
      "Epoch 8937, Train Loss: 0.01300922129303217, Test Loss: 0.06899412721395493\n",
      "Epoch 8938, Train Loss: 0.013007194735109806, Test Loss: 0.06898949295282364\n",
      "Epoch 8939, Train Loss: 0.013004499487578869, Test Loss: 0.0689835175871849\n",
      "Epoch 8940, Train Loss: 0.013001504354178905, Test Loss: 0.06899034231901169\n",
      "Epoch 8941, Train Loss: 0.012998871505260468, Test Loss: 0.06898447871208191\n",
      "Epoch 8942, Train Loss: 0.012996782548725605, Test Loss: 0.06898453086614609\n",
      "Epoch 8943, Train Loss: 0.012994278222322464, Test Loss: 0.06897075474262238\n",
      "Epoch 8944, Train Loss: 0.012991662137210369, Test Loss: 0.06898534297943115\n",
      "Epoch 8945, Train Loss: 0.012988682836294174, Test Loss: 0.06897661089897156\n",
      "Epoch 8946, Train Loss: 0.012986547313630581, Test Loss: 0.06897693127393723\n",
      "Epoch 8947, Train Loss: 0.012983541935682297, Test Loss: 0.06897758692502975\n",
      "Epoch 8948, Train Loss: 0.012981470674276352, Test Loss: 0.06896350532770157\n",
      "Epoch 8949, Train Loss: 0.012978416867554188, Test Loss: 0.06896282732486725\n",
      "Epoch 8950, Train Loss: 0.012975853867828846, Test Loss: 0.068968266248703\n",
      "Epoch 8951, Train Loss: 0.012973124161362648, Test Loss: 0.0689602717757225\n",
      "Epoch 8952, Train Loss: 0.012970869429409504, Test Loss: 0.06895853579044342\n",
      "Epoch 8953, Train Loss: 0.01296834833920002, Test Loss: 0.06895772367715836\n",
      "Epoch 8954, Train Loss: 0.012965700589120388, Test Loss: 0.06895869970321655\n",
      "Epoch 8955, Train Loss: 0.01296360231935978, Test Loss: 0.06895111501216888\n",
      "Epoch 8956, Train Loss: 0.012960447929799557, Test Loss: 0.06895539164543152\n",
      "Epoch 8957, Train Loss: 0.012957893311977386, Test Loss: 0.0689491406083107\n",
      "Epoch 8958, Train Loss: 0.012955824844539165, Test Loss: 0.06894377619028091\n",
      "Epoch 8959, Train Loss: 0.012952961958944798, Test Loss: 0.06895201653242111\n",
      "Epoch 8960, Train Loss: 0.012950749136507511, Test Loss: 0.06895548850297928\n",
      "Epoch 8961, Train Loss: 0.012947766110301018, Test Loss: 0.06895721703767776\n",
      "Epoch 8962, Train Loss: 0.012945360504090786, Test Loss: 0.06894811987876892\n",
      "Epoch 8963, Train Loss: 0.0129426633939147, Test Loss: 0.0689396858215332\n",
      "Epoch 8964, Train Loss: 0.012939857318997383, Test Loss: 0.0689348503947258\n",
      "Epoch 8965, Train Loss: 0.01293782889842987, Test Loss: 0.06894055753946304\n",
      "Epoch 8966, Train Loss: 0.012934749014675617, Test Loss: 0.06894195824861526\n",
      "Epoch 8967, Train Loss: 0.012932181358337402, Test Loss: 0.06893851608037949\n",
      "Epoch 8968, Train Loss: 0.012930300086736679, Test Loss: 0.06894373893737793\n",
      "Epoch 8969, Train Loss: 0.012927025556564331, Test Loss: 0.06893301010131836\n",
      "Epoch 8970, Train Loss: 0.012924425303936005, Test Loss: 0.06893309950828552\n",
      "Epoch 8971, Train Loss: 0.012921839021146297, Test Loss: 0.06893264502286911\n",
      "Epoch 8972, Train Loss: 0.012919443659484386, Test Loss: 0.06893131881952286\n",
      "Epoch 8973, Train Loss: 0.012917010113596916, Test Loss: 0.06893263757228851\n",
      "Epoch 8974, Train Loss: 0.012914244085550308, Test Loss: 0.06893131881952286\n",
      "Epoch 8975, Train Loss: 0.012912003323435783, Test Loss: 0.06893549859523773\n",
      "Epoch 8976, Train Loss: 0.012909994460642338, Test Loss: 0.06893443316221237\n",
      "Epoch 8977, Train Loss: 0.012906614691019058, Test Loss: 0.06892067193984985\n",
      "Epoch 8978, Train Loss: 0.012904140166938305, Test Loss: 0.06892424821853638\n",
      "Epoch 8979, Train Loss: 0.012901629321277142, Test Loss: 0.0689186081290245\n",
      "Epoch 8980, Train Loss: 0.012899421155452728, Test Loss: 0.0689057931303978\n",
      "Epoch 8981, Train Loss: 0.01289661880582571, Test Loss: 0.06891530007123947\n",
      "Epoch 8982, Train Loss: 0.012893845327198505, Test Loss: 0.06892334669828415\n",
      "Epoch 8983, Train Loss: 0.012891322374343872, Test Loss: 0.0689135417342186\n",
      "Epoch 8984, Train Loss: 0.012888752855360508, Test Loss: 0.06891383975744247\n",
      "Epoch 8985, Train Loss: 0.012886151671409607, Test Loss: 0.06889959424734116\n",
      "Epoch 8986, Train Loss: 0.012883542105555534, Test Loss: 0.0688994750380516\n",
      "Epoch 8987, Train Loss: 0.012881024740636349, Test Loss: 0.068900465965271\n",
      "Epoch 8988, Train Loss: 0.01287887617945671, Test Loss: 0.06890695542097092\n",
      "Epoch 8989, Train Loss: 0.012875877320766449, Test Loss: 0.06890124827623367\n",
      "Epoch 8990, Train Loss: 0.012873330153524876, Test Loss: 0.06890054792165756\n",
      "Epoch 8991, Train Loss: 0.01287092361599207, Test Loss: 0.06889279186725616\n",
      "Epoch 8992, Train Loss: 0.012868502177298069, Test Loss: 0.068901926279068\n",
      "Epoch 8993, Train Loss: 0.012865977361798286, Test Loss: 0.06888867914676666\n",
      "Epoch 8994, Train Loss: 0.012863043695688248, Test Loss: 0.06889583170413971\n",
      "Epoch 8995, Train Loss: 0.012861153110861778, Test Loss: 0.06888466328382492\n",
      "Epoch 8996, Train Loss: 0.012857828289270401, Test Loss: 0.06889689713716507\n",
      "Epoch 8997, Train Loss: 0.012855177745223045, Test Loss: 0.06888683140277863\n",
      "Epoch 8998, Train Loss: 0.012853361666202545, Test Loss: 0.06888072937726974\n",
      "Epoch 8999, Train Loss: 0.01285029761493206, Test Loss: 0.06888413429260254\n",
      "Epoch 9000, Train Loss: 0.012847415171563625, Test Loss: 0.06888493150472641\n",
      "Epoch 9001, Train Loss: 0.012845071963965893, Test Loss: 0.06888312101364136\n",
      "Epoch 9002, Train Loss: 0.012842525728046894, Test Loss: 0.06888436526060104\n",
      "Epoch 9003, Train Loss: 0.01283987332135439, Test Loss: 0.06887666881084442\n",
      "Epoch 9004, Train Loss: 0.012837419286370277, Test Loss: 0.06887757033109665\n",
      "Epoch 9005, Train Loss: 0.012834854423999786, Test Loss: 0.06887505203485489\n",
      "Epoch 9006, Train Loss: 0.012832489795982838, Test Loss: 0.06887329369783401\n",
      "Epoch 9007, Train Loss: 0.012829707935452461, Test Loss: 0.06887587159872055\n",
      "Epoch 9008, Train Loss: 0.012828348204493523, Test Loss: 0.06887735426425934\n",
      "Epoch 9009, Train Loss: 0.01282457448542118, Test Loss: 0.06887409836053848\n",
      "Epoch 9010, Train Loss: 0.012822146527469158, Test Loss: 0.06886722892522812\n",
      "Epoch 9011, Train Loss: 0.012819497846066952, Test Loss: 0.06886203587055206\n",
      "Epoch 9012, Train Loss: 0.012816688977181911, Test Loss: 0.0688617154955864\n",
      "Epoch 9013, Train Loss: 0.012814413756132126, Test Loss: 0.06886544823646545\n",
      "Epoch 9014, Train Loss: 0.012811764143407345, Test Loss: 0.06886565685272217\n",
      "Epoch 9015, Train Loss: 0.012809233739972115, Test Loss: 0.06886232644319534\n",
      "Epoch 9016, Train Loss: 0.012806604616343975, Test Loss: 0.06886210292577744\n",
      "Epoch 9017, Train Loss: 0.01280396431684494, Test Loss: 0.06886082142591476\n",
      "Epoch 9018, Train Loss: 0.012801519595086575, Test Loss: 0.0688471719622612\n",
      "Epoch 9019, Train Loss: 0.012799144722521305, Test Loss: 0.0688457190990448\n",
      "Epoch 9020, Train Loss: 0.012796618044376373, Test Loss: 0.06883899122476578\n",
      "Epoch 9021, Train Loss: 0.012794368900358677, Test Loss: 0.0688319280743599\n",
      "Epoch 9022, Train Loss: 0.012791093438863754, Test Loss: 0.06884457916021347\n",
      "Epoch 9023, Train Loss: 0.012788725085556507, Test Loss: 0.06884506344795227\n",
      "Epoch 9024, Train Loss: 0.012786831706762314, Test Loss: 0.06883010268211365\n",
      "Epoch 9025, Train Loss: 0.012783657759428024, Test Loss: 0.06883879750967026\n",
      "Epoch 9026, Train Loss: 0.012781502678990364, Test Loss: 0.06883807480335236\n",
      "Epoch 9027, Train Loss: 0.012778300791978836, Test Loss: 0.0688411146402359\n",
      "Epoch 9028, Train Loss: 0.012775846756994724, Test Loss: 0.06883838027715683\n",
      "Epoch 9029, Train Loss: 0.01277320459485054, Test Loss: 0.06884212791919708\n",
      "Epoch 9030, Train Loss: 0.012770943343639374, Test Loss: 0.06883269548416138\n",
      "Epoch 9031, Train Loss: 0.012768743559718132, Test Loss: 0.06882306933403015\n",
      "Epoch 9032, Train Loss: 0.01276590209454298, Test Loss: 0.06881942600011826\n",
      "Epoch 9033, Train Loss: 0.012763641774654388, Test Loss: 0.06882261484861374\n",
      "Epoch 9034, Train Loss: 0.012760939076542854, Test Loss: 0.06881775707006454\n",
      "Epoch 9035, Train Loss: 0.012758507393300533, Test Loss: 0.06881315261125565\n",
      "Epoch 9036, Train Loss: 0.012755329720675945, Test Loss: 0.068816639482975\n",
      "Epoch 9037, Train Loss: 0.012752770446240902, Test Loss: 0.06881240010261536\n",
      "Epoch 9038, Train Loss: 0.012750279158353806, Test Loss: 0.06881413608789444\n",
      "Epoch 9039, Train Loss: 0.01274800207465887, Test Loss: 0.06881386786699295\n",
      "Epoch 9040, Train Loss: 0.012745367363095284, Test Loss: 0.0688169002532959\n",
      "Epoch 9041, Train Loss: 0.01274245698004961, Test Loss: 0.06880901008844376\n",
      "Epoch 9042, Train Loss: 0.01274046953767538, Test Loss: 0.0688086450099945\n",
      "Epoch 9043, Train Loss: 0.012737461365759373, Test Loss: 0.06881991028785706\n",
      "Epoch 9044, Train Loss: 0.012735231779515743, Test Loss: 0.0688091292977333\n",
      "Epoch 9045, Train Loss: 0.012733391486108303, Test Loss: 0.06879454106092453\n",
      "Epoch 9046, Train Loss: 0.012730687856674194, Test Loss: 0.06879031658172607\n",
      "Epoch 9047, Train Loss: 0.012727241963148117, Test Loss: 0.06880133599042892\n",
      "Epoch 9048, Train Loss: 0.012724868953227997, Test Loss: 0.06878878176212311\n",
      "Epoch 9049, Train Loss: 0.012722617015242577, Test Loss: 0.06880195438861847\n",
      "Epoch 9050, Train Loss: 0.012719610705971718, Test Loss: 0.06879805028438568\n",
      "Epoch 9051, Train Loss: 0.012717149220407009, Test Loss: 0.06879166513681412\n",
      "Epoch 9052, Train Loss: 0.012714586220681667, Test Loss: 0.06879152357578278\n",
      "Epoch 9053, Train Loss: 0.012711918912827969, Test Loss: 0.06879682093858719\n",
      "Epoch 9054, Train Loss: 0.012709400616586208, Test Loss: 0.06879407912492752\n",
      "Epoch 9055, Train Loss: 0.012706929817795753, Test Loss: 0.06878925114870071\n",
      "Epoch 9056, Train Loss: 0.012704544700682163, Test Loss: 0.06878388673067093\n",
      "Epoch 9057, Train Loss: 0.012701856903731823, Test Loss: 0.06879009306430817\n",
      "Epoch 9058, Train Loss: 0.012699270620942116, Test Loss: 0.06878134608268738\n",
      "Epoch 9059, Train Loss: 0.012697241269052029, Test Loss: 0.06877142935991287\n",
      "Epoch 9060, Train Loss: 0.012694432400166988, Test Loss: 0.06876722723245621\n",
      "Epoch 9061, Train Loss: 0.012692149728536606, Test Loss: 0.06876787543296814\n",
      "Epoch 9062, Train Loss: 0.012689884752035141, Test Loss: 0.06876404583454132\n",
      "Epoch 9063, Train Loss: 0.012686874717473984, Test Loss: 0.0687689259648323\n",
      "Epoch 9064, Train Loss: 0.012684031389653683, Test Loss: 0.06877098232507706\n",
      "Epoch 9065, Train Loss: 0.012681400403380394, Test Loss: 0.06877071410417557\n",
      "Epoch 9066, Train Loss: 0.012678784318268299, Test Loss: 0.06876841187477112\n",
      "Epoch 9067, Train Loss: 0.012676533311605453, Test Loss: 0.0687638595700264\n",
      "Epoch 9068, Train Loss: 0.012673796154558659, Test Loss: 0.06876013427972794\n",
      "Epoch 9069, Train Loss: 0.012671228498220444, Test Loss: 0.06876450031995773\n",
      "Epoch 9070, Train Loss: 0.012669138610363007, Test Loss: 0.06876687705516815\n",
      "Epoch 9071, Train Loss: 0.012666421942412853, Test Loss: 0.06875920295715332\n",
      "Epoch 9072, Train Loss: 0.012663507834076881, Test Loss: 0.06875824183225632\n",
      "Epoch 9073, Train Loss: 0.012661198154091835, Test Loss: 0.06874591112136841\n",
      "Epoch 9074, Train Loss: 0.012658558785915375, Test Loss: 0.06874939054250717\n",
      "Epoch 9075, Train Loss: 0.012655870988965034, Test Loss: 0.06875395774841309\n",
      "Epoch 9076, Train Loss: 0.01265355572104454, Test Loss: 0.06874880939722061\n",
      "Epoch 9077, Train Loss: 0.012652159668505192, Test Loss: 0.06873769313097\n",
      "Epoch 9078, Train Loss: 0.012649200856685638, Test Loss: 0.06873895227909088\n",
      "Epoch 9079, Train Loss: 0.012645991519093513, Test Loss: 0.06874094158411026\n",
      "Epoch 9080, Train Loss: 0.012643182650208473, Test Loss: 0.06874264776706696\n",
      "Epoch 9081, Train Loss: 0.012641025707125664, Test Loss: 0.06874536722898483\n",
      "Epoch 9082, Train Loss: 0.012638195417821407, Test Loss: 0.06873957812786102\n",
      "Epoch 9083, Train Loss: 0.01263589970767498, Test Loss: 0.06873001158237457\n",
      "Epoch 9084, Train Loss: 0.012634064070880413, Test Loss: 0.06873741745948792\n",
      "Epoch 9085, Train Loss: 0.012630597688257694, Test Loss: 0.06873419880867004\n",
      "Epoch 9086, Train Loss: 0.012628437951207161, Test Loss: 0.06873347610235214\n",
      "Epoch 9087, Train Loss: 0.012625517323613167, Test Loss: 0.06874197721481323\n",
      "Epoch 9088, Train Loss: 0.012623063288629055, Test Loss: 0.06873351335525513\n",
      "Epoch 9089, Train Loss: 0.0126205338165164, Test Loss: 0.06872706860303879\n",
      "Epoch 9090, Train Loss: 0.012618173845112324, Test Loss: 0.06872810423374176\n",
      "Epoch 9091, Train Loss: 0.012615757994353771, Test Loss: 0.06872127205133438\n",
      "Epoch 9092, Train Loss: 0.012612953782081604, Test Loss: 0.0687185674905777\n",
      "Epoch 9093, Train Loss: 0.012611142359673977, Test Loss: 0.06872690469026566\n",
      "Epoch 9094, Train Loss: 0.01260829996317625, Test Loss: 0.0687318816781044\n",
      "Epoch 9095, Train Loss: 0.012606003321707249, Test Loss: 0.06872129440307617\n",
      "Epoch 9096, Train Loss: 0.012603089213371277, Test Loss: 0.06871253252029419\n",
      "Epoch 9097, Train Loss: 0.012600656598806381, Test Loss: 0.06871645897626877\n",
      "Epoch 9098, Train Loss: 0.012597777880728245, Test Loss: 0.06871767342090607\n",
      "Epoch 9099, Train Loss: 0.012595018371939659, Test Loss: 0.06871186196804047\n",
      "Epoch 9100, Train Loss: 0.012592688202857971, Test Loss: 0.06871779263019562\n",
      "Epoch 9101, Train Loss: 0.012589969672262669, Test Loss: 0.06871530413627625\n",
      "Epoch 9102, Train Loss: 0.012587481178343296, Test Loss: 0.06870860606431961\n",
      "Epoch 9103, Train Loss: 0.012585068121552467, Test Loss: 0.06871004402637482\n",
      "Epoch 9104, Train Loss: 0.012582476250827312, Test Loss: 0.06870114803314209\n",
      "Epoch 9105, Train Loss: 0.012579988688230515, Test Loss: 0.06870811432600021\n",
      "Epoch 9106, Train Loss: 0.012577425688505173, Test Loss: 0.06870721280574799\n",
      "Epoch 9107, Train Loss: 0.012575279921293259, Test Loss: 0.068708635866642\n",
      "Epoch 9108, Train Loss: 0.012572369538247585, Test Loss: 0.06870349496603012\n",
      "Epoch 9109, Train Loss: 0.012570002116262913, Test Loss: 0.06869177520275116\n",
      "Epoch 9110, Train Loss: 0.01256820373237133, Test Loss: 0.0687052384018898\n",
      "Epoch 9111, Train Loss: 0.012564837001264095, Test Loss: 0.06869713962078094\n",
      "Epoch 9112, Train Loss: 0.012562594376504421, Test Loss: 0.06868836283683777\n",
      "Epoch 9113, Train Loss: 0.012559575960040092, Test Loss: 0.06869087368249893\n",
      "Epoch 9114, Train Loss: 0.012557310983538628, Test Loss: 0.06868185102939606\n",
      "Epoch 9115, Train Loss: 0.012555125169456005, Test Loss: 0.06869155168533325\n",
      "Epoch 9116, Train Loss: 0.012552023865282536, Test Loss: 0.06868045032024384\n",
      "Epoch 9117, Train Loss: 0.012549472972750664, Test Loss: 0.06867672502994537\n",
      "Epoch 9118, Train Loss: 0.012546928599476814, Test Loss: 0.06868479400873184\n",
      "Epoch 9119, Train Loss: 0.01254489179700613, Test Loss: 0.06867336481809616\n",
      "Epoch 9120, Train Loss: 0.012542110867798328, Test Loss: 0.06867793947458267\n",
      "Epoch 9121, Train Loss: 0.012539682909846306, Test Loss: 0.06867179274559021\n",
      "Epoch 9122, Train Loss: 0.012537140399217606, Test Loss: 0.06868451088666916\n",
      "Epoch 9123, Train Loss: 0.012534840032458305, Test Loss: 0.0686897486448288\n",
      "Epoch 9124, Train Loss: 0.012532089836895466, Test Loss: 0.06867767870426178\n",
      "Epoch 9125, Train Loss: 0.012529714964330196, Test Loss: 0.06867346167564392\n",
      "Epoch 9126, Train Loss: 0.012527705170214176, Test Loss: 0.06867467612028122\n",
      "Epoch 9127, Train Loss: 0.012524932622909546, Test Loss: 0.06866602599620819\n",
      "Epoch 9128, Train Loss: 0.012522045522928238, Test Loss: 0.06866119056940079\n",
      "Epoch 9129, Train Loss: 0.01251953560858965, Test Loss: 0.06865515559911728\n",
      "Epoch 9130, Train Loss: 0.01251680962741375, Test Loss: 0.06864812970161438\n",
      "Epoch 9131, Train Loss: 0.012514489702880383, Test Loss: 0.0686454176902771\n",
      "Epoch 9132, Train Loss: 0.012512230314314365, Test Loss: 0.06864554435014725\n",
      "Epoch 9133, Train Loss: 0.012509230524301529, Test Loss: 0.06865477561950684\n",
      "Epoch 9134, Train Loss: 0.012506664730608463, Test Loss: 0.06865933537483215\n",
      "Epoch 9135, Train Loss: 0.012504209764301777, Test Loss: 0.06864980608224869\n",
      "Epoch 9136, Train Loss: 0.012502006255090237, Test Loss: 0.0686449408531189\n",
      "Epoch 9137, Train Loss: 0.01250026747584343, Test Loss: 0.0686606913805008\n",
      "Epoch 9138, Train Loss: 0.012496690265834332, Test Loss: 0.06865691393613815\n",
      "Epoch 9139, Train Loss: 0.012494734488427639, Test Loss: 0.0686376541852951\n",
      "Epoch 9140, Train Loss: 0.012492002919316292, Test Loss: 0.06864380091428757\n",
      "Epoch 9141, Train Loss: 0.012488959357142448, Test Loss: 0.06864867359399796\n",
      "Epoch 9142, Train Loss: 0.012487029656767845, Test Loss: 0.06864290684461594\n",
      "Epoch 9143, Train Loss: 0.012484156526625156, Test Loss: 0.0686388686299324\n",
      "Epoch 9144, Train Loss: 0.012481562793254852, Test Loss: 0.06863991171121597\n",
      "Epoch 9145, Train Loss: 0.012479241006076336, Test Loss: 0.06863722205162048\n",
      "Epoch 9146, Train Loss: 0.01247655414044857, Test Loss: 0.06863389909267426\n",
      "Epoch 9147, Train Loss: 0.012474232353270054, Test Loss: 0.0686315968632698\n",
      "Epoch 9148, Train Loss: 0.012471653521060944, Test Loss: 0.06862302869558334\n",
      "Epoch 9149, Train Loss: 0.012469002045691013, Test Loss: 0.0686296597123146\n",
      "Epoch 9150, Train Loss: 0.012466548942029476, Test Loss: 0.06863225251436234\n",
      "Epoch 9151, Train Loss: 0.012463606894016266, Test Loss: 0.06862478703260422\n",
      "Epoch 9152, Train Loss: 0.012461290694773197, Test Loss: 0.06862527877092361\n",
      "Epoch 9153, Train Loss: 0.012459206394851208, Test Loss: 0.06862795352935791\n",
      "Epoch 9154, Train Loss: 0.012457256205379963, Test Loss: 0.06862392276525497\n",
      "Epoch 9155, Train Loss: 0.012453819625079632, Test Loss: 0.0686202123761177\n",
      "Epoch 9156, Train Loss: 0.012451094575226307, Test Loss: 0.06861580163240433\n",
      "Epoch 9157, Train Loss: 0.012449340894818306, Test Loss: 0.06861753016710281\n",
      "Epoch 9158, Train Loss: 0.012446117587387562, Test Loss: 0.06860881298780441\n",
      "Epoch 9159, Train Loss: 0.012443527579307556, Test Loss: 0.06861232221126556\n",
      "Epoch 9160, Train Loss: 0.01244117971509695, Test Loss: 0.06860759109258652\n",
      "Epoch 9161, Train Loss: 0.012438587844371796, Test Loss: 0.06860899925231934\n",
      "Epoch 9162, Train Loss: 0.01243603602051735, Test Loss: 0.0686093345284462\n",
      "Epoch 9163, Train Loss: 0.01243374403566122, Test Loss: 0.06859970837831497\n",
      "Epoch 9164, Train Loss: 0.012431206181645393, Test Loss: 0.06860087811946869\n",
      "Epoch 9165, Train Loss: 0.012428537011146545, Test Loss: 0.06860179454088211\n",
      "Epoch 9166, Train Loss: 0.012426246888935566, Test Loss: 0.06859859824180603\n",
      "Epoch 9167, Train Loss: 0.01242341473698616, Test Loss: 0.06860426813364029\n",
      "Epoch 9168, Train Loss: 0.012421059422194958, Test Loss: 0.0685945525765419\n",
      "Epoch 9169, Train Loss: 0.012418352998793125, Test Loss: 0.06860402226448059\n",
      "Epoch 9170, Train Loss: 0.012416158802807331, Test Loss: 0.06859928369522095\n",
      "Epoch 9171, Train Loss: 0.012413757853209972, Test Loss: 0.06859615445137024\n",
      "Epoch 9172, Train Loss: 0.012411586940288544, Test Loss: 0.06858403235673904\n",
      "Epoch 9173, Train Loss: 0.012409171089529991, Test Loss: 0.06858005374670029\n",
      "Epoch 9174, Train Loss: 0.012406072579324245, Test Loss: 0.0685809999704361\n",
      "Epoch 9175, Train Loss: 0.012403421103954315, Test Loss: 0.06858627498149872\n",
      "Epoch 9176, Train Loss: 0.012400955893099308, Test Loss: 0.0685877725481987\n",
      "Epoch 9177, Train Loss: 0.012398610822856426, Test Loss: 0.06858421117067337\n",
      "Epoch 9178, Train Loss: 0.012396334670484066, Test Loss: 0.06857866793870926\n",
      "Epoch 9179, Train Loss: 0.012393558397889137, Test Loss: 0.06858876347541809\n",
      "Epoch 9180, Train Loss: 0.01239099819213152, Test Loss: 0.06857838481664658\n",
      "Epoch 9181, Train Loss: 0.012388164177536964, Test Loss: 0.06857708096504211\n",
      "Epoch 9182, Train Loss: 0.012386261485517025, Test Loss: 0.06858187913894653\n",
      "Epoch 9183, Train Loss: 0.012383338063955307, Test Loss: 0.06857553869485855\n",
      "Epoch 9184, Train Loss: 0.012381011620163918, Test Loss: 0.06856969743967056\n",
      "Epoch 9185, Train Loss: 0.012378296814858913, Test Loss: 0.06857091933488846\n",
      "Epoch 9186, Train Loss: 0.012375880032777786, Test Loss: 0.06857247650623322\n",
      "Epoch 9187, Train Loss: 0.012373377569019794, Test Loss: 0.06856726109981537\n",
      "Epoch 9188, Train Loss: 0.0123713044449687, Test Loss: 0.06856292486190796\n",
      "Epoch 9189, Train Loss: 0.0123685821890831, Test Loss: 0.06856080144643784\n",
      "Epoch 9190, Train Loss: 0.012366232462227345, Test Loss: 0.06855261325836182\n",
      "Epoch 9191, Train Loss: 0.012363514862954617, Test Loss: 0.06855597347021103\n",
      "Epoch 9192, Train Loss: 0.012361238710582256, Test Loss: 0.06854905933141708\n",
      "Epoch 9193, Train Loss: 0.012358744628727436, Test Loss: 0.06854554265737534\n",
      "Epoch 9194, Train Loss: 0.01235585380345583, Test Loss: 0.0685458555817604\n",
      "Epoch 9195, Train Loss: 0.012353938072919846, Test Loss: 0.068550705909729\n",
      "Epoch 9196, Train Loss: 0.012351136654615402, Test Loss: 0.06854136288166046\n",
      "Epoch 9197, Train Loss: 0.012348316609859467, Test Loss: 0.06855181604623795\n",
      "Epoch 9198, Train Loss: 0.012345965951681137, Test Loss: 0.0685531497001648\n",
      "Epoch 9199, Train Loss: 0.01234419271349907, Test Loss: 0.06855355948209763\n",
      "Epoch 9200, Train Loss: 0.012341194786131382, Test Loss: 0.06853512674570084\n",
      "Epoch 9201, Train Loss: 0.012338686734437943, Test Loss: 0.06853614002466202\n",
      "Epoch 9202, Train Loss: 0.012336228042840958, Test Loss: 0.06853757053613663\n",
      "Epoch 9203, Train Loss: 0.012334095314145088, Test Loss: 0.0685369074344635\n",
      "Epoch 9204, Train Loss: 0.012331086210906506, Test Loss: 0.06852937489748001\n",
      "Epoch 9205, Train Loss: 0.012328671291470528, Test Loss: 0.06852281838655472\n",
      "Epoch 9206, Train Loss: 0.01232615951448679, Test Loss: 0.06853023171424866\n",
      "Epoch 9207, Train Loss: 0.01232379861176014, Test Loss: 0.0685439333319664\n",
      "Epoch 9208, Train Loss: 0.01232090126723051, Test Loss: 0.06852835416793823\n",
      "Epoch 9209, Train Loss: 0.012318525463342667, Test Loss: 0.06852678954601288\n",
      "Epoch 9210, Train Loss: 0.012316321022808552, Test Loss: 0.06851917505264282\n",
      "Epoch 9211, Train Loss: 0.012313476763665676, Test Loss: 0.0685245618224144\n",
      "Epoch 9212, Train Loss: 0.012310939840972424, Test Loss: 0.06853244453668594\n",
      "Epoch 9213, Train Loss: 0.012308654375374317, Test Loss: 0.06851869076490402\n",
      "Epoch 9214, Train Loss: 0.012306679971516132, Test Loss: 0.0685184895992279\n",
      "Epoch 9215, Train Loss: 0.01230375375598669, Test Loss: 0.0685088112950325\n",
      "Epoch 9216, Train Loss: 0.012302067130804062, Test Loss: 0.0684991329908371\n",
      "Epoch 9217, Train Loss: 0.012299618683755398, Test Loss: 0.06849367171525955\n",
      "Epoch 9218, Train Loss: 0.012296288274228573, Test Loss: 0.0685013085603714\n",
      "Epoch 9219, Train Loss: 0.012293585576117039, Test Loss: 0.06850326806306839\n",
      "Epoch 9220, Train Loss: 0.012291204184293747, Test Loss: 0.06850343197584152\n",
      "Epoch 9221, Train Loss: 0.012288972735404968, Test Loss: 0.06849479675292969\n",
      "Epoch 9222, Train Loss: 0.012286176905035973, Test Loss: 0.06850000470876694\n",
      "Epoch 9223, Train Loss: 0.012284164316952229, Test Loss: 0.06849852204322815\n",
      "Epoch 9224, Train Loss: 0.012281474657356739, Test Loss: 0.0684949979186058\n",
      "Epoch 9225, Train Loss: 0.012278776615858078, Test Loss: 0.06849309802055359\n",
      "Epoch 9226, Train Loss: 0.012276941910386086, Test Loss: 0.06848207116127014\n",
      "Epoch 9227, Train Loss: 0.01227370835840702, Test Loss: 0.06849612295627594\n",
      "Epoch 9228, Train Loss: 0.01227131299674511, Test Loss: 0.06849220395088196\n",
      "Epoch 9229, Train Loss: 0.012268654070794582, Test Loss: 0.06849565356969833\n",
      "Epoch 9230, Train Loss: 0.0122669767588377, Test Loss: 0.06849578768014908\n",
      "Epoch 9231, Train Loss: 0.012264130637049675, Test Loss: 0.06848465651273727\n",
      "Epoch 9232, Train Loss: 0.012261987663805485, Test Loss: 0.06847631186246872\n",
      "Epoch 9233, Train Loss: 0.01225950662046671, Test Loss: 0.0684872567653656\n",
      "Epoch 9234, Train Loss: 0.012256505899131298, Test Loss: 0.06848685443401337\n",
      "Epoch 9235, Train Loss: 0.012254096567630768, Test Loss: 0.06848152726888657\n",
      "Epoch 9236, Train Loss: 0.012252281419932842, Test Loss: 0.06846322864294052\n",
      "Epoch 9237, Train Loss: 0.012249220162630081, Test Loss: 0.06846605241298676\n",
      "Epoch 9238, Train Loss: 0.012246979400515556, Test Loss: 0.06846305727958679\n",
      "Epoch 9239, Train Loss: 0.01224429439753294, Test Loss: 0.06846479326486588\n",
      "Epoch 9240, Train Loss: 0.012242134660482407, Test Loss: 0.06847185641527176\n",
      "Epoch 9241, Train Loss: 0.012239504605531693, Test Loss: 0.06847481429576874\n",
      "Epoch 9242, Train Loss: 0.012236874550580978, Test Loss: 0.06848184019327164\n",
      "Epoch 9243, Train Loss: 0.012234539724886417, Test Loss: 0.06846477091312408\n",
      "Epoch 9244, Train Loss: 0.012231736443936825, Test Loss: 0.06846809387207031\n",
      "Epoch 9245, Train Loss: 0.012229482643306255, Test Loss: 0.0684739425778389\n",
      "Epoch 9246, Train Loss: 0.012226921506226063, Test Loss: 0.06847136467695236\n",
      "Epoch 9247, Train Loss: 0.012224534526467323, Test Loss: 0.06847178190946579\n",
      "Epoch 9248, Train Loss: 0.012222309596836567, Test Loss: 0.06845628470182419\n",
      "Epoch 9249, Train Loss: 0.012219476513564587, Test Loss: 0.06846192479133606\n",
      "Epoch 9250, Train Loss: 0.01221766509115696, Test Loss: 0.06846040487289429\n",
      "Epoch 9251, Train Loss: 0.012214518152177334, Test Loss: 0.06846193224191666\n",
      "Epoch 9252, Train Loss: 0.012212092988193035, Test Loss: 0.06845723092556\n",
      "Epoch 9253, Train Loss: 0.012209580279886723, Test Loss: 0.0684601217508316\n",
      "Epoch 9254, Train Loss: 0.012207230553030968, Test Loss: 0.06844983994960785\n",
      "Epoch 9255, Train Loss: 0.012204932048916817, Test Loss: 0.06845282763242722\n",
      "Epoch 9256, Train Loss: 0.012202240526676178, Test Loss: 0.06845250725746155\n",
      "Epoch 9257, Train Loss: 0.01219969056546688, Test Loss: 0.06844918429851532\n",
      "Epoch 9258, Train Loss: 0.012197388336062431, Test Loss: 0.0684448629617691\n",
      "Epoch 9259, Train Loss: 0.012195191346108913, Test Loss: 0.0684487447142601\n",
      "Epoch 9260, Train Loss: 0.012193160131573677, Test Loss: 0.06843408197164536\n",
      "Epoch 9261, Train Loss: 0.012189975939691067, Test Loss: 0.06843888014554977\n",
      "Epoch 9262, Train Loss: 0.012187772430479527, Test Loss: 0.06843304634094238\n",
      "Epoch 9263, Train Loss: 0.012185279279947281, Test Loss: 0.0684303417801857\n",
      "Epoch 9264, Train Loss: 0.012182902544736862, Test Loss: 0.06843142211437225\n",
      "Epoch 9265, Train Loss: 0.012180265970528126, Test Loss: 0.0684303268790245\n",
      "Epoch 9266, Train Loss: 0.012177782133221626, Test Loss: 0.06842527538537979\n",
      "Epoch 9267, Train Loss: 0.012175168842077255, Test Loss: 0.06842761486768723\n",
      "Epoch 9268, Train Loss: 0.012172769755125046, Test Loss: 0.06842184066772461\n",
      "Epoch 9269, Train Loss: 0.012170463800430298, Test Loss: 0.06842143088579178\n",
      "Epoch 9270, Train Loss: 0.012167880311608315, Test Loss: 0.06842414289712906\n",
      "Epoch 9271, Train Loss: 0.012165555730462074, Test Loss: 0.06842076778411865\n",
      "Epoch 9272, Train Loss: 0.01216374896466732, Test Loss: 0.06842463463544846\n",
      "Epoch 9273, Train Loss: 0.012160618789494038, Test Loss: 0.0684187188744545\n",
      "Epoch 9274, Train Loss: 0.012158248573541641, Test Loss: 0.06842143833637238\n",
      "Epoch 9275, Train Loss: 0.012156298384070396, Test Loss: 0.06840726733207703\n",
      "Epoch 9276, Train Loss: 0.012154032476246357, Test Loss: 0.06839562207460403\n",
      "Epoch 9277, Train Loss: 0.012151113711297512, Test Loss: 0.06840034574270248\n",
      "Epoch 9278, Train Loss: 0.012148636393249035, Test Loss: 0.06840251386165619\n",
      "Epoch 9279, Train Loss: 0.01214608084410429, Test Loss: 0.06840299069881439\n",
      "Epoch 9280, Train Loss: 0.012143229134380817, Test Loss: 0.06840753555297852\n",
      "Epoch 9281, Train Loss: 0.012141356244683266, Test Loss: 0.06840668618679047\n",
      "Epoch 9282, Train Loss: 0.012138476595282555, Test Loss: 0.06840431690216064\n",
      "Epoch 9283, Train Loss: 0.012136471457779408, Test Loss: 0.0683915913105011\n",
      "Epoch 9284, Train Loss: 0.012133821845054626, Test Loss: 0.06839604675769806\n",
      "Epoch 9285, Train Loss: 0.012131145223975182, Test Loss: 0.06839986890554428\n",
      "Epoch 9286, Train Loss: 0.012128828093409538, Test Loss: 0.06838753074407578\n",
      "Epoch 9287, Train Loss: 0.012126707471907139, Test Loss: 0.06838510185480118\n",
      "Epoch 9288, Train Loss: 0.012124281376600266, Test Loss: 0.06838303804397583\n",
      "Epoch 9289, Train Loss: 0.01212126575410366, Test Loss: 0.06838231533765793\n",
      "Epoch 9290, Train Loss: 0.012120167724788189, Test Loss: 0.06837182492017746\n",
      "Epoch 9291, Train Loss: 0.012116805650293827, Test Loss: 0.06838279962539673\n",
      "Epoch 9292, Train Loss: 0.012114212848246098, Test Loss: 0.06838548928499222\n",
      "Epoch 9293, Train Loss: 0.012111428193747997, Test Loss: 0.06839419901371002\n",
      "Epoch 9294, Train Loss: 0.012109137140214443, Test Loss: 0.06839124113321304\n",
      "Epoch 9295, Train Loss: 0.01210658997297287, Test Loss: 0.06839122623205185\n",
      "Epoch 9296, Train Loss: 0.012104392051696777, Test Loss: 0.06838028877973557\n",
      "Epoch 9297, Train Loss: 0.01210259459912777, Test Loss: 0.06836985051631927\n",
      "Epoch 9298, Train Loss: 0.012099425308406353, Test Loss: 0.06837654858827591\n",
      "Epoch 9299, Train Loss: 0.012097039259970188, Test Loss: 0.06836844235658646\n",
      "Epoch 9300, Train Loss: 0.01209594402462244, Test Loss: 0.06835471093654633\n",
      "Epoch 9301, Train Loss: 0.012092149816453457, Test Loss: 0.06836293637752533\n",
      "Epoch 9302, Train Loss: 0.012089974246919155, Test Loss: 0.06836359202861786\n",
      "Epoch 9303, Train Loss: 0.012087304145097733, Test Loss: 0.0683656632900238\n",
      "Epoch 9304, Train Loss: 0.01208462379872799, Test Loss: 0.0683702901005745\n",
      "Epoch 9305, Train Loss: 0.012082523666322231, Test Loss: 0.06836912781000137\n",
      "Epoch 9306, Train Loss: 0.012079562060534954, Test Loss: 0.06837157160043716\n",
      "Epoch 9307, Train Loss: 0.012077508494257927, Test Loss: 0.06836142390966415\n",
      "Epoch 9308, Train Loss: 0.012075153179466724, Test Loss: 0.06836524605751038\n",
      "Epoch 9309, Train Loss: 0.012072339653968811, Test Loss: 0.06835684180259705\n",
      "Epoch 9310, Train Loss: 0.012069876305758953, Test Loss: 0.0683605819940567\n",
      "Epoch 9311, Train Loss: 0.012067588977515697, Test Loss: 0.06835360080003738\n",
      "Epoch 9312, Train Loss: 0.012065140530467033, Test Loss: 0.06835928559303284\n",
      "Epoch 9313, Train Loss: 0.01206236332654953, Test Loss: 0.06835498660802841\n",
      "Epoch 9314, Train Loss: 0.01206012349575758, Test Loss: 0.06834936141967773\n",
      "Epoch 9315, Train Loss: 0.012057915329933167, Test Loss: 0.06834693998098373\n",
      "Epoch 9316, Train Loss: 0.012055298313498497, Test Loss: 0.06835048645734787\n",
      "Epoch 9317, Train Loss: 0.01205319445580244, Test Loss: 0.06834254413843155\n",
      "Epoch 9318, Train Loss: 0.012050495482981205, Test Loss: 0.06834546476602554\n",
      "Epoch 9319, Train Loss: 0.012048011645674706, Test Loss: 0.06834166496992111\n",
      "Epoch 9320, Train Loss: 0.01204567402601242, Test Loss: 0.0683363825082779\n",
      "Epoch 9321, Train Loss: 0.012043271213769913, Test Loss: 0.06833352893590927\n",
      "Epoch 9322, Train Loss: 0.012041067704558372, Test Loss: 0.06833581626415253\n",
      "Epoch 9323, Train Loss: 0.01203817967325449, Test Loss: 0.06833843141794205\n",
      "Epoch 9324, Train Loss: 0.012035761028528214, Test Loss: 0.06833707541227341\n",
      "Epoch 9325, Train Loss: 0.01203375868499279, Test Loss: 0.06832452863454819\n",
      "Epoch 9326, Train Loss: 0.012031576596200466, Test Loss: 0.068318672478199\n",
      "Epoch 9327, Train Loss: 0.012029619887471199, Test Loss: 0.06831370294094086\n",
      "Epoch 9328, Train Loss: 0.012026430107653141, Test Loss: 0.06831318885087967\n",
      "Epoch 9329, Train Loss: 0.012023462913930416, Test Loss: 0.06831595301628113\n",
      "Epoch 9330, Train Loss: 0.012021541595458984, Test Loss: 0.06832286715507507\n",
      "Epoch 9331, Train Loss: 0.012019568122923374, Test Loss: 0.0683136060833931\n",
      "Epoch 9332, Train Loss: 0.012016115710139275, Test Loss: 0.06832199543714523\n",
      "Epoch 9333, Train Loss: 0.012013884261250496, Test Loss: 0.0683245062828064\n",
      "Epoch 9334, Train Loss: 0.012011385522782803, Test Loss: 0.06832093000411987\n",
      "Epoch 9335, Train Loss: 0.012008963152766228, Test Loss: 0.06831354647874832\n",
      "Epoch 9336, Train Loss: 0.012006527744233608, Test Loss: 0.0683116465806961\n",
      "Epoch 9337, Train Loss: 0.012003998272120953, Test Loss: 0.06831084191799164\n",
      "Epoch 9338, Train Loss: 0.012002089060842991, Test Loss: 0.06830082833766937\n",
      "Epoch 9339, Train Loss: 0.011999084614217281, Test Loss: 0.06831050664186478\n",
      "Epoch 9340, Train Loss: 0.011997032910585403, Test Loss: 0.06830666214227676\n",
      "Epoch 9341, Train Loss: 0.011995122767984867, Test Loss: 0.06829038262367249\n",
      "Epoch 9342, Train Loss: 0.011991755105555058, Test Loss: 0.0683017298579216\n",
      "Epoch 9343, Train Loss: 0.011989458464086056, Test Loss: 0.06830132007598877\n",
      "Epoch 9344, Train Loss: 0.011986972764134407, Test Loss: 0.06829454004764557\n",
      "Epoch 9345, Train Loss: 0.011984740383923054, Test Loss: 0.06829064339399338\n",
      "Epoch 9346, Train Loss: 0.011982542462646961, Test Loss: 0.06828571110963821\n",
      "Epoch 9347, Train Loss: 0.011979715898633003, Test Loss: 0.06828803569078445\n",
      "Epoch 9348, Train Loss: 0.011978275142610073, Test Loss: 0.0682763010263443\n",
      "Epoch 9349, Train Loss: 0.011975460685789585, Test Loss: 0.06827452778816223\n",
      "Epoch 9350, Train Loss: 0.011972486972808838, Test Loss: 0.0682869479060173\n",
      "Epoch 9351, Train Loss: 0.011970579624176025, Test Loss: 0.06828697770833969\n",
      "Epoch 9352, Train Loss: 0.011967803351581097, Test Loss: 0.06828712671995163\n",
      "Epoch 9353, Train Loss: 0.011965330690145493, Test Loss: 0.06828974932432175\n",
      "Epoch 9354, Train Loss: 0.011962717399001122, Test Loss: 0.06828639656305313\n",
      "Epoch 9355, Train Loss: 0.011960398405790329, Test Loss: 0.06828615069389343\n",
      "Epoch 9356, Train Loss: 0.01195798721164465, Test Loss: 0.06828128546476364\n",
      "Epoch 9357, Train Loss: 0.011955630965530872, Test Loss: 0.06827542185783386\n",
      "Epoch 9358, Train Loss: 0.011953018605709076, Test Loss: 0.06827221810817719\n",
      "Epoch 9359, Train Loss: 0.01195074524730444, Test Loss: 0.06827565282583237\n",
      "Epoch 9360, Train Loss: 0.011948417872190475, Test Loss: 0.0682683140039444\n",
      "Epoch 9361, Train Loss: 0.011946245096623898, Test Loss: 0.0682702511548996\n",
      "Epoch 9362, Train Loss: 0.011943276971578598, Test Loss: 0.06826499849557877\n",
      "Epoch 9363, Train Loss: 0.011941767297685146, Test Loss: 0.06826496124267578\n",
      "Epoch 9364, Train Loss: 0.011939886957406998, Test Loss: 0.06825996190309525\n",
      "Epoch 9365, Train Loss: 0.011936130933463573, Test Loss: 0.06826001405715942\n",
      "Epoch 9366, Train Loss: 0.011933590285480022, Test Loss: 0.06825699657201767\n",
      "Epoch 9367, Train Loss: 0.011931242421269417, Test Loss: 0.06825832277536392\n",
      "Epoch 9368, Train Loss: 0.011928820982575417, Test Loss: 0.06826162338256836\n",
      "Epoch 9369, Train Loss: 0.011926372535526752, Test Loss: 0.0682542696595192\n",
      "Epoch 9370, Train Loss: 0.011924190446734428, Test Loss: 0.06825792044401169\n",
      "Epoch 9371, Train Loss: 0.011921491473913193, Test Loss: 0.06825736910104752\n",
      "Epoch 9372, Train Loss: 0.011919431388378143, Test Loss: 0.06826559454202652\n",
      "Epoch 9373, Train Loss: 0.011917040683329105, Test Loss: 0.06825757026672363\n",
      "Epoch 9374, Train Loss: 0.011914338916540146, Test Loss: 0.06825587153434753\n",
      "Epoch 9375, Train Loss: 0.011912287212908268, Test Loss: 0.0682525709271431\n",
      "Epoch 9376, Train Loss: 0.011909544467926025, Test Loss: 0.06825216114521027\n",
      "Epoch 9377, Train Loss: 0.011907135136425495, Test Loss: 0.06824266165494919\n",
      "Epoch 9378, Train Loss: 0.01190448459237814, Test Loss: 0.06824124604463577\n",
      "Epoch 9379, Train Loss: 0.011902089230716228, Test Loss: 0.06824564188718796\n",
      "Epoch 9380, Train Loss: 0.011899877339601517, Test Loss: 0.06823156774044037\n",
      "Epoch 9381, Train Loss: 0.011897246353328228, Test Loss: 0.06823848932981491\n",
      "Epoch 9382, Train Loss: 0.011894802562892437, Test Loss: 0.0682358369231224\n",
      "Epoch 9383, Train Loss: 0.011892863549292088, Test Loss: 0.0682281032204628\n",
      "Epoch 9384, Train Loss: 0.011890287511050701, Test Loss: 0.06823719292879105\n",
      "Epoch 9385, Train Loss: 0.011888254433870316, Test Loss: 0.06822647154331207\n",
      "Epoch 9386, Train Loss: 0.011885333806276321, Test Loss: 0.06823503971099854\n",
      "Epoch 9387, Train Loss: 0.011883002705872059, Test Loss: 0.0682377964258194\n",
      "Epoch 9388, Train Loss: 0.01188044436275959, Test Loss: 0.06823869049549103\n",
      "Epoch 9389, Train Loss: 0.011878123506903648, Test Loss: 0.06822095066308975\n",
      "Epoch 9390, Train Loss: 0.011875640600919724, Test Loss: 0.06822805851697922\n",
      "Epoch 9391, Train Loss: 0.011872975155711174, Test Loss: 0.06823156028985977\n",
      "Epoch 9392, Train Loss: 0.011870852671563625, Test Loss: 0.0682220309972763\n",
      "Epoch 9393, Train Loss: 0.011868372559547424, Test Loss: 0.06821379065513611\n",
      "Epoch 9394, Train Loss: 0.011866127140820026, Test Loss: 0.06820094585418701\n",
      "Epoch 9395, Train Loss: 0.011863256804645061, Test Loss: 0.06820571422576904\n",
      "Epoch 9396, Train Loss: 0.011861437931656837, Test Loss: 0.06820633262395859\n",
      "Epoch 9397, Train Loss: 0.011858498677611351, Test Loss: 0.06820616126060486\n",
      "Epoch 9398, Train Loss: 0.01185611356049776, Test Loss: 0.06820513308048248\n",
      "Epoch 9399, Train Loss: 0.011853615753352642, Test Loss: 0.06820803135633469\n",
      "Epoch 9400, Train Loss: 0.011851463466882706, Test Loss: 0.06820547580718994\n",
      "Epoch 9401, Train Loss: 0.011849134229123592, Test Loss: 0.06819858402013779\n",
      "Epoch 9402, Train Loss: 0.01184661965817213, Test Loss: 0.06820535659790039\n",
      "Epoch 9403, Train Loss: 0.01184411533176899, Test Loss: 0.06819923222064972\n",
      "Epoch 9404, Train Loss: 0.011841936968266964, Test Loss: 0.0682017132639885\n",
      "Epoch 9405, Train Loss: 0.01183901634067297, Test Loss: 0.06819693744182587\n",
      "Epoch 9406, Train Loss: 0.011836768127977848, Test Loss: 0.06818743050098419\n",
      "Epoch 9407, Train Loss: 0.011835123412311077, Test Loss: 0.06817683577537537\n",
      "Epoch 9408, Train Loss: 0.011832213029265404, Test Loss: 0.06817582994699478\n",
      "Epoch 9409, Train Loss: 0.011829441413283348, Test Loss: 0.06819227337837219\n",
      "Epoch 9410, Train Loss: 0.011826980859041214, Test Loss: 0.06818151473999023\n",
      "Epoch 9411, Train Loss: 0.011824951507151127, Test Loss: 0.06817450374364853\n",
      "Epoch 9412, Train Loss: 0.011823107488453388, Test Loss: 0.06816916167736053\n",
      "Epoch 9413, Train Loss: 0.011819873005151749, Test Loss: 0.06817825883626938\n",
      "Epoch 9414, Train Loss: 0.011817571707069874, Test Loss: 0.06816891580820084\n",
      "Epoch 9415, Train Loss: 0.011814919300377369, Test Loss: 0.06816782802343369\n",
      "Epoch 9416, Train Loss: 0.011812949553132057, Test Loss: 0.06815993040800095\n",
      "Epoch 9417, Train Loss: 0.011810223571956158, Test Loss: 0.0681614875793457\n",
      "Epoch 9418, Train Loss: 0.011807630769908428, Test Loss: 0.06816361099481583\n",
      "Epoch 9419, Train Loss: 0.011805190704762936, Test Loss: 0.06816243380308151\n",
      "Epoch 9420, Train Loss: 0.011802919209003448, Test Loss: 0.06815887987613678\n",
      "Epoch 9421, Train Loss: 0.011800559237599373, Test Loss: 0.06816411018371582\n",
      "Epoch 9422, Train Loss: 0.011798282153904438, Test Loss: 0.06816146522760391\n",
      "Epoch 9423, Train Loss: 0.011795749887824059, Test Loss: 0.06815870851278305\n",
      "Epoch 9424, Train Loss: 0.011793372221291065, Test Loss: 0.06814984232187271\n",
      "Epoch 9425, Train Loss: 0.01179063506424427, Test Loss: 0.0681525394320488\n",
      "Epoch 9426, Train Loss: 0.01178900245577097, Test Loss: 0.06814945489168167\n",
      "Epoch 9427, Train Loss: 0.011786220595240593, Test Loss: 0.06815243512392044\n",
      "Epoch 9428, Train Loss: 0.011784116737544537, Test Loss: 0.06815562397241592\n",
      "Epoch 9429, Train Loss: 0.011781366541981697, Test Loss: 0.06815095990896225\n",
      "Epoch 9430, Train Loss: 0.011779524385929108, Test Loss: 0.06814920902252197\n",
      "Epoch 9431, Train Loss: 0.011776373721659184, Test Loss: 0.06813815981149673\n",
      "Epoch 9432, Train Loss: 0.011774608865380287, Test Loss: 0.06813064217567444\n",
      "Epoch 9433, Train Loss: 0.011771608144044876, Test Loss: 0.06813390552997589\n",
      "Epoch 9434, Train Loss: 0.011769547127187252, Test Loss: 0.06813081353902817\n",
      "Epoch 9435, Train Loss: 0.011767212301492691, Test Loss: 0.06813298910856247\n",
      "Epoch 9436, Train Loss: 0.0117645887658, Test Loss: 0.06812942028045654\n",
      "Epoch 9437, Train Loss: 0.011762036010622978, Test Loss: 0.06812970340251923\n",
      "Epoch 9438, Train Loss: 0.011759537272155285, Test Loss: 0.06813514977693558\n",
      "Epoch 9439, Train Loss: 0.01175746787339449, Test Loss: 0.06812315434217453\n",
      "Epoch 9440, Train Loss: 0.011754603125154972, Test Loss: 0.06812521815299988\n",
      "Epoch 9441, Train Loss: 0.011752263642847538, Test Loss: 0.06812535226345062\n",
      "Epoch 9442, Train Loss: 0.011750140227377415, Test Loss: 0.06812076270580292\n",
      "Epoch 9443, Train Loss: 0.011747610755264759, Test Loss: 0.06811141222715378\n",
      "Epoch 9444, Train Loss: 0.011745345778763294, Test Loss: 0.06810947507619858\n",
      "Epoch 9445, Train Loss: 0.01174252014607191, Test Loss: 0.06812172383069992\n",
      "Epoch 9446, Train Loss: 0.011740414425730705, Test Loss: 0.06812483817338943\n",
      "Epoch 9447, Train Loss: 0.011737947352230549, Test Loss: 0.06812813878059387\n",
      "Epoch 9448, Train Loss: 0.011735456995666027, Test Loss: 0.06812074035406113\n",
      "Epoch 9449, Train Loss: 0.011733105406165123, Test Loss: 0.0681191235780716\n",
      "Epoch 9450, Train Loss: 0.0117311030626297, Test Loss: 0.06810718774795532\n",
      "Epoch 9451, Train Loss: 0.01172811072319746, Test Loss: 0.0681140124797821\n",
      "Epoch 9452, Train Loss: 0.011725698597729206, Test Loss: 0.0681067556142807\n",
      "Epoch 9453, Train Loss: 0.011723256669938564, Test Loss: 0.0681120976805687\n",
      "Epoch 9454, Train Loss: 0.011721141636371613, Test Loss: 0.06811243295669556\n",
      "Epoch 9455, Train Loss: 0.011718569323420525, Test Loss: 0.06811443716287613\n",
      "Epoch 9456, Train Loss: 0.011716328561306, Test Loss: 0.06810611486434937\n",
      "Epoch 9457, Train Loss: 0.011714179068803787, Test Loss: 0.06810984015464783\n",
      "Epoch 9458, Train Loss: 0.011711331084370613, Test Loss: 0.06809769570827484\n",
      "Epoch 9459, Train Loss: 0.011709076352417469, Test Loss: 0.06808993220329285\n",
      "Epoch 9460, Train Loss: 0.011706470511853695, Test Loss: 0.06808627396821976\n",
      "Epoch 9461, Train Loss: 0.011704136617481709, Test Loss: 0.0680856853723526\n",
      "Epoch 9462, Train Loss: 0.011701781302690506, Test Loss: 0.06808732450008392\n",
      "Epoch 9463, Train Loss: 0.011699430644512177, Test Loss: 0.06808454543352127\n",
      "Epoch 9464, Train Loss: 0.011697125621140003, Test Loss: 0.06807959824800491\n",
      "Epoch 9465, Train Loss: 0.011694670654833317, Test Loss: 0.06807687878608704\n",
      "Epoch 9466, Train Loss: 0.011692226864397526, Test Loss: 0.06808797270059586\n",
      "Epoch 9467, Train Loss: 0.011690137907862663, Test Loss: 0.06807182729244232\n",
      "Epoch 9468, Train Loss: 0.011687462218105793, Test Loss: 0.06808067858219147\n",
      "Epoch 9469, Train Loss: 0.01168496161699295, Test Loss: 0.06807386875152588\n",
      "Epoch 9470, Train Loss: 0.011682601645588875, Test Loss: 0.0680803656578064\n",
      "Epoch 9471, Train Loss: 0.011680250987410545, Test Loss: 0.0680697038769722\n",
      "Epoch 9472, Train Loss: 0.011677708476781845, Test Loss: 0.06807141751050949\n",
      "Epoch 9473, Train Loss: 0.011675518937408924, Test Loss: 0.06807207316160202\n",
      "Epoch 9474, Train Loss: 0.011673122644424438, Test Loss: 0.06807732582092285\n",
      "Epoch 9475, Train Loss: 0.011670577339828014, Test Loss: 0.06807220727205276\n",
      "Epoch 9476, Train Loss: 0.011668108403682709, Test Loss: 0.06806902587413788\n",
      "Epoch 9477, Train Loss: 0.01166639942675829, Test Loss: 0.06806353479623795\n",
      "Epoch 9478, Train Loss: 0.011663366109132767, Test Loss: 0.06806536763906479\n",
      "Epoch 9479, Train Loss: 0.011661024764180183, Test Loss: 0.06805751472711563\n",
      "Epoch 9480, Train Loss: 0.01165852788835764, Test Loss: 0.06805643439292908\n",
      "Epoch 9481, Train Loss: 0.011656277813017368, Test Loss: 0.06804616749286652\n",
      "Epoch 9482, Train Loss: 0.011654096655547619, Test Loss: 0.06804636120796204\n",
      "Epoch 9483, Train Loss: 0.011651258915662766, Test Loss: 0.0680444985628128\n",
      "Epoch 9484, Train Loss: 0.011648920364677906, Test Loss: 0.06804583221673965\n",
      "Epoch 9485, Train Loss: 0.01164690125733614, Test Loss: 0.06804142147302628\n",
      "Epoch 9486, Train Loss: 0.011644450016319752, Test Loss: 0.06803762912750244\n",
      "Epoch 9487, Train Loss: 0.011642133817076683, Test Loss: 0.0680336058139801\n",
      "Epoch 9488, Train Loss: 0.011639581061899662, Test Loss: 0.0680372342467308\n",
      "Epoch 9489, Train Loss: 0.01163707859814167, Test Loss: 0.06803254783153534\n",
      "Epoch 9490, Train Loss: 0.011634623631834984, Test Loss: 0.06803295761346817\n",
      "Epoch 9491, Train Loss: 0.011632300913333893, Test Loss: 0.06802670657634735\n",
      "Epoch 9492, Train Loss: 0.011629786342382431, Test Loss: 0.06802487373352051\n",
      "Epoch 9493, Train Loss: 0.011627373285591602, Test Loss: 0.06803277879953384\n",
      "Epoch 9494, Train Loss: 0.011625318787992, Test Loss: 0.06802769005298615\n",
      "Epoch 9495, Train Loss: 0.011622831225395203, Test Loss: 0.06801965832710266\n",
      "Epoch 9496, Train Loss: 0.011620542965829372, Test Loss: 0.06802176684141159\n",
      "Epoch 9497, Train Loss: 0.011617841199040413, Test Loss: 0.06801570951938629\n",
      "Epoch 9498, Train Loss: 0.011616012081503868, Test Loss: 0.06800280511379242\n",
      "Epoch 9499, Train Loss: 0.011613699607551098, Test Loss: 0.06800924986600876\n",
      "Epoch 9500, Train Loss: 0.011610842309892178, Test Loss: 0.06801705062389374\n",
      "Epoch 9501, Train Loss: 0.011608527973294258, Test Loss: 0.06801405549049377\n",
      "Epoch 9502, Train Loss: 0.011606314219534397, Test Loss: 0.0679975375533104\n",
      "Epoch 9503, Train Loss: 0.011604910716414452, Test Loss: 0.0680060163140297\n",
      "Epoch 9504, Train Loss: 0.01160153467208147, Test Loss: 0.06800724565982819\n",
      "Epoch 9505, Train Loss: 0.011599292047321796, Test Loss: 0.06799139082431793\n",
      "Epoch 9506, Train Loss: 0.011596945114433765, Test Loss: 0.06799288094043732\n",
      "Epoch 9507, Train Loss: 0.011594448238611221, Test Loss: 0.06799284368753433\n",
      "Epoch 9508, Train Loss: 0.011592128314077854, Test Loss: 0.06801106035709381\n",
      "Epoch 9509, Train Loss: 0.011589888483285904, Test Loss: 0.06800009310245514\n",
      "Epoch 9510, Train Loss: 0.011587454006075859, Test Loss: 0.06798442453145981\n",
      "Epoch 9511, Train Loss: 0.011584335938096046, Test Loss: 0.06799248605966568\n",
      "Epoch 9512, Train Loss: 0.011582854203879833, Test Loss: 0.06799965351819992\n",
      "Epoch 9513, Train Loss: 0.011579819954931736, Test Loss: 0.0679846778512001\n",
      "Epoch 9514, Train Loss: 0.0115772420540452, Test Loss: 0.06799135357141495\n",
      "Epoch 9515, Train Loss: 0.01157511118799448, Test Loss: 0.0679834857583046\n",
      "Epoch 9516, Train Loss: 0.011572771705687046, Test Loss: 0.06798375397920609\n",
      "Epoch 9517, Train Loss: 0.011570492759346962, Test Loss: 0.06797695904970169\n",
      "Epoch 9518, Train Loss: 0.011568747460842133, Test Loss: 0.06797576695680618\n",
      "Epoch 9519, Train Loss: 0.011565566062927246, Test Loss: 0.06797818839550018\n",
      "Epoch 9520, Train Loss: 0.011562992818653584, Test Loss: 0.06797858327627182\n",
      "Epoch 9521, Train Loss: 0.011560811661183834, Test Loss: 0.06798210740089417\n",
      "Epoch 9522, Train Loss: 0.011558298952877522, Test Loss: 0.06797963380813599\n",
      "Epoch 9523, Train Loss: 0.01155620813369751, Test Loss: 0.06797026097774506\n",
      "Epoch 9524, Train Loss: 0.011553965508937836, Test Loss: 0.06796945631504059\n",
      "Epoch 9525, Train Loss: 0.01155129075050354, Test Loss: 0.06796693056821823\n",
      "Epoch 9526, Train Loss: 0.011548777110874653, Test Loss: 0.06796514987945557\n",
      "Epoch 9527, Train Loss: 0.011547300964593887, Test Loss: 0.06796332448720932\n",
      "Epoch 9528, Train Loss: 0.01154413539916277, Test Loss: 0.0679699257016182\n",
      "Epoch 9529, Train Loss: 0.011542360298335552, Test Loss: 0.06797266751527786\n",
      "Epoch 9530, Train Loss: 0.011539461091160774, Test Loss: 0.06795812398195267\n",
      "Epoch 9531, Train Loss: 0.011537023819983006, Test Loss: 0.06795386224985123\n",
      "Epoch 9532, Train Loss: 0.01153454277664423, Test Loss: 0.06796229630708694\n",
      "Epoch 9533, Train Loss: 0.011532336473464966, Test Loss: 0.06796093285083771\n",
      "Epoch 9534, Train Loss: 0.01152985729277134, Test Loss: 0.06795446574687958\n",
      "Epoch 9535, Train Loss: 0.01152758952230215, Test Loss: 0.06795244663953781\n",
      "Epoch 9536, Train Loss: 0.011526358313858509, Test Loss: 0.06793757528066635\n",
      "Epoch 9537, Train Loss: 0.01152282115072012, Test Loss: 0.06794104725122452\n",
      "Epoch 9538, Train Loss: 0.011520474217832088, Test Loss: 0.06793981790542603\n",
      "Epoch 9539, Train Loss: 0.0115182651206851, Test Loss: 0.06793130934238434\n",
      "Epoch 9540, Train Loss: 0.011515829712152481, Test Loss: 0.06794041395187378\n",
      "Epoch 9541, Train Loss: 0.011513594537973404, Test Loss: 0.06793753057718277\n",
      "Epoch 9542, Train Loss: 0.011511167511343956, Test Loss: 0.06793757528066635\n",
      "Epoch 9543, Train Loss: 0.01150871068239212, Test Loss: 0.06793881952762604\n",
      "Epoch 9544, Train Loss: 0.011506419628858566, Test Loss: 0.06793120503425598\n",
      "Epoch 9545, Train Loss: 0.011503891088068485, Test Loss: 0.06793007999658585\n",
      "Epoch 9546, Train Loss: 0.011501647531986237, Test Loss: 0.06792403012514114\n",
      "Epoch 9547, Train Loss: 0.011499248445034027, Test Loss: 0.0679255872964859\n",
      "Epoch 9548, Train Loss: 0.011496745981276035, Test Loss: 0.06792701035737991\n",
      "Epoch 9549, Train Loss: 0.011494530364871025, Test Loss: 0.06792911142110825\n",
      "Epoch 9550, Train Loss: 0.011492375284433365, Test Loss: 0.0679197907447815\n",
      "Epoch 9551, Train Loss: 0.011489784345030785, Test Loss: 0.06792177259922028\n",
      "Epoch 9552, Train Loss: 0.011488081887364388, Test Loss: 0.06791294366121292\n",
      "Epoch 9553, Train Loss: 0.011485441587865353, Test Loss: 0.06791418045759201\n",
      "Epoch 9554, Train Loss: 0.011483042500913143, Test Loss: 0.0679081603884697\n",
      "Epoch 9555, Train Loss: 0.011480522342026234, Test Loss: 0.06790813058614731\n",
      "Epoch 9556, Train Loss: 0.011478492058813572, Test Loss: 0.06790648400783539\n",
      "Epoch 9557, Train Loss: 0.011476117186248302, Test Loss: 0.06790749728679657\n",
      "Epoch 9558, Train Loss: 0.011473369784653187, Test Loss: 0.06790486723184586\n",
      "Epoch 9559, Train Loss: 0.011470898054540157, Test Loss: 0.06790167838335037\n",
      "Epoch 9560, Train Loss: 0.011468597687780857, Test Loss: 0.06789756566286087\n",
      "Epoch 9561, Train Loss: 0.011466301046311855, Test Loss: 0.06789502501487732\n",
      "Epoch 9562, Train Loss: 0.011463925242424011, Test Loss: 0.06789328902959824\n",
      "Epoch 9563, Train Loss: 0.01146186888217926, Test Loss: 0.06788011640310287\n",
      "Epoch 9564, Train Loss: 0.01145918108522892, Test Loss: 0.0678875520825386\n",
      "Epoch 9565, Train Loss: 0.011456902138888836, Test Loss: 0.06789633631706238\n",
      "Epoch 9566, Train Loss: 0.011454409919679165, Test Loss: 0.06789045035839081\n",
      "Epoch 9567, Train Loss: 0.011452343314886093, Test Loss: 0.06788763403892517\n",
      "Epoch 9568, Train Loss: 0.011449814774096012, Test Loss: 0.06788214296102524\n",
      "Epoch 9569, Train Loss: 0.011447478085756302, Test Loss: 0.06787912547588348\n",
      "Epoch 9570, Train Loss: 0.011444932781159878, Test Loss: 0.06788469105958939\n",
      "Epoch 9571, Train Loss: 0.011442930437624454, Test Loss: 0.06787792593240738\n",
      "Epoch 9572, Train Loss: 0.01144128292798996, Test Loss: 0.06788696348667145\n",
      "Epoch 9573, Train Loss: 0.011437933892011642, Test Loss: 0.0678807869553566\n",
      "Epoch 9574, Train Loss: 0.011435856111347675, Test Loss: 0.06787121295928955\n",
      "Epoch 9575, Train Loss: 0.011433585546910763, Test Loss: 0.0678744986653328\n",
      "Epoch 9576, Train Loss: 0.011430812999606133, Test Loss: 0.06788194924592972\n",
      "Epoch 9577, Train Loss: 0.01142862532287836, Test Loss: 0.06788099557161331\n",
      "Epoch 9578, Train Loss: 0.011426512151956558, Test Loss: 0.06787195801734924\n",
      "Epoch 9579, Train Loss: 0.011423971503973007, Test Loss: 0.0678783655166626\n",
      "Epoch 9580, Train Loss: 0.011421987786889076, Test Loss: 0.06787892431020737\n",
      "Epoch 9581, Train Loss: 0.011419239453971386, Test Loss: 0.06786426156759262\n",
      "Epoch 9582, Train Loss: 0.011416925117373466, Test Loss: 0.0678640753030777\n",
      "Epoch 9583, Train Loss: 0.011415151879191399, Test Loss: 0.06785707175731659\n",
      "Epoch 9584, Train Loss: 0.011412848718464375, Test Loss: 0.06785879284143448\n",
      "Epoch 9585, Train Loss: 0.01141035370528698, Test Loss: 0.06784949451684952\n",
      "Epoch 9586, Train Loss: 0.01140827126801014, Test Loss: 0.06784689426422119\n",
      "Epoch 9587, Train Loss: 0.01140550896525383, Test Loss: 0.0678517147898674\n",
      "Epoch 9588, Train Loss: 0.011402828618884087, Test Loss: 0.06785562634468079\n",
      "Epoch 9589, Train Loss: 0.01140074897557497, Test Loss: 0.06785853952169418\n",
      "Epoch 9590, Train Loss: 0.01139818038791418, Test Loss: 0.06785821914672852\n",
      "Epoch 9591, Train Loss: 0.011395890265703201, Test Loss: 0.06785481423139572\n",
      "Epoch 9592, Train Loss: 0.011393601074814796, Test Loss: 0.06784945726394653\n",
      "Epoch 9593, Train Loss: 0.01139126904308796, Test Loss: 0.06785256415605545\n",
      "Epoch 9594, Train Loss: 0.01138920709490776, Test Loss: 0.06784354895353317\n",
      "Epoch 9595, Train Loss: 0.011386611498892307, Test Loss: 0.06784364581108093\n",
      "Epoch 9596, Train Loss: 0.011384150013327599, Test Loss: 0.06784232705831528\n",
      "Epoch 9597, Train Loss: 0.011382178403437138, Test Loss: 0.06783655285835266\n",
      "Epoch 9598, Train Loss: 0.01137950737029314, Test Loss: 0.06783652305603027\n",
      "Epoch 9599, Train Loss: 0.011377439834177494, Test Loss: 0.06782644987106323\n",
      "Epoch 9600, Train Loss: 0.011375067755579948, Test Loss: 0.06782670319080353\n",
      "Epoch 9601, Train Loss: 0.01137285865843296, Test Loss: 0.06781576573848724\n",
      "Epoch 9602, Train Loss: 0.011370253749191761, Test Loss: 0.0678192600607872\n",
      "Epoch 9603, Train Loss: 0.011367830447852612, Test Loss: 0.06782524287700653\n",
      "Epoch 9604, Train Loss: 0.011365478858351707, Test Loss: 0.06782075762748718\n",
      "Epoch 9605, Train Loss: 0.011363610625267029, Test Loss: 0.06781134009361267\n",
      "Epoch 9606, Train Loss: 0.011360843665897846, Test Loss: 0.06781475991010666\n",
      "Epoch 9607, Train Loss: 0.011358627118170261, Test Loss: 0.06780745089054108\n",
      "Epoch 9608, Train Loss: 0.011356281116604805, Test Loss: 0.06780223548412323\n",
      "Epoch 9609, Train Loss: 0.011354029178619385, Test Loss: 0.0678107962012291\n",
      "Epoch 9610, Train Loss: 0.01135150995105505, Test Loss: 0.0678127259016037\n",
      "Epoch 9611, Train Loss: 0.011349364183843136, Test Loss: 0.06781819462776184\n",
      "Epoch 9612, Train Loss: 0.011346936225891113, Test Loss: 0.06781934201717377\n",
      "Epoch 9613, Train Loss: 0.011345038190484047, Test Loss: 0.06781534105539322\n",
      "Epoch 9614, Train Loss: 0.01134275458753109, Test Loss: 0.06780313700437546\n",
      "Epoch 9615, Train Loss: 0.011340120807290077, Test Loss: 0.06779542565345764\n",
      "Epoch 9616, Train Loss: 0.011337587609887123, Test Loss: 0.0678061991930008\n",
      "Epoch 9617, Train Loss: 0.011335285380482674, Test Loss: 0.06780919432640076\n",
      "Epoch 9618, Train Loss: 0.011333225294947624, Test Loss: 0.06781202554702759\n",
      "Epoch 9619, Train Loss: 0.011330566368997097, Test Loss: 0.06780082732439041\n",
      "Epoch 9620, Train Loss: 0.011328795924782753, Test Loss: 0.06779390573501587\n",
      "Epoch 9621, Train Loss: 0.01132665853947401, Test Loss: 0.06778685748577118\n",
      "Epoch 9622, Train Loss: 0.01132389809936285, Test Loss: 0.0677943155169487\n",
      "Epoch 9623, Train Loss: 0.01132157165557146, Test Loss: 0.06778638064861298\n",
      "Epoch 9624, Train Loss: 0.01131904125213623, Test Loss: 0.06779057532548904\n",
      "Epoch 9625, Train Loss: 0.01131665613502264, Test Loss: 0.06779176741838455\n",
      "Epoch 9626, Train Loss: 0.011314459145069122, Test Loss: 0.06779240071773529\n",
      "Epoch 9627, Train Loss: 0.011312106624245644, Test Loss: 0.06779094785451889\n",
      "Epoch 9628, Train Loss: 0.011309710331261158, Test Loss: 0.0677848756313324\n",
      "Epoch 9629, Train Loss: 0.011307360604405403, Test Loss: 0.06778857111930847\n",
      "Epoch 9630, Train Loss: 0.01130520086735487, Test Loss: 0.06777957826852798\n",
      "Epoch 9631, Train Loss: 0.011302738450467587, Test Loss: 0.06778296083211899\n",
      "Epoch 9632, Train Loss: 0.011300776153802872, Test Loss: 0.06777218729257584\n",
      "Epoch 9633, Train Loss: 0.011298155412077904, Test Loss: 0.06777103990316391\n",
      "Epoch 9634, Train Loss: 0.01129667367786169, Test Loss: 0.06776100397109985\n",
      "Epoch 9635, Train Loss: 0.011294306255877018, Test Loss: 0.06776362657546997\n",
      "Epoch 9636, Train Loss: 0.01129118911921978, Test Loss: 0.06777001172304153\n",
      "Epoch 9637, Train Loss: 0.011289176531136036, Test Loss: 0.06776832789182663\n",
      "Epoch 9638, Train Loss: 0.01128706056624651, Test Loss: 0.06776820868253708\n",
      "Epoch 9639, Train Loss: 0.011284168809652328, Test Loss: 0.06776770204305649\n",
      "Epoch 9640, Train Loss: 0.011282077990472317, Test Loss: 0.0677616223692894\n",
      "Epoch 9641, Train Loss: 0.011279714293777943, Test Loss: 0.06775977462530136\n",
      "Epoch 9642, Train Loss: 0.01127777062356472, Test Loss: 0.06774876266717911\n",
      "Epoch 9643, Train Loss: 0.011275028809905052, Test Loss: 0.06775474548339844\n",
      "Epoch 9644, Train Loss: 0.01127313170582056, Test Loss: 0.06775078922510147\n",
      "Epoch 9645, Train Loss: 0.011270611546933651, Test Loss: 0.06776302307844162\n",
      "Epoch 9646, Train Loss: 0.011268050409853458, Test Loss: 0.06775517016649246\n",
      "Epoch 9647, Train Loss: 0.011265634559094906, Test Loss: 0.06774861365556717\n",
      "Epoch 9648, Train Loss: 0.01126361358910799, Test Loss: 0.067746102809906\n",
      "Epoch 9649, Train Loss: 0.011261516250669956, Test Loss: 0.06773989647626877\n",
      "Epoch 9650, Train Loss: 0.011258725076913834, Test Loss: 0.06774460524320602\n",
      "Epoch 9651, Train Loss: 0.011256693862378597, Test Loss: 0.06773587316274643\n",
      "Epoch 9652, Train Loss: 0.01125448476523161, Test Loss: 0.06773242354393005\n",
      "Epoch 9653, Train Loss: 0.011251911520957947, Test Loss: 0.06773460656404495\n",
      "Epoch 9654, Train Loss: 0.011250007897615433, Test Loss: 0.06773189455270767\n",
      "Epoch 9655, Train Loss: 0.011247357353568077, Test Loss: 0.06772725284099579\n",
      "Epoch 9656, Train Loss: 0.011245819739997387, Test Loss: 0.06772318482398987\n",
      "Epoch 9657, Train Loss: 0.01124285813421011, Test Loss: 0.06772565096616745\n",
      "Epoch 9658, Train Loss: 0.0112405214458704, Test Loss: 0.06772330403327942\n",
      "Epoch 9659, Train Loss: 0.01123830396682024, Test Loss: 0.06771985441446304\n",
      "Epoch 9660, Train Loss: 0.011235729791224003, Test Loss: 0.06772123277187347\n",
      "Epoch 9661, Train Loss: 0.01123378612101078, Test Loss: 0.06772041320800781\n",
      "Epoch 9662, Train Loss: 0.011231106705963612, Test Loss: 0.06772468984127045\n",
      "Epoch 9663, Train Loss: 0.011228742077946663, Test Loss: 0.0677250400185585\n",
      "Epoch 9664, Train Loss: 0.011226600967347622, Test Loss: 0.06771686673164368\n",
      "Epoch 9665, Train Loss: 0.011224418878555298, Test Loss: 0.06771683692932129\n",
      "Epoch 9666, Train Loss: 0.0112221147865057, Test Loss: 0.06770981103181839\n",
      "Epoch 9667, Train Loss: 0.011219709180295467, Test Loss: 0.06771044433116913\n",
      "Epoch 9668, Train Loss: 0.011217627674341202, Test Loss: 0.0677105039358139\n",
      "Epoch 9669, Train Loss: 0.011215652339160442, Test Loss: 0.06769852340221405\n",
      "Epoch 9670, Train Loss: 0.011212924495339394, Test Loss: 0.0676986575126648\n",
      "Epoch 9671, Train Loss: 0.011210684664547443, Test Loss: 0.06770182400941849\n",
      "Epoch 9672, Train Loss: 0.011208080686628819, Test Loss: 0.0676976665854454\n",
      "Epoch 9673, Train Loss: 0.011205902323126793, Test Loss: 0.06770709902048111\n",
      "Epoch 9674, Train Loss: 0.011203850619494915, Test Loss: 0.06770019978284836\n",
      "Epoch 9675, Train Loss: 0.01120118796825409, Test Loss: 0.06769859790802002\n",
      "Epoch 9676, Train Loss: 0.011198875494301319, Test Loss: 0.06769450753927231\n",
      "Epoch 9677, Train Loss: 0.011196398176252842, Test Loss: 0.06769557297229767\n",
      "Epoch 9678, Train Loss: 0.011194458231329918, Test Loss: 0.06768972426652908\n",
      "Epoch 9679, Train Loss: 0.011192122474312782, Test Loss: 0.06768619269132614\n",
      "Epoch 9680, Train Loss: 0.011189586482942104, Test Loss: 0.06769582629203796\n",
      "Epoch 9681, Train Loss: 0.011187477968633175, Test Loss: 0.06768656522035599\n",
      "Epoch 9682, Train Loss: 0.011185506358742714, Test Loss: 0.06769078224897385\n",
      "Epoch 9683, Train Loss: 0.011183208785951138, Test Loss: 0.06769172847270966\n",
      "Epoch 9684, Train Loss: 0.01118047721683979, Test Loss: 0.06768771260976791\n",
      "Epoch 9685, Train Loss: 0.011178121902048588, Test Loss: 0.06768182665109634\n",
      "Epoch 9686, Train Loss: 0.01117602176964283, Test Loss: 0.06767900288105011\n",
      "Epoch 9687, Train Loss: 0.011174650862812996, Test Loss: 0.06768616288900375\n",
      "Epoch 9688, Train Loss: 0.011171308346092701, Test Loss: 0.06768530607223511\n",
      "Epoch 9689, Train Loss: 0.011168936267495155, Test Loss: 0.06767930835485458\n",
      "Epoch 9690, Train Loss: 0.011166640557348728, Test Loss: 0.06767681986093521\n",
      "Epoch 9691, Train Loss: 0.011164519004523754, Test Loss: 0.06767616420984268\n",
      "Epoch 9692, Train Loss: 0.01116211898624897, Test Loss: 0.0676669031381607\n",
      "Epoch 9693, Train Loss: 0.01116007100790739, Test Loss: 0.06766466051340103\n",
      "Epoch 9694, Train Loss: 0.011158092878758907, Test Loss: 0.06766952574253082\n",
      "Epoch 9695, Train Loss: 0.011155735701322556, Test Loss: 0.06766436994075775\n",
      "Epoch 9696, Train Loss: 0.011153021827340126, Test Loss: 0.06766719371080399\n",
      "Epoch 9697, Train Loss: 0.011150593869388103, Test Loss: 0.06766252219676971\n",
      "Epoch 9698, Train Loss: 0.011148586869239807, Test Loss: 0.06766141206026077\n",
      "Epoch 9699, Train Loss: 0.011146405711770058, Test Loss: 0.0676661804318428\n",
      "Epoch 9700, Train Loss: 0.011143678799271584, Test Loss: 0.06766369938850403\n",
      "Epoch 9701, Train Loss: 0.011141722090542316, Test Loss: 0.06766548752784729\n",
      "Epoch 9702, Train Loss: 0.011139328591525555, Test Loss: 0.0676501989364624\n",
      "Epoch 9703, Train Loss: 0.01113694254308939, Test Loss: 0.06764917820692062\n",
      "Epoch 9704, Train Loss: 0.011134781874716282, Test Loss: 0.06764493882656097\n",
      "Epoch 9705, Train Loss: 0.011132639832794666, Test Loss: 0.06763993948698044\n",
      "Epoch 9706, Train Loss: 0.011130142956972122, Test Loss: 0.06764647364616394\n",
      "Epoch 9707, Train Loss: 0.011127729900181293, Test Loss: 0.06764699518680573\n",
      "Epoch 9708, Train Loss: 0.01112569309771061, Test Loss: 0.06764459609985352\n",
      "Epoch 9709, Train Loss: 0.011123443953692913, Test Loss: 0.06763911992311478\n",
      "Epoch 9710, Train Loss: 0.011121049523353577, Test Loss: 0.06764546781778336\n",
      "Epoch 9711, Train Loss: 0.011118598282337189, Test Loss: 0.06764224916696548\n",
      "Epoch 9712, Train Loss: 0.011117094196379185, Test Loss: 0.06764232367277145\n",
      "Epoch 9713, Train Loss: 0.011114074848592281, Test Loss: 0.06763219833374023\n",
      "Epoch 9714, Train Loss: 0.01111175399273634, Test Loss: 0.06763146072626114\n",
      "Epoch 9715, Train Loss: 0.011110077612102032, Test Loss: 0.06763002276420593\n",
      "Epoch 9716, Train Loss: 0.011107245460152626, Test Loss: 0.06762924045324326\n",
      "Epoch 9717, Train Loss: 0.011104708537459373, Test Loss: 0.06763261556625366\n",
      "Epoch 9718, Train Loss: 0.0111027667298913, Test Loss: 0.06763290613889694\n",
      "Epoch 9719, Train Loss: 0.01110024657100439, Test Loss: 0.06762274354696274\n",
      "Epoch 9720, Train Loss: 0.011098346672952175, Test Loss: 0.06762316823005676\n",
      "Epoch 9721, Train Loss: 0.011095681227743626, Test Loss: 0.06761489063501358\n",
      "Epoch 9722, Train Loss: 0.011093384586274624, Test Loss: 0.06761445105075836\n",
      "Epoch 9723, Train Loss: 0.011091155931353569, Test Loss: 0.06761936098337173\n",
      "Epoch 9724, Train Loss: 0.011088715866208076, Test Loss: 0.06761675328016281\n",
      "Epoch 9725, Train Loss: 0.011087952181696892, Test Loss: 0.0676104724407196\n",
      "Epoch 9726, Train Loss: 0.011085056699812412, Test Loss: 0.06760850548744202\n",
      "Epoch 9727, Train Loss: 0.011082053184509277, Test Loss: 0.0676078200340271\n",
      "Epoch 9728, Train Loss: 0.011080199852585793, Test Loss: 0.06759575009346008\n",
      "Epoch 9729, Train Loss: 0.01107767317444086, Test Loss: 0.06759706139564514\n",
      "Epoch 9730, Train Loss: 0.011075073853135109, Test Loss: 0.06759858131408691\n",
      "Epoch 9731, Train Loss: 0.01107349619269371, Test Loss: 0.06759079545736313\n",
      "Epoch 9732, Train Loss: 0.011070490814745426, Test Loss: 0.06760060787200928\n",
      "Epoch 9733, Train Loss: 0.011068754829466343, Test Loss: 0.06759002804756165\n",
      "Epoch 9734, Train Loss: 0.011066609993577003, Test Loss: 0.0675869807600975\n",
      "Epoch 9735, Train Loss: 0.011063789017498493, Test Loss: 0.06760378926992416\n",
      "Epoch 9736, Train Loss: 0.011061711236834526, Test Loss: 0.06759646534919739\n",
      "Epoch 9737, Train Loss: 0.011059937998652458, Test Loss: 0.06758159399032593\n",
      "Epoch 9738, Train Loss: 0.011056982912123203, Test Loss: 0.06758906692266464\n",
      "Epoch 9739, Train Loss: 0.01105587650090456, Test Loss: 0.06758200377225876\n",
      "Epoch 9740, Train Loss: 0.01105259358882904, Test Loss: 0.06758604943752289\n",
      "Epoch 9741, Train Loss: 0.011050521396100521, Test Loss: 0.0675889104604721\n",
      "Epoch 9742, Train Loss: 0.011047757230699062, Test Loss: 0.06758774816989899\n",
      "Epoch 9743, Train Loss: 0.011045442894101143, Test Loss: 0.06758684664964676\n",
      "Epoch 9744, Train Loss: 0.011043539270758629, Test Loss: 0.06757757812738419\n",
      "Epoch 9745, Train Loss: 0.01104107964783907, Test Loss: 0.06758163869380951\n",
      "Epoch 9746, Train Loss: 0.011038831435143948, Test Loss: 0.06757397949695587\n",
      "Epoch 9747, Train Loss: 0.011036316864192486, Test Loss: 0.067571260035038\n",
      "Epoch 9748, Train Loss: 0.011034084483981133, Test Loss: 0.0675782561302185\n",
      "Epoch 9749, Train Loss: 0.011031879112124443, Test Loss: 0.06758109480142593\n",
      "Epoch 9750, Train Loss: 0.011030102148652077, Test Loss: 0.06756378710269928\n",
      "Epoch 9751, Train Loss: 0.011027244850993156, Test Loss: 0.06756949424743652\n",
      "Epoch 9752, Train Loss: 0.011025024577975273, Test Loss: 0.0675600990653038\n",
      "Epoch 9753, Train Loss: 0.011022842489182949, Test Loss: 0.06756031513214111\n",
      "Epoch 9754, Train Loss: 0.011021038517355919, Test Loss: 0.06756477802991867\n",
      "Epoch 9755, Train Loss: 0.011018279008567333, Test Loss: 0.06756287068128586\n",
      "Epoch 9756, Train Loss: 0.011016866192221642, Test Loss: 0.06756384670734406\n",
      "Epoch 9757, Train Loss: 0.011013785377144814, Test Loss: 0.06755771487951279\n",
      "Epoch 9758, Train Loss: 0.011011459864675999, Test Loss: 0.06756066530942917\n",
      "Epoch 9759, Train Loss: 0.011009542271494865, Test Loss: 0.06755759567022324\n",
      "Epoch 9760, Train Loss: 0.011007164604961872, Test Loss: 0.06755219399929047\n",
      "Epoch 9761, Train Loss: 0.011004537343978882, Test Loss: 0.0675550103187561\n",
      "Epoch 9762, Train Loss: 0.0110024930909276, Test Loss: 0.06755607575178146\n",
      "Epoch 9763, Train Loss: 0.011000106111168861, Test Loss: 0.06754475831985474\n",
      "Epoch 9764, Train Loss: 0.010997702367603779, Test Loss: 0.06754828244447708\n",
      "Epoch 9765, Train Loss: 0.010995701886713505, Test Loss: 0.06753900647163391\n",
      "Epoch 9766, Train Loss: 0.010993906296789646, Test Loss: 0.0675342008471489\n",
      "Epoch 9767, Train Loss: 0.010991791263222694, Test Loss: 0.06753071397542953\n",
      "Epoch 9768, Train Loss: 0.010988781228661537, Test Loss: 0.0675351470708847\n",
      "Epoch 9769, Train Loss: 0.010986469686031342, Test Loss: 0.06753398478031158\n",
      "Epoch 9770, Train Loss: 0.010984052903950214, Test Loss: 0.06753694266080856\n",
      "Epoch 9771, Train Loss: 0.010981893166899681, Test Loss: 0.06753823906183243\n",
      "Epoch 9772, Train Loss: 0.01097958441823721, Test Loss: 0.067538321018219\n",
      "Epoch 9773, Train Loss: 0.010977344587445259, Test Loss: 0.067532017827034\n",
      "Epoch 9774, Train Loss: 0.010975198820233345, Test Loss: 0.0675346776843071\n",
      "Epoch 9775, Train Loss: 0.010972822085022926, Test Loss: 0.06753218919038773\n",
      "Epoch 9776, Train Loss: 0.010970458388328552, Test Loss: 0.06752399355173111\n",
      "Epoch 9777, Train Loss: 0.010968239046633244, Test Loss: 0.06752137839794159\n",
      "Epoch 9778, Train Loss: 0.010965876281261444, Test Loss: 0.06751897931098938\n",
      "Epoch 9779, Train Loss: 0.0109636215493083, Test Loss: 0.06751716881990433\n",
      "Epoch 9780, Train Loss: 0.010961437597870827, Test Loss: 0.06751169264316559\n",
      "Epoch 9781, Train Loss: 0.010959708131849766, Test Loss: 0.06750418245792389\n",
      "Epoch 9782, Train Loss: 0.010956907644867897, Test Loss: 0.06751543283462524\n",
      "Epoch 9783, Train Loss: 0.010954541154205799, Test Loss: 0.06751527637243271\n",
      "Epoch 9784, Train Loss: 0.010952389799058437, Test Loss: 0.06750443577766418\n",
      "Epoch 9785, Train Loss: 0.010950515978038311, Test Loss: 0.06750492006540298\n",
      "Epoch 9786, Train Loss: 0.010948188602924347, Test Loss: 0.06751430779695511\n",
      "Epoch 9787, Train Loss: 0.010945922695100307, Test Loss: 0.06750398874282837\n",
      "Epoch 9788, Train Loss: 0.010943496599793434, Test Loss: 0.06750157475471497\n",
      "Epoch 9789, Train Loss: 0.010941765271127224, Test Loss: 0.06749153137207031\n",
      "Epoch 9790, Train Loss: 0.010939663276076317, Test Loss: 0.06748493015766144\n",
      "Epoch 9791, Train Loss: 0.01093640923500061, Test Loss: 0.06749197840690613\n",
      "Epoch 9792, Train Loss: 0.010934954509139061, Test Loss: 0.06748881191015244\n",
      "Epoch 9793, Train Loss: 0.010932404547929764, Test Loss: 0.06748128682374954\n",
      "Epoch 9794, Train Loss: 0.010930027812719345, Test Loss: 0.06748033314943314\n",
      "Epoch 9795, Train Loss: 0.010927749797701836, Test Loss: 0.06748949736356735\n",
      "Epoch 9796, Train Loss: 0.010925180278718472, Test Loss: 0.06748104095458984\n",
      "Epoch 9797, Train Loss: 0.01092301495373249, Test Loss: 0.06748323142528534\n",
      "Epoch 9798, Train Loss: 0.010920757427811623, Test Loss: 0.0674860030412674\n",
      "Epoch 9799, Train Loss: 0.010918385349214077, Test Loss: 0.06748294085264206\n",
      "Epoch 9800, Train Loss: 0.010916497558355331, Test Loss: 0.06749006360769272\n",
      "Epoch 9801, Train Loss: 0.010913929902017117, Test Loss: 0.06748485565185547\n",
      "Epoch 9802, Train Loss: 0.010911677032709122, Test Loss: 0.06748571246862411\n",
      "Epoch 9803, Train Loss: 0.010910101234912872, Test Loss: 0.06749261915683746\n",
      "Epoch 9804, Train Loss: 0.010907076299190521, Test Loss: 0.06748033314943314\n",
      "Epoch 9805, Train Loss: 0.010905196890234947, Test Loss: 0.06747066229581833\n",
      "Epoch 9806, Train Loss: 0.010902891866862774, Test Loss: 0.06747476011514664\n",
      "Epoch 9807, Train Loss: 0.010900555178523064, Test Loss: 0.06746932864189148\n",
      "Epoch 9808, Train Loss: 0.010898074135184288, Test Loss: 0.06747458875179291\n",
      "Epoch 9809, Train Loss: 0.01089596375823021, Test Loss: 0.06746872514486313\n",
      "Epoch 9810, Train Loss: 0.010894255712628365, Test Loss: 0.06746774911880493\n",
      "Epoch 9811, Train Loss: 0.010891512036323547, Test Loss: 0.06747504323720932\n",
      "Epoch 9812, Train Loss: 0.010889274999499321, Test Loss: 0.06746205687522888\n",
      "Epoch 9813, Train Loss: 0.010887051932513714, Test Loss: 0.06746064871549606\n",
      "Epoch 9814, Train Loss: 0.010884811170399189, Test Loss: 0.06745356321334839\n",
      "Epoch 9815, Train Loss: 0.010883028618991375, Test Loss: 0.06745236366987228\n",
      "Epoch 9816, Train Loss: 0.010880282148718834, Test Loss: 0.06745122373104095\n",
      "Epoch 9817, Train Loss: 0.010877758264541626, Test Loss: 0.06745389103889465\n",
      "Epoch 9818, Train Loss: 0.010875537991523743, Test Loss: 0.06745077669620514\n",
      "Epoch 9819, Train Loss: 0.010873349383473396, Test Loss: 0.06744580715894699\n",
      "Epoch 9820, Train Loss: 0.01087107788771391, Test Loss: 0.06744278222322464\n",
      "Epoch 9821, Train Loss: 0.010868756100535393, Test Loss: 0.06744521111249924\n",
      "Epoch 9822, Train Loss: 0.010866580531001091, Test Loss: 0.06744172424077988\n",
      "Epoch 9823, Train Loss: 0.010864273644983768, Test Loss: 0.0674450695514679\n",
      "Epoch 9824, Train Loss: 0.010861970484256744, Test Loss: 0.06744380295276642\n",
      "Epoch 9825, Train Loss: 0.010859852656722069, Test Loss: 0.06744109839200974\n",
      "Epoch 9826, Train Loss: 0.01085780467838049, Test Loss: 0.06744147092103958\n",
      "Epoch 9827, Train Loss: 0.01085560955107212, Test Loss: 0.06742895394563675\n",
      "Epoch 9828, Train Loss: 0.010853495448827744, Test Loss: 0.06742393970489502\n",
      "Epoch 9829, Train Loss: 0.010851648636162281, Test Loss: 0.06742240488529205\n",
      "Epoch 9830, Train Loss: 0.010848812758922577, Test Loss: 0.06742063164710999\n",
      "Epoch 9831, Train Loss: 0.010846412740647793, Test Loss: 0.06742621958255768\n",
      "Epoch 9832, Train Loss: 0.010843982920050621, Test Loss: 0.06742246448993683\n",
      "Epoch 9833, Train Loss: 0.01084193680435419, Test Loss: 0.06742668896913528\n",
      "Epoch 9834, Train Loss: 0.010839632712304592, Test Loss: 0.06741917878389359\n",
      "Epoch 9835, Train Loss: 0.010837499983608723, Test Loss: 0.0674222931265831\n",
      "Epoch 9836, Train Loss: 0.010835970751941204, Test Loss: 0.06740248203277588\n",
      "Epoch 9837, Train Loss: 0.010832768864929676, Test Loss: 0.06740758568048477\n",
      "Epoch 9838, Train Loss: 0.010830587707459927, Test Loss: 0.0674140602350235\n",
      "Epoch 9839, Train Loss: 0.01082831621170044, Test Loss: 0.0674108937382698\n",
      "Epoch 9840, Train Loss: 0.010825932025909424, Test Loss: 0.06741448491811752\n",
      "Epoch 9841, Train Loss: 0.010823762975633144, Test Loss: 0.06741025298833847\n",
      "Epoch 9842, Train Loss: 0.010821493342518806, Test Loss: 0.06741706281900406\n",
      "Epoch 9843, Train Loss: 0.010820457711815834, Test Loss: 0.06742025911808014\n",
      "Epoch 9844, Train Loss: 0.010817823931574821, Test Loss: 0.06740424036979675\n",
      "Epoch 9845, Train Loss: 0.010814822278916836, Test Loss: 0.06740700453519821\n",
      "Epoch 9846, Train Loss: 0.010812345892190933, Test Loss: 0.06739978492259979\n",
      "Epoch 9847, Train Loss: 0.010810102336108685, Test Loss: 0.06739454716444016\n",
      "Epoch 9848, Train Loss: 0.010807850398123264, Test Loss: 0.06739185005426407\n",
      "Epoch 9849, Train Loss: 0.010806413367390633, Test Loss: 0.06737792491912842\n",
      "Epoch 9850, Train Loss: 0.010803704150021076, Test Loss: 0.06738008558750153\n",
      "Epoch 9851, Train Loss: 0.010801760479807854, Test Loss: 0.0673743188381195\n",
      "Epoch 9852, Train Loss: 0.010799607262015343, Test Loss: 0.06738042831420898\n",
      "Epoch 9853, Train Loss: 0.010796675458550453, Test Loss: 0.0673867017030716\n",
      "Epoch 9854, Train Loss: 0.010794729925692081, Test Loss: 0.06739350408315659\n",
      "Epoch 9855, Train Loss: 0.010792404413223267, Test Loss: 0.06737985461950302\n",
      "Epoch 9856, Train Loss: 0.010790470987558365, Test Loss: 0.06737367808818817\n",
      "Epoch 9857, Train Loss: 0.010787930339574814, Test Loss: 0.06737321615219116\n",
      "Epoch 9858, Train Loss: 0.010785630904138088, Test Loss: 0.06737853586673737\n",
      "Epoch 9859, Train Loss: 0.0107840271666646, Test Loss: 0.06737257540225983\n",
      "Epoch 9860, Train Loss: 0.010780923999845982, Test Loss: 0.06736819446086884\n",
      "Epoch 9861, Train Loss: 0.010778765194118023, Test Loss: 0.06736888736486435\n",
      "Epoch 9862, Train Loss: 0.01077656727284193, Test Loss: 0.06736487150192261\n",
      "Epoch 9863, Train Loss: 0.01077426690608263, Test Loss: 0.06736371666193008\n",
      "Epoch 9864, Train Loss: 0.010772076435387135, Test Loss: 0.06735966354608536\n",
      "Epoch 9865, Train Loss: 0.010769878514111042, Test Loss: 0.06735769659280777\n",
      "Epoch 9866, Train Loss: 0.01076755952090025, Test Loss: 0.06736532598733902\n",
      "Epoch 9867, Train Loss: 0.010765081271529198, Test Loss: 0.06735516339540482\n",
      "Epoch 9868, Train Loss: 0.010762926191091537, Test Loss: 0.06735564023256302\n",
      "Epoch 9869, Train Loss: 0.010760841891169548, Test Loss: 0.06735923141241074\n",
      "Epoch 9870, Train Loss: 0.01075888890773058, Test Loss: 0.06734780967235565\n",
      "Epoch 9871, Train Loss: 0.010755999945104122, Test Loss: 0.0673539787530899\n",
      "Epoch 9872, Train Loss: 0.010753847658634186, Test Loss: 0.06734812259674072\n",
      "Epoch 9873, Train Loss: 0.010751785710453987, Test Loss: 0.0673423632979393\n",
      "Epoch 9874, Train Loss: 0.010749979875981808, Test Loss: 0.06735116988420486\n",
      "Epoch 9875, Train Loss: 0.010747503489255905, Test Loss: 0.06735112518072128\n",
      "Epoch 9876, Train Loss: 0.010744906961917877, Test Loss: 0.06734401732683182\n",
      "Epoch 9877, Train Loss: 0.010742560029029846, Test Loss: 0.06734438240528107\n",
      "Epoch 9878, Train Loss: 0.010740648955106735, Test Loss: 0.06734345108270645\n",
      "Epoch 9879, Train Loss: 0.010739102028310299, Test Loss: 0.06733763962984085\n",
      "Epoch 9880, Train Loss: 0.010736069642007351, Test Loss: 0.06734604388475418\n",
      "Epoch 9881, Train Loss: 0.010733808390796185, Test Loss: 0.06734015792608261\n",
      "Epoch 9882, Train Loss: 0.01073207426816225, Test Loss: 0.06733588129281998\n",
      "Epoch 9883, Train Loss: 0.01072956807911396, Test Loss: 0.06733632832765579\n",
      "Epoch 9884, Train Loss: 0.01072718296200037, Test Loss: 0.06733471155166626\n",
      "Epoch 9885, Train Loss: 0.010724751278758049, Test Loss: 0.06734392046928406\n",
      "Epoch 9886, Train Loss: 0.010722833685576916, Test Loss: 0.06732866168022156\n",
      "Epoch 9887, Train Loss: 0.010720350779592991, Test Loss: 0.06732166558504105\n",
      "Epoch 9888, Train Loss: 0.010718046687543392, Test Loss: 0.0673312172293663\n",
      "Epoch 9889, Train Loss: 0.010716152377426624, Test Loss: 0.06731878966093063\n",
      "Epoch 9890, Train Loss: 0.01071433536708355, Test Loss: 0.06731326878070831\n",
      "Epoch 9891, Train Loss: 0.010712094604969025, Test Loss: 0.0673094242811203\n",
      "Epoch 9892, Train Loss: 0.01070913951843977, Test Loss: 0.06731471419334412\n",
      "Epoch 9893, Train Loss: 0.010707376524806023, Test Loss: 0.06731344014406204\n",
      "Epoch 9894, Train Loss: 0.010704904794692993, Test Loss: 0.0673227310180664\n",
      "Epoch 9895, Train Loss: 0.010702911764383316, Test Loss: 0.06731858849525452\n",
      "Epoch 9896, Train Loss: 0.010700192302465439, Test Loss: 0.06731808185577393\n",
      "Epoch 9897, Train Loss: 0.010698008351027966, Test Loss: 0.06731041520833969\n",
      "Epoch 9898, Train Loss: 0.010695808567106724, Test Loss: 0.06730969250202179\n",
      "Epoch 9899, Train Loss: 0.010693592950701714, Test Loss: 0.06730977445840836\n",
      "Epoch 9900, Train Loss: 0.010691515170037746, Test Loss: 0.06729777902364731\n",
      "Epoch 9901, Train Loss: 0.010689381510019302, Test Loss: 0.06730439513921738\n",
      "Epoch 9902, Train Loss: 0.010686993598937988, Test Loss: 0.06729409843683243\n",
      "Epoch 9903, Train Loss: 0.010684765875339508, Test Loss: 0.0672992467880249\n",
      "Epoch 9904, Train Loss: 0.010682815685868263, Test Loss: 0.06728857755661011\n",
      "Epoch 9905, Train Loss: 0.010680205188691616, Test Loss: 0.06729387491941452\n",
      "Epoch 9906, Train Loss: 0.010678035207092762, Test Loss: 0.0672886073589325\n",
      "Epoch 9907, Train Loss: 0.010675844736397266, Test Loss: 0.06728503853082657\n",
      "Epoch 9908, Train Loss: 0.010673746466636658, Test Loss: 0.06728214025497437\n",
      "Epoch 9909, Train Loss: 0.010672223754227161, Test Loss: 0.06727386265993118\n",
      "Epoch 9910, Train Loss: 0.010669241659343243, Test Loss: 0.06728259474039078\n",
      "Epoch 9911, Train Loss: 0.01066697109490633, Test Loss: 0.06727422028779984\n",
      "Epoch 9912, Train Loss: 0.010665006004273891, Test Loss: 0.06727071106433868\n",
      "Epoch 9913, Train Loss: 0.010662425309419632, Test Loss: 0.06727483868598938\n",
      "Epoch 9914, Train Loss: 0.010660279542207718, Test Loss: 0.06726960837841034\n",
      "Epoch 9915, Train Loss: 0.010658180341124535, Test Loss: 0.06727884709835052\n",
      "Epoch 9916, Train Loss: 0.010656130500137806, Test Loss: 0.06728696078062057\n",
      "Epoch 9917, Train Loss: 0.010653924196958542, Test Loss: 0.06727638095617294\n",
      "Epoch 9918, Train Loss: 0.01065151672810316, Test Loss: 0.06726527959108353\n",
      "Epoch 9919, Train Loss: 0.01064975094050169, Test Loss: 0.06726042181253433\n",
      "Epoch 9920, Train Loss: 0.010647125542163849, Test Loss: 0.06726022809743881\n",
      "Epoch 9921, Train Loss: 0.010644703172147274, Test Loss: 0.0672643706202507\n",
      "Epoch 9922, Train Loss: 0.01064388733357191, Test Loss: 0.06727313250303268\n",
      "Epoch 9923, Train Loss: 0.010640321299433708, Test Loss: 0.0672648698091507\n",
      "Epoch 9924, Train Loss: 0.010638700798153877, Test Loss: 0.06725206226110458\n",
      "Epoch 9925, Train Loss: 0.010636406019330025, Test Loss: 0.06725073605775833\n",
      "Epoch 9926, Train Loss: 0.01063375174999237, Test Loss: 0.06725846230983734\n",
      "Epoch 9927, Train Loss: 0.01063154824078083, Test Loss: 0.0672493651509285\n",
      "Epoch 9928, Train Loss: 0.010629289783537388, Test Loss: 0.06724724173545837\n",
      "Epoch 9929, Train Loss: 0.010627117939293385, Test Loss: 0.06724432855844498\n",
      "Epoch 9930, Train Loss: 0.010624878108501434, Test Loss: 0.06724327057600021\n",
      "Epoch 9931, Train Loss: 0.010623111389577389, Test Loss: 0.06723333150148392\n",
      "Epoch 9932, Train Loss: 0.01062096282839775, Test Loss: 0.06724409013986588\n",
      "Epoch 9933, Train Loss: 0.010618581436574459, Test Loss: 0.06723901629447937\n",
      "Epoch 9934, Train Loss: 0.010616689920425415, Test Loss: 0.06724448502063751\n",
      "Epoch 9935, Train Loss: 0.010613895952701569, Test Loss: 0.06724473834037781\n",
      "Epoch 9936, Train Loss: 0.010611757636070251, Test Loss: 0.0672445222735405\n",
      "Epoch 9937, Train Loss: 0.010609489865601063, Test Loss: 0.06723683327436447\n",
      "Epoch 9938, Train Loss: 0.010607326403260231, Test Loss: 0.06723086535930634\n",
      "Epoch 9939, Train Loss: 0.010605374351143837, Test Loss: 0.06722638756036758\n",
      "Epoch 9940, Train Loss: 0.010603176429867744, Test Loss: 0.06722041964530945\n",
      "Epoch 9941, Train Loss: 0.010600815527141094, Test Loss: 0.06722799688577652\n",
      "Epoch 9942, Train Loss: 0.0105984415858984, Test Loss: 0.06722486019134521\n",
      "Epoch 9943, Train Loss: 0.010596565902233124, Test Loss: 0.06721356511116028\n",
      "Epoch 9944, Train Loss: 0.010594147257506847, Test Loss: 0.06721395999193192\n",
      "Epoch 9945, Train Loss: 0.010592125356197357, Test Loss: 0.0672202780842781\n",
      "Epoch 9946, Train Loss: 0.010589983314275742, Test Loss: 0.06721833348274231\n",
      "Epoch 9947, Train Loss: 0.010587504133582115, Test Loss: 0.06721197068691254\n",
      "Epoch 9948, Train Loss: 0.010585243813693523, Test Loss: 0.06720956414937973\n",
      "Epoch 9949, Train Loss: 0.010583274997770786, Test Loss: 0.06720282137393951\n",
      "Epoch 9950, Train Loss: 0.01058105006814003, Test Loss: 0.06721081584692001\n",
      "Epoch 9951, Train Loss: 0.010578804649412632, Test Loss: 0.0672077015042305\n",
      "Epoch 9952, Train Loss: 0.010576607659459114, Test Loss: 0.0672006756067276\n",
      "Epoch 9953, Train Loss: 0.010574613697826862, Test Loss: 0.06719821691513062\n",
      "Epoch 9954, Train Loss: 0.010572189465165138, Test Loss: 0.06721194088459015\n",
      "Epoch 9955, Train Loss: 0.01057016383856535, Test Loss: 0.0672135204076767\n",
      "Epoch 9956, Train Loss: 0.010567797347903252, Test Loss: 0.0672009140253067\n",
      "Epoch 9957, Train Loss: 0.01056603342294693, Test Loss: 0.06719789654016495\n",
      "Epoch 9958, Train Loss: 0.01056323666125536, Test Loss: 0.06719724088907242\n",
      "Epoch 9959, Train Loss: 0.010561526753008366, Test Loss: 0.06719573587179184\n",
      "Epoch 9960, Train Loss: 0.010559271089732647, Test Loss: 0.06719095259904861\n",
      "Epoch 9961, Train Loss: 0.010556795634329319, Test Loss: 0.06718461215496063\n",
      "Epoch 9962, Train Loss: 0.010554544627666473, Test Loss: 0.06718544661998749\n",
      "Epoch 9963, Train Loss: 0.010552788153290749, Test Loss: 0.06718599051237106\n",
      "Epoch 9964, Train Loss: 0.010550853796303272, Test Loss: 0.06719174981117249\n",
      "Epoch 9965, Train Loss: 0.010548143647611141, Test Loss: 0.06717638671398163\n",
      "Epoch 9966, Train Loss: 0.010545792058110237, Test Loss: 0.06718157231807709\n",
      "Epoch 9967, Train Loss: 0.010544192045927048, Test Loss: 0.06717953830957413\n",
      "Epoch 9968, Train Loss: 0.010541317984461784, Test Loss: 0.0671829804778099\n",
      "Epoch 9969, Train Loss: 0.010539169423282146, Test Loss: 0.0671834796667099\n",
      "Epoch 9970, Train Loss: 0.010537080466747284, Test Loss: 0.06716768443584442\n",
      "Epoch 9971, Train Loss: 0.010534767992794514, Test Loss: 0.06717313081026077\n",
      "Epoch 9972, Train Loss: 0.010533026419579983, Test Loss: 0.06717181950807571\n",
      "Epoch 9973, Train Loss: 0.01053086668252945, Test Loss: 0.06716374307870865\n",
      "Epoch 9974, Train Loss: 0.010528450831770897, Test Loss: 0.06716739386320114\n",
      "Epoch 9975, Train Loss: 0.010526199825108051, Test Loss: 0.06716442108154297\n",
      "Epoch 9976, Train Loss: 0.010524660348892212, Test Loss: 0.06716584414243698\n",
      "Epoch 9977, Train Loss: 0.01052157860249281, Test Loss: 0.06716077029705048\n",
      "Epoch 9978, Train Loss: 0.01051965169608593, Test Loss: 0.06716103106737137\n",
      "Epoch 9979, Train Loss: 0.01051738765090704, Test Loss: 0.06715583801269531\n",
      "Epoch 9980, Train Loss: 0.010515149682760239, Test Loss: 0.06716355681419373\n",
      "Epoch 9981, Train Loss: 0.010512974113225937, Test Loss: 0.06715857982635498\n",
      "Epoch 9982, Train Loss: 0.010511032305657864, Test Loss: 0.067159503698349\n",
      "Epoch 9983, Train Loss: 0.010508942417800426, Test Loss: 0.06715290993452072\n",
      "Epoch 9984, Train Loss: 0.010506329126656055, Test Loss: 0.0671587586402893\n",
      "Epoch 9985, Train Loss: 0.010504510253667831, Test Loss: 0.06714755296707153\n",
      "Epoch 9986, Train Loss: 0.010502181947231293, Test Loss: 0.06713889539241791\n",
      "Epoch 9987, Train Loss: 0.01050035934895277, Test Loss: 0.06713380664587021\n",
      "Epoch 9988, Train Loss: 0.010498094372451305, Test Loss: 0.06714396178722382\n",
      "Epoch 9989, Train Loss: 0.010495665483176708, Test Loss: 0.06713338196277618\n",
      "Epoch 9990, Train Loss: 0.010493822395801544, Test Loss: 0.06713495403528214\n",
      "Epoch 9991, Train Loss: 0.010491080582141876, Test Loss: 0.06713689118623734\n",
      "Epoch 9992, Train Loss: 0.010488993488252163, Test Loss: 0.06713877618312836\n",
      "Epoch 9993, Train Loss: 0.01048705168068409, Test Loss: 0.06713641434907913\n",
      "Epoch 9994, Train Loss: 0.010484928265213966, Test Loss: 0.06714261323213577\n",
      "Epoch 9995, Train Loss: 0.010482758283615112, Test Loss: 0.06714178621768951\n",
      "Epoch 9996, Train Loss: 0.010480348020792007, Test Loss: 0.06712894141674042\n",
      "Epoch 9997, Train Loss: 0.010478178970515728, Test Loss: 0.06712537258863449\n",
      "Epoch 9998, Train Loss: 0.010476415976881981, Test Loss: 0.06712473928928375\n",
      "Epoch 9999, Train Loss: 0.010474245995283127, Test Loss: 0.0671202763915062\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.2\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y.reshape(y_pred.shape))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_train = model(X_train)\n",
    "        y_pred_test = model(X_test)\n",
    "        train_loss = criterion(y_pred_train, Y_train.reshape(y_pred_train.shape))\n",
    "        test_loss = criterion(y_pred_test, Y_test.reshape(y_pred_test.shape))\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练和测试损失\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('PM-Euler(SGD) with lr=0.2 D=40 l=64')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Numerical_Method",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
