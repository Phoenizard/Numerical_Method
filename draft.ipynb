{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules import Simple_Perceptron\n",
    "from data import grip_data\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from utils import validate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 如果有多个GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设置随机数种子，例如：\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Simple_Perceptron.Simple_Perceptron(41, 100, 1)\n",
    "model = model.to(device)\n",
    "train_loader, X_train, Y_train, X_test, Y_test, D = grip_data.load_data(l=64, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0243, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(model(X_train), Y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified_CP(X, model):\n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 遍历输入维度和神经元，确保顺序正确\n",
    "    for i in range(input_dim):\n",
    "        for j in range(m):\n",
    "            mask = relu_output[:, j] > 0  # 只选择 ReLU 激活大于0的元素\n",
    "            # 修正后的索引填充顺序\n",
    "            J[:, j*input_dim + i] = (model.a[j] * X[:, i] * mask) / m\n",
    "    \n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified(X, model):\n",
    "    # 开始计时\n",
    "    # start = time.time()\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    # 计算模型输出\n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    for j in range(m):\n",
    "        mask = relu_output[:, j] > 0  # 只选择 ReLU 激活大于0的元素\n",
    "        J[:, j*input_dim:(j+1)*input_dim] = (model.a[j] * X * mask.view(-1, 1)) / m\n",
    "    \n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "\n",
    "    # 结束计时\n",
    "    # end = time.time()\n",
    "    # print(\"优化后Time: \", end - start)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "G_modified() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m U \u001b[38;5;241m=\u001b[39m (model(X) \u001b[38;5;241m-\u001b[39m Y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m J_1 \u001b[38;5;241m=\u001b[39m G(X, Y, model)\n\u001b[0;32m---> 30\u001b[0m J_2 \u001b[38;5;241m=\u001b[39m \u001b[43mG_modified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m U \u001b[38;5;241m=\u001b[39m (model\u001b[38;5;241m.\u001b[39mforward(X) \u001b[38;5;241m-\u001b[39m Y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m theta_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([model\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mflatten(), model\u001b[38;5;241m.\u001b[39ma\u001b[38;5;241m.\u001b[39mflatten()])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: G_modified() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def G(X, Y, model):\n",
    "    # 开始计时\n",
    "    start = time.time()\n",
    "    \n",
    "    input_dim, m= model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x = X[i].reshape(1, -1)  # x 的尺寸为 (,input_dim)\n",
    "        # print(x.shape, model.W.shape)\n",
    "        relu_input = x @ model.W \n",
    "        relu_output = torch.relu(relu_input)   # 计算 relu(<w_i, x>)，结果为 (m,)\n",
    "        for j in range(m):\n",
    "            if relu_output[0, j] > 0:\n",
    "                # print(J[i, j*input_dim: (j+1)*input_dim].shape)\n",
    "                # print(relu_output.shape)\n",
    "                J[i, j*input_dim: (j+1)*input_dim] = model.a[j] * x.view(-1) / m\n",
    "        J[i, m*input_dim:] = relu_output / m\n",
    "    # 结束计时\n",
    "    end = time.time()\n",
    "    # print(\"优化前Time: \", end - start)\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    X_tmp = X\n",
    "    Y_tmp = Y\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化后Time:  0.011729717254638672\n"
     ]
    }
   ],
   "source": [
    "U = (model.forward(X_tmp) - Y_tmp.reshape(-1, 1))\n",
    "J_2 = G_modified(X_tmp, model)\n",
    "#=========Remain to be Update=============\n",
    "theta_0 = torch.cat([model.W.flatten(), model.a.flatten()]).reshape(-1, 1)\n",
    "J_3 = torch.zeros(U.shape[0], theta_0.numel(), device=device)\n",
    "for i in range(U.shape[0]):\n",
    "    U[i].backward(retain_graph=True)\n",
    "    J_3[i] = torch.cat([model.W.grad.flatten(), model.a.grad.flatten()])\n",
    "    model.W.grad.zero_()\n",
    "    model.a.grad.zero_()\n",
    "#========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm difference between J_3 and J_2:\n",
      "tensor(8.3748, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Element-wise difference between J_3 and J_2:\n",
      "tensor([[0.0000e+00, 2.0560e-02, 8.5018e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.1807e-02, 9.6919e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 2.7847e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 4.1264e-03, 1.5707e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.4330e-02, 5.4546e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "Maximum difference: 0.11241750419139862 at position 21646\n"
     ]
    }
   ],
   "source": [
    "# 打印范数差异\n",
    "print(\"Norm difference between J_3 and J_2:\")\n",
    "print(torch.norm(J_3 - J_2))\n",
    "\n",
    "# 逐元素比较 J_3 和 J_2\n",
    "print(\"Element-wise difference between J_3 and J_2:\")\n",
    "print(J_3 - J_2)\n",
    "\n",
    "# 找出最大差异的位置\n",
    "max_diff = torch.max(torch.abs(J_3 - J_2))\n",
    "max_diff_idx = torch.argmax(torch.abs(J_3 - J_2))\n",
    "print(f\"Maximum difference: {max_diff.item()} at position {max_diff_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:08,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8559078574180603, Test Loss: 0.8777651190757751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.824286937713623, Test Loss: 0.8392741680145264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:06,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.750862181186676, Test Loss: 0.7655225992202759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:05,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.6808584332466125, Test Loss: 0.6921696066856384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:04<00:04,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.6468988656997681, Test Loss: 0.6569773554801941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:05<00:03,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.6141281127929688, Test Loss: 0.6221404075622559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:06<00:02,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.5697447657585144, Test Loss: 0.5789356231689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:07<00:01,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.5346863269805908, Test Loss: 0.5419045090675354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.5036500692367554, Test Loss: 0.5074693560600281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.47113943099975586, Test Loss: 0.4733397364616394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    flag = True\n",
    "    for X, Y in train_loader:\n",
    "        if flag:\n",
    "            U = (model(X) - Y.reshape(-1, 1))\n",
    "            flag = False\n",
    "        # 增广模型中的参数\n",
    "        theta_0 = torch.cat([model.W.flatten(), model.a.flatten()]).reshape(-1, 1)\n",
    "        #=====Jacobian 矩阵=========================\n",
    "        J = G_modified(X, model)\n",
    "        #===========================================\n",
    "        # 转置矩阵 J_T\n",
    "        with torch.no_grad():\n",
    "        # 计算 A = I + 2Δt * J_n * J_n^T，确保 A 在 CUDA 上\n",
    "            A = torch.eye(J.shape[0], device=device) + 2 * lr * torch.mm(J, J.T)\n",
    "            \n",
    "            # 使用 Cholesky 分解计算 A 的逆矩阵，确保操作在 CUDA 上\n",
    "            L = torch.linalg.cholesky(A)\n",
    "            A_inv = torch.cholesky_inverse(L)\n",
    "            \n",
    "            # 更新 U^{n+1}\n",
    "            U_1 = torch.mm(A_inv, U)\n",
    "            \n",
    "            # 更新 theta^{n+1}\n",
    "            theta_1 = theta_0 - 2 * lr * torch.mm(J.T, U_1)\n",
    "            \n",
    "            # 更新模型参数，确保更新后的参数在 GPU 上\n",
    "            model.W.data = theta_1[:model.W.numel()].reshape(model.W.shape)\n",
    "            model.a.data = theta_1[model.W.numel():].reshape(model.a.shape)\n",
    "            \n",
    "            # 更新 U_n 和 theta_n\n",
    "            U = U_1\n",
    "        \n",
    "    validate(model, X_train, Y_train, X_test, Y_test, epoch, is_recoard=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumericalMethod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
