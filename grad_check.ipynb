{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified(X, model):\n",
    "    # 开始计时\n",
    "    start = time.time()\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    W_grad = torch.zeros(batch_size, input_dim, m)\n",
    "    for j in range(m):\n",
    "        mask = (relu_input[:, j] > 0).float()  # 只选择 ReLU 激活大于0的元素\n",
    "        W_grad[:, :, j] = model.a[j] * X * mask.view(-1, 1) / m\n",
    "    J[:, :m*input_dim] = W_grad.reshape(W_grad.shape[0], -1)\n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    end = time.time()\n",
    "    print(\"计算Jacobian矩阵耗时：\", end - start)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified_CUDA(X, model):\n",
    "    # 开始计时\n",
    "    start = time.time()\n",
    "    # 确保所有张量在 CUDA 设备上\n",
    "    device = X.device\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "        # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    mask = (relu_input > 0).float()  # (batch_size, m)\n",
    "    \n",
    "    # 使用广播机制计算 W_grad\n",
    "    W_grad = (X.unsqueeze(2) * mask.unsqueeze(1)) / m  # (batch_size, input_dim, m)\n",
    "    W_grad = W_grad * model.a.view(1, 1, m)  # (batch_size, input_dim, m)\n",
    "    \n",
    "    # 将 W_grad 转换为二维矩阵并赋值给 J\n",
    "    J[:, :m*input_dim] = W_grad.reshape(batch_size, -1)\n",
    "    \n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"CUDA计算Jacobian矩阵耗时：\", end - start)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_model(torch.nn.Module):\n",
    "    def __init__(self, D, m, W: torch.Tensor, a: torch.Tensor):\n",
    "        super(test_model, self).__init__()\n",
    "        self.m = m\n",
    "        self.W = torch.nn.Parameter(W, requires_grad=True)\n",
    "        self.a = torch.nn.Parameter(a, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.relu(X @ self.W) @ self.a / self.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, m = 3, 2\n",
    "W = torch.tensor([[0.5, -0.3], [0.8, 0.6], [-0.2, 0.7]])\n",
    "a = torch.tensor([0.9, -1.1])\n",
    "X = torch.tensor([[1., 2., 3.], [0.5, 1.0, 1.5], [3., -2., 5]])\n",
    "model = test_model(3, 2, W, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算Jacobian矩阵耗时： 0.001752614974975586\n",
      "tensor([[ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250,  0.3750,  0.7500],\n",
      "        [ 0.0000, -1.6500, -0.0000,  1.1000,  0.0000, -2.7500,  0.0000,  0.7000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "CUDA计算Jacobian矩阵耗时： 0.00024890899658203125\n",
      "tensor([[ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250,  0.3750,  0.7500],\n",
      "        [ 0.0000, -1.6500, -0.0000,  1.1000,  0.0000, -2.7500,  0.0000,  0.7000]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 计算 G_modified\n",
    "J = G_modified(X, model)\n",
    "print(J)\n",
    "J_cuda = G_modified_CUDA(X, model)\n",
    "print(J_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.zeros((m*(D+1), m*(D+1)))\n",
    "\n",
    "for i in range(D):\n",
    "    for j in range(m):\n",
    "        P[j*D + i, i*m + j] = 1\n",
    "# 令最后m行m列为单位矩阵\n",
    "for i in range(m*D, m*(D+1)):\n",
    "    P[i, i] = 1\n",
    "\n",
    "print(torch.mm(J, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad_G(X, model):\n",
    "    # length = model.W.shape[0] * model.W.shape[1]\n",
    "    # height = X.shape[0]\n",
    "    # J = torch.zeros(height, length)\n",
    "    # y = model(X).flatten()\n",
    "    # grad_y = torch.zeros(y.shape)\n",
    "    # for i in range(y.shape[0]):\n",
    "    #     grad_y.zero_()\n",
    "    #     grad_y[i] = 1\n",
    "    #     w_grad = torch.autograd.grad(y, model.W, grad_y, retain_graph=True, create_graph=True)[0]\n",
    "    #     J[i] = w_grad.flatten()\n",
    "    # return J\n",
    "    output = model(X)\n",
    "    output.backward()\n",
    "    return model.W.grad, model.a.grad\n",
    "\n",
    "model = test_model(3, 2, W, a)\n",
    "X = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "w_grad, a_grad = auto_grad_G(X, model)\n",
    "print(model.W.data.flatten())\n",
    "print(w_grad)\n",
    "print(model.W.grad.flatten())\n",
    "print(model.a.data.flatten(), a_grad.flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Numerical_Method",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
