{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified(X, model):\n",
    "    # 开始计时\n",
    "    # start = time.time()\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    for j in range(m):\n",
    "        mask = (relu_input[:, j] > 0).float()  # 只选择 ReLU 激活大于0的元素\n",
    "        J[:, j*input_dim:(j+1)*input_dim] = (model.a[j] * X * mask.view(-1, 1)) / m\n",
    "        print(j, (model.a[j] * X * mask.view(-1, 1)) / m)\n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_model(torch.nn.Module):\n",
    "    def __init__(self, D, m, W: torch.Tensor, a: torch.Tensor):\n",
    "        super(test_model, self).__init__()\n",
    "        self.m = m\n",
    "        self.W = torch.nn.Parameter(W, requires_grad=True)\n",
    "        self.a = torch.nn.Parameter(a, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.relu(X @ self.W) @ self.a / self.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, m = 3, 2\n",
    "W = torch.tensor([[0.5, -0.3], [0.8, 0.6], [-0.2, 0.7]])\n",
    "a = torch.tensor([0.9, -1.1])\n",
    "X = torch.tensor([[1., 2., 3.], [0.5, 1.0, 1.5]])\n",
    "model = test_model(3, 2, W, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.4500, 0.9000, 1.3500],\n",
      "        [0.2250, 0.4500, 0.6750]], grad_fn=<DivBackward0>)\n",
      "1 tensor([[-0.5500, -1.1000, -1.6500],\n",
      "        [-0.2750, -0.5500, -0.8250]], grad_fn=<DivBackward0>)\n",
      "tensor([[ 0.4500,  0.9000,  1.3500, -0.5500, -1.1000, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250,  0.4500,  0.6750, -0.2750, -0.5500, -0.8250,  0.3750,  0.7500]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 计算 G_modified\n",
    "J = G_modified(X, model)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250,  0.3750,  0.7500]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "P = torch.zeros((m*(D+1), m*(D+1)))\n",
    "\n",
    "for i in range(D):\n",
    "    for j in range(m):\n",
    "        P[j*D + i, i*m + j] = 1\n",
    "# 令最后m行m列为单位矩阵\n",
    "for i in range(m*D, m*(D+1)):\n",
    "    P[i, i] = 1\n",
    "\n",
    "print(torch.mm(J, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000, -0.3000,  0.8000,  0.6000, -0.2000,  0.7000])\n",
      "tensor([ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250])\n",
      "tensor([ 0.9000, -1.1000]) tensor([0.3750, 0.7500])\n"
     ]
    }
   ],
   "source": [
    "def auto_grad_G(X, model):\n",
    "    # length = model.W.shape[0] * model.W.shape[1]\n",
    "    # height = X.shape[0]\n",
    "    # J = torch.zeros(height, length)\n",
    "    # y = model(X).flatten()\n",
    "    # grad_y = torch.zeros(y.shape)\n",
    "    # for i in range(y.shape[0]):\n",
    "    #     grad_y.zero_()\n",
    "    #     grad_y[i] = 1\n",
    "    #     w_grad = torch.autograd.grad(y, model.W, grad_y, retain_graph=True, create_graph=True)[0]\n",
    "    #     J[i] = w_grad.flatten()\n",
    "    # return J\n",
    "    output = model(X)\n",
    "    output.backward()\n",
    "    return model.W.grad, model.a.grad\n",
    "\n",
    "model = test_model(3, 2, W, a)\n",
    "X = torch.tensor([[0.5, 1.0, 1.5]])\n",
    "w_grad, a_grad = auto_grad_G(X, model)\n",
    "print(model.W.data.flatten())\n",
    "print(w_grad.flatten())\n",
    "print(model.a.data.flatten(), a_grad.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4500,  0.9000],\n",
      "        [ 1.3500, -0.5500],\n",
      "        [-1.1000, -1.6500]])\n"
     ]
    }
   ],
   "source": [
    "tmp_G = torch.tensor([0.45, 0.9, 1.35, -0.55, -1.1, -1.65])\n",
    "print(tmp_G.reshape(model.W.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Numerical_Method",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
