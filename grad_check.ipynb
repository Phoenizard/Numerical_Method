{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算Jacobi Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified(X, model):\n",
    "    # 开始计时\n",
    "    start = time.time()\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "    # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=X.device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    W_grad = torch.zeros(batch_size, input_dim, m)\n",
    "    for j in range(m):\n",
    "        mask = (relu_input[:, j] > 0).float()  # 只选择 ReLU 激活大于0的元素\n",
    "        W_grad[:, :, j] = model.a[j] * X * mask.view(-1, 1) / m\n",
    "    J[:, :m*input_dim] = W_grad.reshape(W_grad.shape[0], -1)\n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    end = time.time()\n",
    "    print(\"计算Jacobian矩阵耗时：\", end - start)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_modified_CUDA(X, model):\n",
    "    # 开始计时\n",
    "    start = time.time()\n",
    "    # 确保所有张量在 CUDA 设备上\n",
    "    device = X.device\n",
    "    \n",
    "    input_dim, m = model.W.shape  # m: 隐藏层神经元数量, input_dim: 输入维度\n",
    "    batch_size = X.shape[0]       # batch_size: 批处理大小\n",
    "    \n",
    "        # 初始化 Jacobian 矩阵 J，大小为 (batch_size, m * (input_dim + 1))\n",
    "    J = torch.zeros(batch_size, m * (input_dim + 1), device=device)\n",
    "    \n",
    "    # 计算所有样本的 <w_i, x> 和 ReLU 激活\n",
    "    relu_input = X @ model.W  # (batch_size, m)\n",
    "    relu_output = torch.relu(relu_input)  # (batch_size, m)\n",
    "    \n",
    "    # 对 w_i 的部分并行计算 Jacobian\n",
    "    mask = (relu_input > 0).float()  # (batch_size, m)\n",
    "    \n",
    "    # 使用广播机制计算 W_grad\n",
    "    W_grad = (X.unsqueeze(2) * mask.unsqueeze(1)) / m  # (batch_size, input_dim, m)\n",
    "    W_grad = W_grad * model.a.view(1, 1, m)  # (batch_size, input_dim, m)\n",
    "    \n",
    "    # 将 W_grad 转换为二维矩阵并赋值给 J\n",
    "    J[:, :m*input_dim] = W_grad.reshape(batch_size, -1)\n",
    "    \n",
    "    # 对 a_i 的部分并行计算 Jacobian\n",
    "    J[:, m*input_dim:] = relu_output / m\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"CUDA计算Jacobian矩阵耗时：\", end - start)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_model(torch.nn.Module):\n",
    "    def __init__(self, D, m, W: torch.Tensor, a: torch.Tensor):\n",
    "        super(test_model, self).__init__()\n",
    "        self.m = m\n",
    "        self.W = torch.nn.Parameter(W, requires_grad=True)\n",
    "        self.a = torch.nn.Parameter(a, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.nn.ReLU()(X @ self.W) @ self.a / self.m\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred - Y.reshape(y_pred.shape)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, m = 3, 2\n",
    "W = torch.tensor([[0.5, -0.3], [0.8, 0.6], [-0.2, 0.7]])\n",
    "a = torch.tensor([0.9, -1.1])\n",
    "X = torch.tensor([[1., 2., 3.], [0.5, 1.0, 1.5], [3., -2., 5]])\n",
    "model = test_model(3, 2, W, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算Jacobian矩阵耗时： 0.006013154983520508\n",
      "tensor([[ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250,  0.3750,  0.7500],\n",
      "        [ 0.0000, -1.6500, -0.0000,  1.1000,  0.0000, -2.7500,  0.0000,  0.7000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "CUDA计算Jacobian矩阵耗时： 0.0005838871002197266\n",
      "tensor([[ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500,  0.7500,  1.5000],\n",
      "        [ 0.2250, -0.2750,  0.4500, -0.5500,  0.6750, -0.8250,  0.3750,  0.7500],\n",
      "        [ 0.0000, -1.6500, -0.0000,  1.1000,  0.0000, -2.7500,  0.0000,  0.7000]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 计算 G_modified\n",
    "J = G_modified(X, model)\n",
    "print(J)\n",
    "J_cuda = G_modified_CUDA(X, model)\n",
    "print(J_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000, -0.3000,  0.8000,  0.6000, -0.2000,  0.7000])\n",
      "tensor([[ 0.4500, -0.5500],\n",
      "        [ 0.9000, -1.1000],\n",
      "        [ 1.3500, -1.6500]])\n",
      "tensor([ 0.4500, -0.5500,  0.9000, -1.1000,  1.3500, -1.6500])\n",
      "tensor([ 0.9000, -1.1000]) tensor([0.7500, 1.5000])\n"
     ]
    }
   ],
   "source": [
    "def auto_grad_G(X, model):\n",
    "    # length = model.W.shape[0] * model.W.shape[1]\n",
    "    # height = X.shape[0]\n",
    "    # J = torch.zeros(height, length)\n",
    "    # y = model(X).flatten()\n",
    "    # grad_y = torch.zeros(y.shape)\n",
    "    # for i in range(y.shape[0]):\n",
    "    #     grad_y.zero_()\n",
    "    #     grad_y[i] = 1\n",
    "    #     w_grad = torch.autograd.grad(y, model.W, grad_y, retain_graph=True, create_graph=True)[0]\n",
    "    #     J[i] = w_grad.flatten()\n",
    "    # return J\n",
    "    output = model(X)\n",
    "    output.backward()\n",
    "    return model.W.grad, model.a.grad\n",
    "\n",
    "model = test_model(3, 2, W, a)\n",
    "X = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "w_grad, a_grad = auto_grad_G(X, model)\n",
    "print(model.W.data.flatten())\n",
    "print(w_grad)\n",
    "print(model.W.grad.flatten())\n",
    "print(model.a.data.flatten(), a_grad.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W,a的Euler梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "D, m = 3, 2\n",
    "W = torch.tensor([[0.5, -0.3], [0.8, 0.6], [-0.2, 0.7]])\n",
    "a = torch.tensor([0.9, -1.1])\n",
    "X = torch.tensor([[1., 2., 3.], [0.5, 1.0, 1.5]])\n",
    "Y = torch.tensor([1., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9750, -0.4875, -0.7700], grad_fn=<DivBackward0>)\n",
      "tensor([3.9006, 0.2377, 3.1329], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = test_model(D, m, W, a)\n",
    "print(model(X))\n",
    "print(model.loss(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_grad(model, X, Y):\n",
    "    start = time.time()\n",
    "    # 获取模型的参数 a 和 W\n",
    "    with torch.no_grad():\n",
    "        a, W, hidden_dim = model.a, model.W, model.W.shape[1]\n",
    "        y_pred = model(X)\n",
    "        # 计算损失\n",
    "        grad_a = torch.zeros_like(a)  # 初始化 grad_a\n",
    "        z1 = torch.nn.ReLU()(torch.mm(X, W))  # 计算 z1 (激活后的隐藏层输出)\n",
    "        \n",
    "        # 计算每个隐藏神经元的梯度\n",
    "        for j in range(hidden_dim):\n",
    "            grad_a[j] = (2 * (y_pred - Y.reshape(y_pred.shape)) * z1[:, j]).mean() / hidden_dim\n",
    "        \n",
    "        # Step 2: 对 W 的梯度\n",
    "        grad_W = torch.zeros_like(W)  # 初始化 grad_W\n",
    "        \n",
    "        # 计算 ReLU 的梯度\n",
    "        for j in range(hidden_dim):\n",
    "            for i in range(X.shape[0]):  # 对每个样本进行求和\n",
    "                if z1[i, j] > 0:  # ReLU 导数 (只有正值才有梯度)\n",
    "                    grad_W[:, j] += (2 * (y_pred[i] - Y[i]) * a[j] / hidden_dim) * X[i]\n",
    "        \n",
    "        grad_W /= X.shape[0]  # 对样本数 N 进行归一化\n",
    "    end = time.time()\n",
    "    print(\"计算梯度耗时：\", end - start)\n",
    "    return grad_W, grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算梯度耗时： 0.012209177017211914\n",
      "tensor([[-0.6656,  2.7605],\n",
      "        [-1.3313,  0.3291],\n",
      "        [-1.9969,  5.6856]])\n",
      "tensor([-1.1094, -3.0448])\n"
     ]
    }
   ],
   "source": [
    "grad_W, grad_a = N_grad(model, X, Y)\n",
    "print(grad_W)\n",
    "print(grad_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自动求导耗时： 0.0026521682739257812\n",
      "tensor([[-0.9984,  1.2203],\n",
      "        [-1.9969,  2.4406],\n",
      "        [-2.9953,  3.6609]])\n",
      "tensor([-1.6641, -3.3281])\n",
      "计算梯度耗时： 0.000743865966796875\n",
      "tensor([[-0.9984,  1.2203],\n",
      "        [-1.9969,  2.4406],\n",
      "        [-2.9953,  3.6609]])\n",
      "tensor([-1.6641, -3.3281])\n"
     ]
    }
   ],
   "source": [
    "model = test_model(D, m, W, a)\n",
    "start = time.time()\n",
    "loss = model.loss(X, Y).mean()\n",
    "loss.backward()\n",
    "end = time.time()\n",
    "print(\"自动求导耗时：\", end - start)\n",
    "print(model.W.grad)\n",
    "print(model.a.grad)\n",
    "grad_W, grad_a = N_grad(model, X, Y)\n",
    "print(grad_W)\n",
    "print(grad_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Numerical_Method",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
